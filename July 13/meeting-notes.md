## Meeting Recording

[YouTube link](https://youtu.be/yRXvoEAZ3j0?si=iHa4GbJ63yQsHqow)


## Mastodon thread

[link](https://neuromatch.social/@OREL/112782145009203711)


## NOTES
SMN -- July 13. 

Jesse -- attended a workshop on Polyvagal theory --> social work, psychology, regulation of oneself.

* phenomenology ties (cardio-, micro-).

* direct interactiosn with individuals.


Siloed spaces. Applied update: sympathetic/parasympathetic nervous systems.

* direct interactions with individuals.

* can phenomenology be --> sybjective, personal experience.

* DevAI -- child development. Polyvagal (are things regulated or not?)

* JoPro updates -- websit updates. Media lab --> conferderation of labs.

* effective altruism (EA) mentoring.


Three states, two macrostates. Overlaps with Varela -- navigating states.

* World Models I. 

* Sarrah's work and the Sims paper. Polyvagal theory could have a space here.

* Avery's ideas into the SIMSulator. how states change --> BVs and meta-brains (personality descriptions).

* Simulation of Interactions approach. 


Emile Torres reading group. Broad, big takes on the future (divergent opinions).

* NeuroAI -- two weeks of synthesis, interdisciplinary.

* career development mentor. Long-term aim --> space for history of science.

* pacing of paper for historical trends are good.


Why were there AI winters? "Practical neccessity rather than curiosity".

* what is practical vs. curiosity-driven? Related to static takes of AI. 

* Winter Histories (one possible lens).

* language for degree of formalization in a certain field? Maturity ~ formalized.


Children's mental models of recursive logo programs: https://telearn.hal.science/hal-00190537

Kurland, D.M. and Pea, R.D. (1985). Children's mental models of recursive logo programs. Educational Computing Research, 1(2), 235-243.

* recursion concepts (kids got the wrong idea on their own). Resilient to evidence and counterfactuals.

* let's find the reduced wrong form --> data analysis and mapping to a manifold.

* isn't it useful? Mathematical importance of differentiability (singularity).

* Scardapane, S. (2024). "Alice's Adventures in a Differentiable Wonderland -- Volume I, A Tour of the Land". arXiv, 2404.17625.

UC Berkeley: course on LLMs. "Why do children and adults learn differently"?

* continual vs. subset, discrete learning (multistep, multiphase process).

* do not need to get everything right at first pass -- redescription needed.

* Babies -- fast learners, Adults -- slow learners (a different version of fast and slow).

## TRANSCRIPT
     
Transcript     
0:06     
hello all right um yeah we can get     
0:11     
started okay so uh good morning welcome to the meeting uh so we this week we had     
0:20     
our D meeting of course uh we didn't have an open source meeting this week but we did have     
0:26     
updates from the students um we had SAR of course and we had chup     
0:33     
who's doing open source sort of an open source scholar is not being funded by     
0:39     
gach and they're doing some really interesting work on the different     
0:45     
um forms of this open source sustainability so simulating uh open source communities in     
0:52     
different ways so SAR of course is doing the large language models and shom is doing something with     
0:59     
multi-agent enforcement learning and so they're both coming along pretty well on their projects uh the multi-agent     
1:06     
reinforcement learning is really interesting because we've been working on the reinforcement learning angle of     
1:12     
that for a couple years now so we hopefully will have by the end of this summer some really interesting work in     
1:19     
that area uh but also the large language models work is very interesting and so     
1:25     
SAR has been really kind of uh making that happen I'm really really impressed with some of the work     
1:32     
she's been de demoing in the meetings making poll requests so she just made     
1:38     
another poll request yesterday so thanks to SAR for that poll request and uh the     
1:44     
poll request she's made so far during gso and speaking of gso we've just     
1:49     
passed the first evaluation so I've been evaluating not only Sara but the     
1:56     
students in the diva worm group as well and have run past so congratulations to     
2:02     
them that's Pocky and mahul and so we're looking to get back     
2:09     
into the swing of the cybernetics meetings as well as the cognition Futures meeting so the cybernetics     
2:16     
meeting we held off on uh last week because it was the 4th of July in the     
2:21     
United States this week you know we didn't have time for it and so we're going to try to     
2:28     
get back in swing of things soon so that's     
2:33     
good okay so yeah let me get to some of the things     
2:40     
that we have here for today so the first thing is is that yes uh I just mentioned     
2:45     
that Sara basala uh has been working on this large language model framework for     
2:51     
gach for open to model open source communities she's been posting on our     
2:57     
medium uh she's posted the last four blog posts post on her medium so we've have these different updates that she's     
3:03     
given on the project show so she kind of in the first one highlighted from     
3:09     
reinforcement learning to large language models uh then sort of her Reflections on our community period and then uh the     
3:17     
building blocks for her approach and then the midpoint Milestone so this is the latest post where she talks about     
3:26     
using um llms and this framework and this larger framework so this is the LA     
3:32     
uh sort of using a llama and llama models so this is uh so she talks about     
3:41     
uh large using large language models exploring those sort of the recap of the     
3:46     
previous work and then creating this open source Community environment where you can deploy agents that are large     
3:53     
language models what the large language models are going to do is go through a     
3:59     
lot of the sort of the process of an contribution so like you know looking     
4:05     
for issues addressing issues working as both a contributor and a maintainer and     
4:11     
doing all that and so that's that's the kind of thing we you know we we spent a     
4:16     
lot of time two years ago on trying to figure out the sort of the uh how to     
4:21     
measure those things in open source communities and how to characterize them in an agent based model so this is     
4:29     
definitely you know um something that we can always     
4:35     
benefit from going over again but you know she's made some pretty nice advances in this so looking at different     
4:42     
algorithms to use to sort of characterize these multi-agent systems and how people might behave in an open-     
4:49     
Source community so you have the authoritarian model versus the decentralized model um this is of course     
4:56     
you know if you go to different open source communities some are very authoritarian some are     
5:03     
what they call benevolent dictatorships and some are rather decentralized and you know without     
5:10     
accounting for disruptive and bad behaviors or you know uh ego trips or     
5:17     
whatever you want to uh think of you can think of a lot of examples of that in open source these are basically models     
5:24     
of you know when you have different types of governance so if you have the authoritarian former governance where     
5:31     
you just have one person making decisions for everyone or decentralized governance where people     
5:37     
always vote on everything now most uh open source communities are somewhere in between     
5:43     
this so but but nevertheless these are good models just to kind of take it as a     
5:49     
as a way to sort of characterize you know different styles of governments and so this is you know this     
5:56     
is a nice post um thank you Sara for making that contribution and all the     
6:03     
contributions you've made both gach and to the project the poll request     
6:10     
Andor media which is always looking for Content so looks like we have Jesse here     
6:17     
today hello Jesse good morning     
6:27     
morning okay so I guess I could ask if you have an update or if     
6:33     
you're ex um essentially the answer is no because     
6:40     
I've actually been on sort of a real vacation the last week uh nice and very     
6:48     
different but even on my vacation I've stumbled into some interesting things     
6:55     
um one of them is I intended a     
7:01     
Workshop uh on poly vagal Theory which is sort of in     
7:09     
the space of uh kind of social work kind of like     
7:15     
clinical social work or reg like regulatory not really I don't really know of it's psychiatric stuff but     
7:22     
psychology social work space and and its cors     
7:27     
about regulation of oneself and um I really like the teacher the     
7:35     
teacher was de Dana who very been a social worker for many many many years and does like clinical work and     
7:41     
counseling and all this stuff uh but there's also I think it's     
7:46     
Steve pores who does the actual research and is working on also making sort of     
7:54     
biometric uh like tools to to to study in the case of the the the different     
8:00     
states of the theory I'm not going to go know what the theory is right now um I I     
8:06     
am positive about some applications and agnostics about others but it was very     
8:12     
interesting to to tie it directly to the lab it's very interesting to consider in     
8:19     
terms of the sort of phen phenomenology uh quest line that the     
8:25     
coition features group has gone on uh there were some pretty direct ties     
8:33     
to uh maybe things like micr phenomenology or cardio phenomenology in     
8:39     
terms of getting a an individual to sort of understand or demarcate aspects of     
8:47     
their experience and in polyvagal theory there's some really you know for General     
8:54     
categories um and and it's it's a very simple simple     
9:00     
you know mapping of different states or different sort of motivations but I really liked um the course the course I     
9:09     
attend was a pretty General like you know it was interesting because I think     
9:14     
there were many people who are like longtime practitioners of physical therapy um clinical social work which is     
9:24     
the more individualized for those who don't know like clinical social work is when you have people you counsel like     
9:30     
you counsel you're like there like direct interaction with individuals is also like meso and macro social work but     
9:37     
this is Deb Dana is a licensed clinical social worker I believe so she has a lot     
9:44     
of just like hands-on experience of leading people through through that stuff and I think I'm realizing kind of     
9:50     
through all of this is like that space um has it always had one of the things     
9:55     
that I talked about in in sort of the mental health working group and and in here maybe less less with the language     
10:02     
I'm using currently but uh you know there's kind of been these emerging a     
10:07     
lot of things that have developed in in the space of like what is social work or Psychiatry or psychology or cognitive     
10:14     
science or the whole like person studies Arena there's been developments in these spaces across time but many of them are     
10:21     
just quite Silo so what's what's an appealing angle to look at here is how     
10:28     
um in it like no no one said the word phenomenology at all during the whole uh     
10:35     
work of the whole um Workshop but it was indirectly like it's     
10:43     
what it was about um and see if I can find I've picked up a small book from     
10:49     
the library about like Child Development and po vagle Theory which which is might be an interesting bridge between some of     
10:55     
the developmental aspects of of that some of it was very very like a technical and research I have no I have     
11:03     
no particular like background in any of this um so I mean it     
11:09     
it's in essence it's it's it's a sort of Applied update to the idea of the     
11:16     
sympathetic versus the parasympathetic nervous system into a nervous system that's s     
11:24     
regulated or not in the ventral nervous system and and and and what what part of     
11:30     
like the Vagas stuff Vagas nerve and Vagas system is is sort of being more active than others     
11:36     
or things like that and I'm I'm I'm I don't yet I'm very agnostic and     
11:43     
don't have an informed or full opinion on you     
11:48     
know some aspects of that um but it's been interesting to think about in terms     
11:54     
of our investigations into you know like can there     
12:00     
be can phenomenology and sort of the the the verelan quest line to to to get a     
12:08     
little bit more of an informed or scientific take on subjective or personal     
12:14     
experience uh I think there's something interesting there's some interesting things there and especially the work of     
12:19     
Steve forges I believe that's his name who is founded it and is still doing     
12:25     
research and trying to make um like around it in in his lab he at     
12:32     
Indiana um and there's a lot of stuff there that     
12:38     
that everybody else you all you all know more than I do about that but um I'm     
12:44     
quite curious to look into I haven't had any time because I've been on doing a bunch of vacation stuff but it was very     
12:51     
exciting in that sense so that's something that like I definitely want to look more into and um kind of see where     
12:58     
the research is because that's thought the book I picked to the library was maybe it was 2020 it looked a bit older     
13:04     
but it might have been just been 2020 uh but just like applied to Child Development which is might be an     
13:11     
interesting brick between some de AI stuff and or general developmental focus and then you know this other thing this     
13:19     
other thing I'm not I really I really personally do like the applied version     
13:24     
of poly vagle theory that de Dana uh LED through the work jop but I     
13:32     
am I don't you know I'm not sure I don't know I don't know yet what     
13:39     
to think about the the Enterprise as a whole but it was it was a nice kind of un I I didn't plan to to to to walk into     
13:47     
that uh over my last last week or so but I did and I'm I'm happy to do that happy     
13:53     
that are happy mostly so that's sort of one one update I don't know if there's any questions or comments about that     
13:59     
before I move on S all this stuff um you could move on I I have some ideas but     
14:05     
I'm not quite yet sure what they are so yeah um other things on the docket uh A     
14:12     
continuing a very long slope process of updating some things for joepro I think     
14:18     
I'm almost there I think it's so much work just to do so     
14:23     
much Small Things even on like a website like like I like I really just wanted to     
14:28     
update shift the website being hosted from like Google sites to another post     
14:34     
and doing that the right way has taken quite a long time um it's been interesting to compare to     
14:41     
other other groups and and sort of the path of of taking what I want to do with     
14:47     
joepro in a presentational or just a general sense to like other groups that are trying to     
14:53     
do incubator type things um like one comparison that I like at least aesthetically but I'm     
15:00     
obviously nowhere near in reality is sort of what the media lab is doing and how the media lab and MIT it's basically     
15:08     
just a bunch of these sort of in it's almost like a bunch of independent independent states in the United States     
15:14     
way but independent Labs that are you know much more funner than I'll ever     
15:19     
have any fun to do anything so it's like I'm acknowledging that's not going to happen but the presentation of it and     
15:26     
and the way the way they kind of have this Federation of different things I     
15:32     
think is really cool um at least it's kind of honestly I'm like how confusing their website is and how much their     
15:38     
website has a lot of details that you won't know until you pay attention to uh which makes sense in a lot of ways for     
15:44     
what the lab is what M MIT is but I've been I've been like you know between     
15:50     
that and some things that effective altruism has been doing uh for their     
15:57     
um like their m ing stuff they kind of not really competitor analysis but just     
16:03     
realizing when people come to what I'm trying to do with joepro what they're going to see or what the what what their     
16:08     
most obvious reference points are going to be are probably going to be those things and so um as it say joepro is a     
16:17     
humble fledgling small uh group that that is trying to     
16:23     
support a few products and I don't think it's really good to be anything more than that ever I'm not I'm interested in     
16:29     
doing that right now but I'm interested in having a     
16:34     
legitimate um I don't know a little bit I'm upgrading a little bit what what what it     
16:41     
what it is in terms of what people can see from it uh but hopefully that will     
16:47     
basically be that website update will hopefully come online in the next week or so it really will look almost exactly     
16:53     
the same but but it will have a much better foundation for things I want to in the     
16:59     
future than what was existing so that's what the goal is     
17:04     
um while I was on vacation uh Jen was doing some things for the mental health     
17:10     
working group and know she's been writing about um things about inside out     
17:15     
one really cool thing as a side note would be um what if     
17:21     
uh what I might write a small post about this but it's not not a big deal but like what I've canide out was actually     
17:26     
based in poly bagle Theory instead of ifs internal family systems actually     
17:31     
think it would be much easier in some ways because the models the model is is     
17:37     
is much less complicated like it's basically three states and and sort of     
17:43     
two macro states of either are you set are you are you are you eventually     
17:49     
regulated or are you not and and it's a very very simple it's it's not it's not     
17:55     
it's not I'm not even sure it's like     
18:01     
um I don't know and I haven't looked in the research I can't say like as as I     
18:06     
was taught it it's it's not really like I don't even know if has the potential to be academically rigorous to an extent     
18:12     
but functionally speaking and this is where I think there's some really interesting overlaps     
18:18     
to what the verelan phenomenological investigation line is trying to do     
18:24     
functionally speaking in terms of giving a language to people navigating this States I think it's     
18:31     
actually has some uh substance there so that's why thinking about like almost a     
18:38     
sense of what if Inside Out was made with poly bagle Theory it would be a very interesting sort of discussion or     
18:44     
at least a bridging point and I think I think inside out     
18:49     
um in the Contemporary life of our kind of limited models for     
18:56     
anything in this space uh is is a great it's a great bridging Point um so we'll     
19:03     
see about that um conition Futures didn't meet once again I apologize uh     
19:08     
travel plans have been I've actually had like legitimate stuff going on the whole     
19:14     
time so um but uh I'm looking forward to next week being at least at least for     
19:20     
the cybernetics reading group I've been I got I looked a little bit more at the uh the paper your paper or our paper I     
19:27     
guess the models one I'm looking forward to I look forward to getting like going     
19:33     
through that one totally again um and what else other General updates     
19:40     
are uh     
19:45     
um I have other I'm trying to just as I'm back from vacation it's sort     
19:52     
of like that unpleasant like you know it's just slow to pick up where you left     
19:57     
off but I left off in the middle of a lot of Joo stuff I'm realizing I need to update my own website some which I don't     
20:03     
really want to do like like I I realized like the effort I'm realizing just the disparity     
20:10     
between the effort to bless you the effort to create things     
20:15     
versus the effort to design stuff that other people are looking at it just     
20:20     
feels so unpleasant to go back and forth between those two things like I wanted to stay and okay I'm writing it and     
20:28     
really I'm really trying to put my my all into writing this thing that doesn't exist or words that haven't been said at     
20:35     
least by me before versus designing how things look and it's like oh like you     
20:41     
want to make it like I understand it I continue to understand why so many     
20:46     
academic websites or like people's websites are just so minimalistic or     
20:52     
just whatever because it's just it's just it's not the easiest thing to     
20:57     
Outsource either sound like I'd like to make a website to sell my sell some products like on my a digital     
21:03     
Marketplace my business and sell these products like it's not as simple as that yeah it just isn't because because how     
21:10     
you phrase things is the decision pressure how are you GNA show this you     
21:16     
know how even even the most basic versions of it are are are tedious in     
21:21     
different ways so I'm dealing with that but I'm happy to be back I'm thinking about okay I think it's like 50 days     
21:28     
between now and September 1st okay and I'm just thinking like what is it that I     
21:33     
want to try to do in these 50 days right including stuff some other suim things to do but like I I really would like to     
21:42     
finish in my high my deals there's a bunch of preprints I want     
21:47     
to like get get into preprint status so     
21:54     
we'll see um any other updates on my broadly uh plot Twisters     
22:02     
this nothing new to say there kind of an upholding pattern and thank you     
22:08     
for letting the update happen there oh no problem yeah okay uh I think that's     
22:15     
good so yeah thanks for that update Jesse um it was interesting lot of I'm     
22:20     
not familiar with poly vagel Theory it might be an interesting thing to talk about with respect to develop uh     
22:26     
development or developmental AI or whatever you're getting at with that and of course we'll pick up with     
22:34     
the cognition Futures meetings when you're ready and of course the cybernetics meetings uh so that that'll     
22:41     
be fun too I think there there I'm     
22:46     
remembering something right now just I'll briefly say just to put it just to mention     
22:52     
it it would be very there's some interesting like essentially bridging from the work that I guess SAR is     
22:59     
somewhat doing now and the paper that we had that was basically the Sims paper about modeling every The Sims Town     
23:06     
potential interactions with with LMS and updating that I think there could be something     
23:12     
there as well in terms of um like Po Bag Theory could have a space     
23:20     
there and also there was something that uh I think last week or week     
23:28     
beforehand I don't know if I mention it but Avery actually     
23:34     
um was discussing some of their theories and     
23:39     
ideas and oh I remember one other thing too uh but but but I think I would     
23:46     
really it would it wouldn't it may even be like a fall or the next year but     
23:53     
legitimate thing that could happen uh would be putting some of Avery's ideas     
23:59     
into essentially the Sim simulator okay uh because Avery has very sort of unique     
24:05     
and novel takes on interpersonal intra and Inter interpersonal management I     
24:11     
guess management sounds a little administrated about like how States change and how different characteristics     
24:18     
play out so to say so it would be really interesting to basically remake that paper with some of Avery's theories I     
24:25     
think that'd be a really fun thing to do and legitimate like I would like to do that I think and I think it could be     
24:30     
pretty tenable to do like you know it wouldn't it wouldn't it's one of the things that seems like very much     
24:36     
possible like as either as a um I don't know if we could brid it all     
24:42     
the way into like actually picking up brenberg vehicles or some metab brain stuff I don't know if that's that would     
24:48     
be a bit of a stretch that would be like an amazing potential future Google some code type thing yeah uh um but in a in a     
24:56     
in a much more General sense just basically making the paper happen again with different different like uh     
25:04     
personality um descriptions would be interesting     
25:10     
um and and there was there was a few a few things on on that line like there's a     
25:18     
space for like I don't know I don't know what to say and maybe it's essentially just representational brains and     
25:24     
phenotypes but I feel like there's there's a version where     
25:29     
between all that stuff Dei well no let me back up between everything under that     
25:35     
General Banner which is sort of rain briak Vehicles metab brains and maybe developmental     
25:42     
AI um and and and I guess it Bridges a little bit with things that are     
25:47     
happening this summer but um I think there'd be something really     
25:52     
cool to return I don't know if to return to is the word but for the lab     
26:02     
to the lab to build out or return to some of it like simulation of interaction approach     
26:10     
because it's something we can totally do like we don't need we don't need to make we don't we don't need more     
26:17     
resources than we have you know what I'm saying right so I think I think that might be a cool thing to do and especially if you     
26:25     
can combine it with Aver's like every's ever you know EverQuest for um     
26:34     
what what what they do uh that's something that's definitely interesting and appealing and     
26:40     
something I'd like to get more experience in down the road as well um     
26:45     
so yeah um just sort of making a big point about     
26:51     
that just to say hey let's do that um and maybe that's actually let me me um     
26:58     
just gonna write give me give me like 10 seconds     
27:03     
okay just I write my own     
27:17     
note okay um the last thing which is pretty minor     
27:22     
um I also happen to attend a reading group by Emil Torres     
27:28     
who is uh the the sort of the the um currently     
27:35     
probably most known for being like with timet jabu a co-author on the test     
27:41     
real uh bundle stuff like the transhumanist um rationalists blah     
27:49     
effective alism long termers and whatnot um but I think their home base as in     
27:57     
ail's home base is more in sort of studying     
28:03     
um like they they've written books in like uh takes on extinction or like like     
28:09     
how how people feel about where things are like where the future is going or     
28:15     
the end end of civilizations or ends of whatever and was is kind of you know Grim topics but     
28:22     
um I attended to work a a book club with that um and I can't     
28:29     
remember the details of it specifically but it it was it was it was     
28:34     
basically going through some of their uh it was it was very philos     
28:40     
philosophy oriented so maybe some to talk about Amanda with but um just it     
28:46     
was just sort of big bigger broader takes on on how the future uh is evolving it was it it was     
28:53     
quite interesting um it's it's always interesting to see     
28:59     
um it's always interesting to think back to the Princeton Vision time where Phil     
29:05     
Logan myself Avery uh uh Angela and some other people     
29:12     
were all together and we've all kind of branched out in different ways and I think I think     
29:18     
Emil uh and um Logan are both deeply     
29:24     
concerned about the future in the different ways uh and it's kind of interesting because I     
29:30     
think I'm somewhere in the middle between them whereas Emil is very     
29:36     
uh Grim like Grim not Grim dark but like     
29:41     
Grim hope like let's look at all these really negative things and throw out     
29:46     
where they're coming from was Logan's very very no we need to talk positively about the future I think they're both     
29:52     
right in different ways but it's interesting to kind of see um     
29:58     
to see this group of this sort of I don't know the the the class of of     
30:03     
Princeton Vision 2019 and all the things we're trying to do in the world um since     
30:09     
then uh so yeah um I I I deeply     
30:17     
deeply uh appreciate all of those people     
30:22     
and and being able to see where they're going I Logan posted something recently also but I I don't I don't have anything     
30:29     
great to say like I can't I can't think of it clearly enough to talk about it but um I know there's some things with     
30:35     
the foresight Institute that he's looking to do or has done uh that I want to try to do next week I think there's     
30:41     
some some event coming up that him and or my friend Dan uh are trying to do but more more     
30:48     
about that later so that's it that's my those are my updat for the time being okay well that's great yeah it sounds     
30:55     
really interesting um and yeah following up on some of that     
31:01     
sort of futurist stuff too it's interesting     
31:06     
yeah so why don't I move on to some other things actually this coming week     
31:12     
will be the neuro AI course for neurom match so oh yes uh neurom matches of     
31:19     
course happening this year they have now I think at least three courses they have the Deep learning course the neuro     
31:26     
computational neuroscience course and the neuro AI course which is brand new so I'm going to be mentoring for the     
31:33     
neuro AI course um and it's you know like I've done every year where students     
31:39     
will have projects they'll well I'll meet with them uh I think twice to sort     
31:45     
of mentor them on their projects and they'll go through the summer school which is like a series of     
31:51     
topics uh that they'll do a crash C you know like a two or three week crash course in     
31:58     
and then they do the project and then they present on their project at the end of the     
32:03     
course so since given that the nuro AI course is a new course I figured we'd     
32:10     
look through what they have on their syllabus so this is neurom match's     
32:17     
website this is the neuro AI course this is a brand new course this year and of course neuro AI is where you     
32:23     
have the neuro which is the Neuroscience the computational Neuroscience the models of the brain the models of     
32:30     
behavior and then the AI which is the artificial intelligence part which is where you have you know simulations of     
32:37     
behavior simulations of you know things that you models that you train and they     
32:43     
produce output large language models deep learning whatever and so that the     
32:49     
idea behind that is where you want to be able to look at these two things together maybe with some uh bio inspired     
32:56     
models as well I'm not really sure how they're going to approach this uh at the at the very uh technical level but they     
33:05     
have some you know some data sets like they do for the computational Neuroscience course they have some basic     
33:12     
topics that they're going to go over so let's see what they have um so this is a two-e live     
33:18     
instruction course so it's actually two weeks I believe the Deep learning course is three weeks but or the computational     
33:25     
Neuroscience course as well but that's you know they're they always keep     
33:30     
changing it and tinkering with it so uh this course is aimed in a more     
33:36     
advanced audience than our other courses students should have already taken our deep learning and computational     
33:41     
Neuroscience courses are the equivalent so I think both like this is for people have taken both of these or     
33:48     
at least one of these and you know they kind of want to go over topics more in depth given that that sort of um BAS of     
33:59     
knowledge uh so the neuro AI course the motivating question is what are the     
34:05     
common principles of natural and artificial intelligence so the natural intelligence systems the artificial     
34:11     
intelligence systems uh and then sort of find common principles between them um so I mean a     
34:18     
lot of the neuro I've seen is not really I mean people haven't really come up with these common principles it sort of     
34:26     
maybe a goal but a lot of the stuff is largely that at least I've seen has largely been around you know focused     
34:32     
around applications of maybe like you know some like maybe like     
34:40     
predicting uh V1 activity with an all net or something like that uh and then     
34:46     
of course the idea is that you get common principles from that but the common principles we don't really know what those on so that's what we're     
34:53     
trying to get at the course and usually you know when you teach a course you you kind of ask the question and it's kind     
35:00     
of rhetorical you maybe know the answer somewhat or you are trying to find out     
35:07     
more you know but that's usually a graduate level course where you can like unpack things with discussion this more     
35:14     
is more of a summer school so it's interesting that they're approaching at this way the core challenge of intelligence     
35:21     
is generalization so this is the the what they Define as the core challenge of intelligence so it's basically     
35:26     
learning things and then General izing from the training set or from that body of knowledge to other     
35:32     
things Neuroscience cognitive science and AI are questing for principles to help     
35:39     
generalization so this is again uh I I guess that's true I mean you know     
35:45     
cognitive science I'm not really sure that that's the same as what they're trying to do in AI same thing with     
35:52     
Neuroscience but you know that's that's the way you put these together major system features that affect     
35:58     
generalization include task structure like multitasking multiple inputs of the same     
36:04     
output and vice versa so like multiple input multiple output systems basically     
36:10     
where you know you go beyond the single input single output system um and micro     
36:16     
circuitry nonlinearities canonical motifs and their operations sparsities so this whole area here where you're     
36:23     
looking at small circuits you're looking at their Dynamics their behaviors your     
36:28     
structure and you know some of the attributes of those circuits so you know     
36:34     
we know for example that you know we can get intelligence out of circuits that are relatively simple or sometimes     
36:42     
circuits that are relatively hard to like see the structure and so this is     
36:47     
this is all kind of what we're interested in is this what is this what's the secret sauce of this     
36:53     
circuit um if we can if we can boil it down to a circuit     
36:59     
uh so there's micro circuitry there's macro circuitry or architecture and this includes modules for memory information     
37:06     
segregation weight sharing by input symmetry or common development this is     
37:11     
an interesting um this is actually a particular interesting statement here     
37:17     
weight sharing by input symmetry or common development so this is something we're doing with developmental     
37:24     
AI um I don't have it with me but there's a paper I'm working on on uh     
37:31     
developmental breitenberg vehicles and sort of the morphology of the vehicle or     
37:37     
the the the sort of the Symmetry relative Symmetry and shape of the body     
37:43     
especially as it changes in development with respect to like some external stimulus and then an evolving nervous     
37:51     
system Network so this is an interesting question for my interests uh but also     
37:58     
for the lab and More gener in any case that's that just shows you where kind of we fit into this learning rules synaptic     
38:06     
plasticity modulation so you know when we're dealing with synapses you know we have we could have     
38:12     
just simple weights we could have weights with rules we could have then we have to also deal with elasticity we     
38:18     
have to deal with modulation so these are things that kind of go beyond what we have in uh you know typical deep net     
38:26     
architectures for example where we just have like a matrix of Weights you know we'll have a matrix of     
38:32     
Weights over time but those weights are sort of governed by certain transition rules or you know dynamics that we don't     
38:40     
usually put into the models and then they also have data     
38:45     
stream and what they the word they use is curriculum so this is like curriculum learning or curriculum training or autoc     
38:51     
curricula that that theme in in the machine learning literature okay so there are a bunch of     
38:58     
really interesting issues around n they're kind of unpacking a lot of the sort of the basics of the field so this     
39:05     
is an interesting kind of uh set of approaches I mean you know you could argue whether this is the totality of     
39:12     
neuro AI whether maybe we should talk more about like bio inspired systems or     
39:17     
whatever but this is at least I think a good start we aim to present current     
39:24     
understanding of how these issues arise in both natural and artificial intelligence comparing how these system features affect     
39:30     
representations computations and learning we provide case studies and coding exercises that illustrate these     
39:37     
issues in neuroscience and Co cognitive science and AI so across these three Fields Neuroscience cognitive science AI     
39:45     
one is dealing kind of with the mind one is dealing with the brain and one is dealing with     
39:51     
artificial applications of that although you know AI is traditionally not really     
39:57     
been that closely tied to neuroscience and cognitive science at least the the sort of experimental work you know that     
40:04     
you know AI is sort of a a cogntive cognitive science but you know uh it's a     
40:12     
you know how that goes you get you start to drift away from your cognate as you     
40:17     
sort of develop your own agenda yeah here we go plus one so there     
40:24     
are three learning goals for this or actually Four learning goals for this uh workshop for the summer     
40:30     
school learning goal number one a common understanding of vocabulary to describe     
40:36     
changes faced by natural intelligent systems so or challenges okay so     
40:41     
learning goal number one a common understanding of vocabulary describe challenges faced by natural intelligence     
40:47     
systems so it's this common understanding of vocabulary describe core con core     
40:54     
concept U describe core shared concept in Neuroscience cognitive science and     
40:59     
machine learning and how they differ from each other describe and Implement different ways in which Ann can be     
41:05     
compared with the BNN the Ann being the artificial neural network the BNN being     
41:11     
the biological neural network then describe multiple scales of computation     
41:16     
and multiple scales of study so in this case they're thinking of Mars levels or what how and why which is sort of the     
41:23     
tin bgan View and kind of bringing that together in into a single sort of     
41:29     
framework for looking at intelligence so again Mar's levels are     
41:34     
these different levels of explanation uh in the Tim bgan view you're asking how and why something is     
41:42     
useful both developmentally and evolutionarily and proximally and distally and so you get this model of     
41:50     
sort of level of explanation and also function and     
41:55     
structure so that's good um this I guess you know the way I see this is like they're     
42:02     
trying to really kind of synthesize the field because there doesn't really exist a strong synthesis so you're teaching     
42:09     
people something about the synthesis but it's also an incomplete synthesis so you're trying to encourage them to think     
42:16     
about like how to bring those things together that's an interesting learning goal we'll see how it     
42:21     
goes learning goal number two experience a multiplicity of approaches and interests at the     
42:29     
intersection of neuro and AI be able to describe some of these approaches and interests that's that's good learning     
42:36     
goal three be able to practically Implement Nur AI model so this is basically just standard you know going     
42:43     
technical demos you know going through uh how to do something coding and     
42:50     
training models adding more features to existing models debugging with the within guard rails interpreting izing     
42:57     
critiquing existing models so this is just implementing debugging models which     
43:03     
is of course important and then know being able to sort of critique models     
43:08     
and their shortcomings this is an interesting learning goal I don't know if they can do this in like two weeks     
43:13     
though this is tough because I guess this is what the projects are for and so     
43:18     
you know you're going to have to do all those things in the project so having a practical aspect of it is nice but it's     
43:24     
like you know that's something that as an instructor of a technical of     
43:29     
technical subjects I know that you can only you know you have to really kind of constrain how people spend their time on     
43:37     
this like you could have them do a practical implementation it could take them of $100 hours that's not something     
43:44     
that's practical for a course especially a twoe course so you know getting that those sort of you know examples the sort     
43:52     
of laboratory exercises practical exercises down to a manager piece is     
43:58     
always said Challenge and then learning goal four complete research that deals with difficulties and narrow a out so     
44:05     
this is an interesting point um this is research that sort of you know if you     
44:11     
can Implement something it's always going to be difficult this is not something that people I mean there's a     
44:18     
research field in neuro AI but really the applications are not quite there yet     
44:23     
so that's the thing that they're getting at here um you know can you TR try to     
44:29     
implement something as a neuro AI application if you can't why can't you     
44:34     
if you can how hard was it and they say that knowing that it's not an easy lift     
44:40     
because we don't have really good models that are you know sort of this neuro AI     
44:46     
approach it's just kind of bringing things together and seeing if they work so a lot of this is actually posing     
44:53     
and solving problems so the first part is writing down a problem in a way that makes it     
45:00     
tractable interacting with other people from other disciplines do research read     
45:05     
papers Implement previous State ofthe art and then communicating their research in ways that are comprehensible     
45:12     
so this is all kind of working in an interdisciplinary fashion from the you know from AI Neuroscience cognitive     
45:19     
science and then give you know figuring out kind of how to write out a problem     
45:25     
that is solvable and can be solved in two weeks to at least some degree of     
45:30     
satisfaction and then also bringing these fields together and implementing something getting it to work that's a     
45:36     
lot it's a lot of stuff in two weeks but this is I think very useful and you know     
45:42     
there's going to be a lot uh that you can learn from this so     
45:47     
we have open source content again this is the narrow AI course content on GitHub so there's a GitHub     
45:55     
repository for this um we haven't forked it yet I'll have to Fork it later um but this is uh the     
46:05     
syllabus so it looks like we have a lot of contributors here to this repository we have the     
46:12     
book which is the Jupiter book this is where you know you have all of your sort     
46:18     
of demos and things like that um     
46:24     
prere so I guess the PRX are just like different you know Neuroscience familiarity programming knowing pytorch     
46:33     
deep learning math skills basic stuff like that I think the requirements are     
46:38     
similar to The Deep learning course um the projects that's you know kind of     
46:45     
working on will'll probably fill this in there's the keynote in the Jupiter     
46:50     
notebook format so this is kind of going over uh this is actually a climate based     
46:58     
keynote so this is I don't know if this is filler but this is basically it's an     
47:04     
interesting uh sort of approach because you're kind of giving uh code for the slides code for     
47:12     
the video here it's interest I don't know what they're doing here but um and then of course we have     
47:21     
tutorials and these are all kind of for these different areas so you have modules schedules technical help the     
47:28     
generalization comparing tasks comparing artificial and biological networks micro     
47:34     
circuits macro circuits neuros symbolic methods we've talked about that     
47:40     
microlearning macro learning Mysteries I don't know what that is but these are     
47:46     
this is their agenda for the two weeks so week one will be generalization comparing tasks will be day two day     
47:53     
three is comparing artificial and biological networks day five is micro circuits so day four they didn't have on     
48:00     
here the second week macro circuits is day one neuros symbolic methods is day     
48:06     
two micro learning is day three macro learning is day four and Mysteries is     
48:11     
day five and I wonder what Mysteries is so this kind of goes through but they     
48:17     
don't have I guess they have the tutorials in here the intro and the     
48:23     
outro um just curious what these are I guess they don't have them really ready yet     
48:29     
but um yeah they're going to have well they have the video If feedback and they     
48:35     
have slides so they're they're I don't think they're quite have everything populated yet but that's the breakdown     
48:42     
of the course so that's the breakdown of sort of the uh the learning goals the     
48:48     
curriculum and then this is a thing that kind of puts the curriculum in a context     
48:54     
so you have this sort of uh quadrant system where you have     
49:00     
applied a I applied artificial intelligence at the top left um and that involves mining neuro     
49:08     
to find new AI so that means that you go to back to neuro and you look what people were doing and you try to find     
49:15     
interesting AI in them so there's all sorts of stuff like predictive coding     
49:21     
perceptrons reinforcement learning attention and turn test test and those     
49:27     
are all things I guess I don't know about embod oh the embodi turn test so those are all things that you look at     
49:33     
neuro and you see what people are doing major things and then you bring that into     
49:39     
AI uh using neuro data is realign AI meaning that I guess you can use neuro     
49:46     
uh Neuroscience data to improve your models you have things like biologically     
49:51     
inspired perceptrons biologically detailed modeling cognitive architectures and symbolic AI That's all     
49:57     
an applied AI in that box there's clinical Neuroscience which is this     
50:04     
quadrant you have things like digital twins plus AI to optimize     
50:10     
treatments uh Foundation models for Neuroscience AI based health biomarkers     
50:15     
and I think they don't really have enough to fill this out but um there's also AI tooling like computational     
50:23     
athology which is animal behavior and looking at that computationally dynamical models Spike sorting encoding     
50:31     
and BCI that's all in this clinical Neuroscience bin uh basic Neuroscience     
50:38     
AI as models of the brain so this includes like looking at different types of models and how they sort of     
50:44     
approximate different things in Neuroscience so for example the ventral stream of the visual system can be     
50:51     
characterized by a convolutional neural network or a dorsal stream of the of the     
50:58     
uh visual system which is the one that goes over the top versus over the bottom of the U brain is 3D convolutional     
51:07     
neural metor uh hippocampus equals a Transformer so they view the Transformer     
51:13     
as being equivalent to the campus reinforcement learning is equivalent to the basil ganglia which is a series of     
51:20     
centers in the brain that you know model motivation and movement behavior and so forth wave Toc is equivalent to the     
51:28     
auditory system and then grid cells which are equivalent to some sort of     
51:34     
navigation model which we don't have defined yet so you can see there are a lot of     
51:39     
missing parts here and the point is is to bring those together uh either through learning or through kind of like     
51:47     
doing this course a number of times then there's theoretical AI so there's a lot of stuff sort of at the     
51:53     
boundary basic neuroscience and theoretical AI some of those include animats which are you know simulated uh     
52:01     
animal models uh plus brain AI hybrids so you have a lot of stuff from artificial life     
52:08     
and from the area but there's a formal area called animats but it's basically artificial life stuff biologically     
52:16     
plausible back propagation so this is where we take Concepts like back propagation and try to find more     
52:22     
biologically plausible mechanisms adversarial attack X theoretical deep learning those are all     
52:29     
kind of on the boundary and then theoretical AI being pure theoretical AI     
52:35     
which is where we use neurot tools to investigate dnns and so this is where we look at     
52:42     
things like circuits psychophysics preferred stimul and then geometric deep learning     
52:48     
sort of exists at this boundary between applied Ai and theoretical Ai and there     
52:54     
are a whole bunch of things a graph neural networks I think would be there too where you go between like kind of     
53:00     
theoretical AI implied AI where you're looking at the networks and actually if you wanted to throw some neuros science     
53:06     
into that you could say okay we have graph neural networks that represent like the     
53:13     
connectome and that graph neural network has like this theor theoretical aspect and this applied aspect where we're     
53:19     
actually trying to get some results out of it so that's sort of like this the     
53:24     
neural developmental programs that um Pocky is working with in D.A one we have     
53:30     
these networks that grow these networks that you can describe theoretically using some sort of network model and     
53:37     
then you want to actually Implement them and get them to process     
53:42     
information so that's what we've got we've got this really interesting curriculum we've got this really     
53:49     
interesting set of questions and that's what's going to be happening the next two weeks online and     
53:56     
so so I'm going to be mentoring for the projects part of that I don't know what the projects are the students have to     
54:01     
come up with them but I think you know we should revisit some of these topics especially with respect to some of our     
54:09     
interests it's a nice way to kind of bring nrow AI you know into     
54:19     
Focus there any questions or comments about that     
54:29     
It looks interesting I actually was I forget if I no I don't think I ever officially     
54:36     
signed up for this one but neuro AI I think is the one I didn't I haven't done yet yes and um had     
54:45     
I known that my summer would be the way it is I might have tried to do it but it does seem like I'd be really interested     
54:50     
to see what what they do in the two weeks you know n is always are think all the courses are two     
54:57     
weeks now right I think when I did it originally it was three weeks yeah I think they may be shifting but this is     
55:03     
actually a new course so they might just try to be you know trying to condense this one down a little     
55:09     
bit um it's a ton of stuff as always whatever whatever yeah two or three weeks doesn't really matter it's like     
55:15     
gonna be you know not enough um but there still good um if I'm feeling     
55:22     
ambitious I might try to do mirror a little bit of the things so at least some of the content myself I probably     
55:29     
won't because I'm trying to focus on all this stuff I will be a mentor but not     
55:36     
the way Bradley is I'm like a per like the personal the career development     
55:43     
Mentor um I signed up for that because I kept had some calls for that so I'm     
55:48     
looking forward to that too I I I'm very limited I there were like a whole set of check boxes like are you and you know     
55:54     
are you like in a PhD are you are you are you a neuroscience person so I'm very much not     
56:02     
many of those things but apparently I was enough of this the one that there some people at me and dispense you know     
56:09     
um be being in arrena to talk about some managing career stuff orbe industry     
56:16     
versus Academia or some other other kind of mentoring things that we we be talked about here some time so that to be fun I     
56:23     
think I think my first it's like I it's like two I don't even know     
56:30     
um maybe somewhere between an half an hour and an hour sessions at some point     
56:35     
um that I'll be doing just just small things but I have a few ideas in mind     
56:41     
and just conversations but I think is really cool like I     
56:46     
I what I I I'm a big fan of neurom match I'm always curious to see     
56:53     
what in some ways I missed the early days where people were a little [Music]     
56:59     
more trying to things are more structured now and     
57:04     
they could do more and that's great but I also like I like the conferences and things like that too and that was such a     
57:11     
fun time in spite of the you know pandemic Horrors but um I'm really     
57:17     
curious to see what what this latest R of things is in one things I always liked about them is they     
57:23     
always I think Nur match is a great spot to see what are people who are trying to push a lot of things forward in Academia     
57:29     
doing and how are they being aware of the needs for mentoring and     
57:35     
communication and trying to do some of the I guess synthesis work that I'm always kind of Haring on L I feel like     
57:41     
they're actually they're endeavoring to do it so let's see how it goes this time and curious     
57:48     
um you know what what Bradley and and I and others will think after this this     
57:54     
this summer session goes let see yeah I think that it'll be great     
58:01     
uh no I would like to revisit some of these too and as you saw from the     
58:06     
syllabus you know they don't they don't have a really strong focus on bio inspired models and kind of put some     
58:12     
things on there and there's a lot more you could put on that list of things you know that that are sort of potential     
58:19     
models and how they relate to you know because I mean there are a lot of Bio     
58:25     
inspired models a lot of the modeling that isn't like sort of conventional AI or reinforcement     
58:32     
learning you know deep learning so that'd be interesting as     
58:38     
well it's okay so let me share my screen     
58:51     
again okay so I don't know if we've gone over this this is an interesting paper I     
58:57     
ran across um this is from 2020 and it's interesting from the     
59:03     
standpoint of what we've been talking about with sort of the history of machine learning history of like     
59:11     
cybernetics and computational methods so this is by Alexander fradkov this is the     
59:17     
early history of machine learning and so this is a kind of a take on the history     
59:23     
of machine learning and uh his his way of kind of summarizing this so the     
59:30     
abstract reads machine learning belongs to the crossroads of cybernetics control science in parentheses and computer     
59:38     
science so this is a guess his take on where machine learning belongs in that     
59:44     
resides in that area that area of intellectual history so you have     
59:49     
cybernetics which of course is control science control theory but also kind of its own thing and computer science so     
59:56     
it's a great there no mention of course of uh statistics or Neuroscience but     
1:00:02     
that's you know maybe another paper it is attracting recently an overwhelming     
1:00:08     
interest both professionals and the general public that's in it in the in the talk     
1:00:14     
of a brief overview of the historical development of the machine learning field with a focus on the development of     
1:00:19     
mathematical apparatus in the first decades is provided so he talks about sort of the mathematics     
1:00:26     
and some of these early aspects of machine learning a number of little unone facts published and hardto reach     
1:00:33     
sources are presented so means we're going to get some pretty obscure stuff     
1:00:40     
in so talks about how there's a connection between cybernetics and     
1:00:46     
computer science or control science and computer science or you know there is     
1:00:52     
this connection whatever it is um and and so you know it kind of talks a     
1:00:58     
little bit about that let's talk about the early years so the origin of machine learning and its modern sens is usually     
1:01:05     
associated with the name of the psychologist Frank Frank Rosen blat from Cornell based on ideas about the work of     
1:01:11     
the human nervous system created a group that built a machine for recognizing the letters of the     
1:01:17     
alphabet the machine called the perceptron by its creator used both analog and discrete signals and included     
1:01:24     
a threshold element that converted analog signals and discreet ones so this is you know kind of cybernetics but also     
1:01:33     
you know you could say cognitive science I guess um but that that's not brought     
1:01:38     
up here but U also computer science because it it is a computational     
1:01:44     
tool it became the Prototype of the modern artificial moral Network the model of its learning is close to the     
1:01:50     
models of animal and human learning developed in Psychology so parallel to     
1:01:55     
of the perceptron of course we had a lot of behaviorism and a lot of experiments in     
1:02:01     
that realm but also sort of the dawning of cognitive science um we we you know     
1:02:08     
kind of interested more in cognitive functions rather than stimulus response     
1:02:14     
but in any case you know that's kind of where we are that uh Rosen blad himself     
1:02:20     
performed the first mathematical studies of perceptron and so in then the early 60s     
1:02:25     
several groups are engaged in the design and testing of earning recognition systems uh General statements of pattern     
1:02:33     
recognition problems are proposed in the 60s um this kind of talks a little bit     
1:02:40     
about this uh in the papers vapnik and learner vapnik and shanus and     
1:02:50     
Y kubovich deterministic approaches were developed one AER probabilistic     
1:02:56     
statement of machine learning problems were suggested so you know in the cybernetics     
1:03:02     
paper that we're working on we have uh there yeah there is this really early     
1:03:08     
history of like statistics and statistical models and you know different ways of     
1:03:14     
kind of modeling problems um you know as probabilistic     
1:03:20     
state machines and uh you know Marian systems     
1:03:25     
and semi morovian systems and those are all kind of part of that history of cybernetics it's really kind of an     
1:03:31     
interesting area I mean you can really do a deep dive into statistics but there is this aspect of you know how these     
1:03:38     
statistical models are being developed alongside what cybernetics is doing and then also alongside computer     
1:03:47     
science and so he gets into this mathematical formalism about developed algorithms of     
1:03:54     
learning and so this kind of gets into this very simple I guess it's he says     
1:03:59     
it's a simple example let's see so let each object or image which is if this     
1:04:04     
set XK uh shown to the machine being encoded by a set of real numbers so you expose     
1:04:12     
the machine to a set of images you encode them as real numbers and it's     
1:04:17     
part of the set XK and it is required to train the machine to recognize which one     
1:04:22     
of two classes A or B this new object belongs and so you basically take XK     
1:04:28     
which are these images you train the machine on them so that they can recognize any images part of one of two     
1:04:35     
classes and you define the classes through a label and then let for certainty XK     
1:04:41     
belongs to class A for K12 Ka a and Class B for K A + one n so basically you     
1:04:49     
have K and KB is the class members that is the objects of a     
1:04:54     
membership fun function YX such as yxk so now each object in those classes     
1:05:01     
have a membership function which is uh you know I guess it's it's some probability it sounds kind of like uh     
1:05:09     
fuzzy uh control theory or fuzzy learning or whatever because in fuzzy     
1:05:14     
learning you have this membership function that kind of determines the degree of membership in a in a     
1:05:21     
class uh is assume that for convex system     
1:05:27     
uh XK belonging different classes do not intersect so for your standard convex uh     
1:05:34     
uh system where you have convex relationships classes don't overlap     
1:05:41     
that's kind of the basis of machine learning can you can build a discriminative model convex system which     
1:05:46     
is a system where a normal degree of variation uh and things uh you know are     
1:05:53     
sort of normally distributed you can typically treat two classes as being     
1:05:59     
separable and you can do that using uh sort of a discriminative model where you can basically draw a line and there's no     
1:06:06     
overlap between the two classes you can separate the classes with that line so     
1:06:11     
again uh different sets do not intersect these sets can be separated by a hyper     
1:06:17     
plane which is this line that you're drawing and that's the logic of sort of a separable or discriminative     
1:06:24     
model mean there a vector of Weights W uh WT and a number wo such as that     
1:06:34     
wxk so that's where you have wxk here which is greater than zero uh thus the     
1:06:40     
problem is reduced to the approximation of the function based on its values of a finite set so basically you're trying to     
1:06:48     
approximate a finite set you're trying to find a function that fits that finite set and that's separate from any other     
1:06:56     
set that you could build so you have these basically these classes that are sets they have some Affinity to each     
1:07:02     
other they're separable from other sets and that's what you're trying to achieve with this method gradient type     
1:07:10     
algorithms for perceptron learning construct some of the separating hyperplanes so you can do this with     
1:07:16     
perceptron learning you can you know these are gradient type algorithms and     
1:07:21     
this is the result shown in these papers here and so this can be done in a finite     
1:07:28     
number of steps so we should expect to be able to find this solution and so this is interesting also with respect to     
1:07:34     
cybernetics because cybernetics would be more accurately control theory but cybernetics     
1:07:41     
suggests that you know there's some form of data that there's some form of learning and the learning of course     
1:07:47     
takes this form of finding sort of structure in the data set and so we     
1:07:53     
don't really think of this connection between statis ICS and cybernetics too much but when we do you know it's     
1:07:59     
usually this sort of thing and we actually attribute this more to control theory but it's interesting that     
1:08:05     
cybernautics also sort of has this aspect to it where you know you're talking about Good regulation and you're     
1:08:11     
talking about you know feedback being sort of the you know mechanism that allows you     
1:08:19     
to uh find good regulation so there's this connection there too um     
1:08:26     
and so it kind of goes through some tech more technical stuff here uh another way to another approach     
1:08:34     
to is to choose a vector of Weights W in such a way that the corresponding hyper     
1:08:39     
plane is a supporting hyperplane to the convex Hall of the available set of vectors so that the minimum distance     
1:08:46     
from it to the convex Hall of classes is maximum so this is the idea behind the support Vector machine method and so you     
1:08:54     
know the a lot of times papers will go through sort of history and figure out     
1:08:59     
if they can attribute to one person or another person and uh that's what they're doing     
1:09:05     
here uh it is a historical fact however that in the same year 1964 an employee of the laboratory of     
1:09:12     
theoretical cybernetics of LSU orus kinet published in a fair fairly simple     
1:09:19     
recurrent algorithm converging to an optimal supporting hyperplane and that's in cinet 4 so this     
1:09:26     
is like you know stuff from the Soviet Union that we probably are in the west     
1:09:31     
aren't aware of as much but this is what people were working on sort of in parallel so there's a cybernetics aspect     
1:09:39     
to this sport Vector machine is what he's saying uh there other you know     
1:09:45     
algorithms that you know have historical roots in cybernetics it's you know it's really     
1:09:50     
interesting how some of these things kind of dovetail talks about stochastic     
1:09:56     
approaches where you have this uh so the above mentioned methods are based upon     
1:10:01     
deterministic approaches or an uncertainty is modeled as element of a bounded set so in other words you have a     
1:10:09     
a bounded set it's separable and then there's this uncertainty so you know how     
1:10:14     
wide is the uncertainty you know how how much you know this relates to the     
1:10:20     
membership function so how certain are we that this these data points belong to     
1:10:25     
these different sets how act how competent are we in our     
1:10:31     
classification uh so then talks kind of about how uh this whole deterministic     
1:10:38     
approach is sort of the sort of the the standard in the field but many papers     
1:10:44     
are devoted to uh the statistical approach where you can go back to some     
1:10:49     
of the roots of decision Theory and Communications which of course is the     
1:10:55     
sort of the roots also of control theory the most systematic framework     
1:11:00     
based on average risk and minimization was developed by this person here was     
1:11:05     
brilliantly demonstrated that it encompasses a large number of algorithms proposed by different authors in special     
1:11:11     
cases so there's this connection between thinking about uh machine learning as a     
1:11:18     
deterministic approach but also as a stochastic approach uh kind of going over the roots     
1:11:24     
of machine learning and sort of communications as we know that's linked     
1:11:30     
to cybernetics decision Theory which is somewhat linked to cybernetics and then     
1:11:35     
things like Risk minimization and optimization and you see this in operations research and other areas so     
1:11:42     
there are Links of course between early optimiz operations research and cybernetics as     
1:11:49     
well um so this kind of talks a bit about like stochastic approximation     
1:11:55     
uh and then you have these uh so choosing the cost function for     
1:12:00     
stochastic optimization in an appropriate way allowed the author to design different classes of algorithms     
1:12:06     
described previously in the literature as well as a number of new ones convergence of the algorithms can be     
1:12:12     
established based on the stochastic approximation scheme under conditions of convexity and bounded growth JW which is     
1:12:20     
part of this this is the target of the stochastic uh search so this unifying     
1:12:27     
idea here is in the formulation of the adaptation and learning problem in terms of a performance index which is     
1:12:35     
JW uh which is the average of the cost function x uh qxw so you have this cost     
1:12:42     
function qxw you have this uh term here at the     
1:12:48     
end and then you are basically integrating over X to find this uh     
1:12:55     
performance index which is an average of the cost function uh so that's and then the     
1:13:01     
problem is formulated by finding the minimal value of JW and then the solution for that is based on these     
1:13:08     
stochastic gradient algorithms which gives you the stochastic aspect of it um     
1:13:13     
and so that's that's what they're getting in here so that's that's an set you know     
1:13:20     
another set of interesting results from early um machine learning and     
1:13:26     
cybernetics and operations research then he gets into this about     
1:13:32     
the two winners of AI and the second boom of machine learning so the first I     
1:13:37     
guess boom of machine learning was back when you know a lot of cybernetics and     
1:13:42     
operations research was kesing decision Theory and then this other part here     
1:13:48     
talks about um the winters of AI and the second gr of machine learning so     
1:13:55     
in 1969 the book of Minsky and paper was published were some limitations for complexity of the problem that can be     
1:14:02     
solved by perceptrons were established namely the authors emphasized that perceptrons cannot represent some     
1:14:09     
logical functions like XO or not XO so     
1:14:15     
this is where we have exclusive War not exclusive or so basically the idea is     
1:14:20     
that perceptrons can produce uh outputs they can classify things     
1:14:25     
but they can't represent logical functions and so this is a problem in computer science because we want things     
1:14:31     
to be able to represent different logical functions and so perceptrons can't do that you know they can't I     
1:14:39     
guess the implication is they can't reason although I think the the shorter term implication is that they're not     
1:14:46     
computationally useful in the ways would like so this is uh you know where this     
1:14:53     
kind of triggered the first AI winter so this is where you went from 1969 to     
1:15:00     
around the 1980s middle of the 1980s maybe where perceptrons kind of died or     
1:15:07     
got put put on the back burner and nothing was really done so of course we     
1:15:12     
know that perceptrons are sort of the basis maybe a couple steps removed from Deep learning and so you know that was     
1:15:20     
kind of put away because we couldn't figure out how these uh models uh could     
1:15:27     
produce these kind of results um this trigger reduction of     
1:15:33     
funding and AI research in the world for more than two decades this period was later called the first winter of     
1:15:40     
AI uh but there was still work being done on complex learning algorithms of     
1:15:46     
different types just not perceptions okay so but in the during     
1:15:52     
this winter there was a lot of work being done so maybe the term winter is misapplied because we got a lot of     
1:15:59     
different types of multi-layer neural networks for example that were being investigated at this     
1:16:04     
time so in 1980 Fukushima proposed a hierarchical multi-layered convolutional     
1:16:11     
neural network known as the neocognitron and this is something that is I think um     
1:16:17     
probably not well known to maybe a lot of people but it's an important Advance     
1:16:24     
because it's not only is it you know an technical advance but it happened during the CI     
1:16:30     
winter a significant impact was made by the invention of back propagation learning algorithms and this was done by     
1:16:37     
several authors in the mid 80s so rumel Hart for example uh although the initial ideas     
1:16:43     
were proposed still in the early 1960s so there were some sort of results that     
1:16:49     
pointed us towards back propagation as early as the 1960s but in 1986     
1:16:55     
this was proposed uh in this paper by rart at all uh compared to a standard gradient     
1:17:02     
desent approach which we talked about where you have the stochastic search and then you're trying to find a performance     
1:17:09     
metric the standard radiant descent approach updates all the parameters with respect to error     
1:17:15     
simultaneously back propagation first propagates the error term at the output layer and then sends it back to the     
1:17:22     
layer at which the parameters need to be updated and then a chain rule is used to update     
1:17:28     
parameters with respect to the propagated area so this is something where you have     
1:17:33     
basically a feedback mechanism between the output layer and the inner layers     
1:17:39     
where you have these need you know we need to make corrections to the     
1:17:45     
model um so you know prop back propagation had some     
1:17:50     
drawbacks sometimes it fails in cases where the classes cannot be linearly separated this is what we were talking     
1:17:56     
about before these discriminant models where you have these two neatly     
1:18:01     
separated classes and this is in a linear sense so you know this is where this is sort of     
1:18:08     
the easiest class of problems where you can separate those two classes and in     
1:18:13     
cases where you can linearly separate your problem it you know back propagation is easy but in cases where     
1:18:20     
you can't that's most real world data then back propagation it has     
1:18:26     
problems so this is of course all sort of during this AI winter and I guess     
1:18:31     
back propagation maybe kicked off a new wave of research because this is about     
1:18:37     
the time when the first AI winter came to a close and people started to do get     
1:18:44     
funded again um so the successive back     
1:18:49     
propagation other computational advances produce great hope for future successes     
1:18:54     
over real successes were less than expected ones and the investments in the area machine learning decreased again     
1:19:00     
until the early 1990s so this is where you know back propagation was kind of seen as a hope     
1:19:07     
but it wasn't exactly the hope that they needed and we really had to wait until the early     
1:19:13     
1990s to get really a good you know uh     
1:19:19     
robust uh restart on on AI funding so this was like I guess in the the mid 80s     
1:19:25     
there was some excitement and then research or research funding declined until sometime in the early 1990s so     
1:19:32     
this is the second Wier of AI um nevertheless an interest in     
1:19:37     
studying neural networks is an instrument a machine learning was growing in the 1990s so this is where we get this     
1:19:44     
breakthrough the Cortez andn paper uh this is where we get the new version of     
1:19:50     
our uh svm algorithm and support Vector machine algorithm but the general non-separable     
1:19:58     
case so this is where we not don't worry about the separable case we worry about     
1:20:03     
the non-separable case or where our categories over that and so this is     
1:20:09     
where this was introduced and under the name support Vector networks so support     
1:20:15     
Vector networks instead of support Vector machine and this is something that led to sort of this advance in being able to     
1:20:23     
analyze data that's not sort of almost like a toy model of thata something that's not completely convex that has     
1:20:30     
some you know non-convex aspects to it it has been widely used and been shown     
1:20:36     
to be effective in practice the algorithm and its theory has had a profound impact on theoretic one applied     
1:20:41     
machine learning and inspired research on a variety of     
1:20:46     
topics um then there's this gold rusher machine learning so this is where we're     
1:20:52     
now seeing this this sort of or at least we saw this gold Russia machine learning in the late 90s and     
1:20:59     
2000s where we're getting sort of these different Trends converging     
1:21:05     
so three synchronous Trends emerged at the same time uh the first trend is Big Data so     
1:21:13     
you know having a lot of data available so you can train the models the amount of data became so big     
1:21:19     
that new approaches were brought to life by practical necessity rather than the Curiosity of scientists     
1:21:25     
the second trend is to reduce the cost of parallel Computing in memory so this     
1:21:31     
uh you know kind of developed with the hardware but also with the software so     
1:21:36     
Google unveiled its map reduce technology in 2004 which is of course software uh followed by its open source     
1:21:43     
counter per Hadoop in 2006 and together they made it possible to distribute the processing of huge     
1:21:50     
amounts of data between processors so nid     
1:21:55     
a little bit later came out with their gpus and so this is the hardware aspect where the gpus were um you know allowed     
1:22:03     
for this type of uh better processing to happen so uh and at the same time the     
1:22:10     
cost of ram significantly decreased which opened the possibility working with large amounts of data in     
1:22:16     
memory and we ended up with new data types of databases including no SQL and     
1:22:22     
so in 2014 then the a spark framework for distributed processing up     
1:22:27     
unstructured and weekly structured data appeared so all these things kind of improved our ability to run these models     
1:22:35     
and to develop new models and modified models from this basic uh sort of set of     
1:22:41     
approaches that were sort of the subject of the last few decades before this time     
1:22:47     
so this is from about 2004 maybe the late 90s to the mid2 T so it's and then     
1:22:56     
the third trend is deep machine learning so this inherits the idea of the     
1:23:02     
perceptron and as the author says in combination with the successful     
1:23:07     
scientific PR campaign so that means that there was this now they had to     
1:23:13     
resell the idea of the perceptron to people but they also developed these deep neural network architectures which     
1:23:19     
took the perceptron and built uh you know more sophisticated basis for it     
1:23:25     
It is believed that the term deep learning was proposed in 1986 by Reena dector dector although the     
1:23:32     
history of its appearance is apparently more complicated a detailed and preferably     
1:23:37     
objective analysis of the events of this period is still waiting for its researcher different points of view and     
1:23:43     
the Deep learning uh controversy of the naming can be found in different     
1:23:49     
places so and then it talks about Jeff Hinton     
1:23:54     
um and so by the middle of the last decade a critical mass of knowledge in the field of DNN was accumulated deep     
1:24:02     
neural networks and as always in such cases someone is broken away from the pelaton that's like a bike breaking away     
1:24:08     
from the pack in this case the leader was Jeff Hinton a British scientist uh in 2006 they published a     
1:24:16     
number of articles on deep neural networks in nature and so this is where you know kind of talk about like uh The     
1:24:24     
Hinton approach I guess and so how there was a whole there were a whole bunch of     
1:24:30     
people kind of came out of that tradition and develop deep learning into the Juggernaut that it is today and so     
1:24:37     
this then this article ends with the discussion kind of recapping this history and this paper is an attempt to     
1:24:45     
present a general control related view of the issue in the historical perspective and kind of giving us a view     
1:24:52     
into sort of those first decades to AI Winters and how that all relates to the     
1:24:58     
development of machine learning going forward and so this is uh you know     
1:25:04     
really kind of an interesting paper I think it has a lot of themes related to control theory cybernetics statistics     
1:25:13     
and then these AI Winters and all the trends associated with that Jesse says     
1:25:18     
by practical necessity rather than curiosity is an interesting line yeah     
1:25:23     
it's interesting it's you know it's kind of like saying that you're doing something for Applied purposes and     
1:25:28     
that's bad instead of just like kind of coming to it through peer research     
1:25:35     
but um yeah did you po I definitely so I     
1:25:41     
I want to long-term aim like I want to make a hub somewhere I don't know if     
1:25:46     
it'll be like a full working group but I want to basically have a you know a a space concentrated for     
1:25:53     
history of science stuff and cybernetics and stuff specifically so I really like to see the paper and put it in there um     
1:26:01     
but I didn't know that I want to get back to the the Practical quote but also did is that     
1:26:09     
paper did you say put the paper in cybernetics or was that somewhere else I'm not sure I posted it in cybernetics     
1:26:15     
I could post it in cybernetics I probably wherever you want yeah but I'd be curious to see the paper because I     
1:26:21     
feel like that's do really that's a it wasn't extremely like the pasting of the     
1:26:28     
paper as an overview for what happened looked really good you know like compared to other papers that kind of tangent like it seemed like a nice it's     
1:26:35     
pretty tight it wasn't too long I missed something but it looked like it it was a     
1:26:40     
nice like it feels like something I could very put in a curriculum for history of AI cybernetics or like like     
1:26:47     
like to fit to fit that that kind of a very specific need that we come     
1:26:53     
across many angles so it was a good I'd like every every want to keep track of that all right yeah I'll post it um yeah     
1:27:01     
I think it was uh it a little hard I don't know if it was translated from Russian or something it was a little yeah hard to follow but it basically he     
1:27:08     
did get into the and he got into the statistical like kind of mathematical     
1:27:14     
minutia which is good it's useful but I mean for like this kind of paper I'd     
1:27:19     
like to break that out and say like you know what is it about because I mean you get into this technical argument oh     
1:27:25     
reason we're doing the technical argument is because we're going to talk about sort of the difficulty of doing     
1:27:33     
like non-convex stuff like you can talk about the very simple case where everything is very well defined and I     
1:27:41     
can just build classes from you know a prediction and it's easy to do but the     
1:27:47     
whole point that he's trying to make is that these AI Winters were because the     
1:27:54     
models that were built were not very robust and so people like when they     
1:27:59     
started to look into them a little bit more they're like well they're not really that good we need to kind of come     
1:28:05     
up with better models and so hyp seeded at that point so yeah and even even like     
1:28:13     
like in in my degree program like we come across like you're taught you know like svm and you're taught these things     
1:28:19     
that are like but but it's a nice it's a nice little context for like where     
1:28:25     
even even in their like potentially initial where where did they come up in     
1:28:30     
it and I always feel like that's nice and then like going zooming all the way out again like exactly what you said     
1:28:37     
about like why why were there winners and what like also that there was stuff going on in the background still in the     
1:28:43     
winters too that weren't necessarily famous or popularized but then what were the things that kind of let it let them     
1:28:50     
come out now and and even to Circle back to the all these different points I want make     
1:28:56     
but the Practical by practical necessity rather than curiosity I think it's a really interesting line just to think     
1:29:02     
about in terms of not not I don't I don't I don't I guess it's easy to say     
1:29:07     
what I don't mean I don't mean like oh yeah like there was this Golden Era where not like like like the meme like     
1:29:13     
the meme about about the nature the guy publishing a nature the guy trying to understand nature I don't want to I     
1:29:19     
don't I don't feel like that's that's not what I'm trying to say just to make like I don't think it's oh well it's     
1:29:25     
only the the pure wondrous curiosity of science that let us down the golden path not so much that but more realizing     
1:29:32     
there was practical necessity to okay deal with the data and things there but     
1:29:38     
also now like it's really weird for me to to compare that to now because in the     
1:29:43     
moment we're in now there's a very different well maybe not very different but it's what is practical and what is     
1:29:51     
curiosity driven or what is sort of investig ation driven is very um     
1:29:57     
different the forces that play are the same but also different so there's     
1:30:02     
there's more there but yeah it interesting it really interesting paper so thank you for going through that yeah     
1:30:07     
no problem yeah I thought that was an interesting it's that kind of approach to the history of this and of course you     
1:30:14     
can Define the history in different ways right like history is exactly but um oh     
1:30:20     
go ahead that's kind of the point of I don't know if I'm going to make it like I I don't know what to do with this     
1:30:26     
yet but like I really want there to be a     
1:30:32     
um a I don't know what to call it right a group of where these exactly what you     
1:30:41     
were just getting like you could Define history in different ways like I'm I'm much less interested in defining having     
1:30:47     
a definition or a a a uh static     
1:30:54     
take I don't want to say here is the history of blank whether it's cognitive science which is sort     
1:31:01     
of I don't know I feel like I can't say the term the term always says it's like am I     
1:31:08     
am I being like ironic when I use the term or do I mean it you know or or like     
1:31:14     
psychology is different the DSM and Psychiatry is kind of its own thing and     
1:31:20     
then like AI or computer science or machine learning and I don't like I'm     
1:31:25     
not interested in uh making it the definitive history but much more what are the lenses to use and like like this     
1:31:32     
place to to discuss one of those perspectives um for that for their     
1:31:37     
development other other like understanding like there sort of like the langu like a language or or just a     
1:31:43     
space to talk about how do you have some fluency in the     
1:31:51     
uh like I don't know the the classes of discussion that take place about these     
1:32:00     
things um so I don't think this paper is like a definitive uh you know the definitive     
1:32:06     
history but but I think it's a great example or as a nice or a useful example     
1:32:11     
of one like one lens um from it and when     
1:32:16     
you take from that vantage point or use that lens here's what you can you know here's here's a take so yeah     
1:32:25     
yeah also thought the sort of discussion about cybernetics was interesting now     
1:32:30     
you know cybernetics like you didn't talk about weer and all these other people and in fact there's a whole     
1:32:36     
history of like Russian and Chinese cybernetics we've talked about this that are kind of separate from like the     
1:32:42     
Western whatever's going on in the west at the time but the standard you know the ratio club or whatever approach but     
1:32:50     
it's interesting how there are these themes in cybernetics that kind of are part of that you know like there's this     
1:32:57     
interest in Good regulation there's this interest in very large     
1:33:02     
systems like uh Gordon pasque and and some of these organizational oriented     
1:33:08     
types that they're looking at these very big systems they're you know or even uh     
1:33:15     
ashb very big systems they don't have the data to like do anything with it yet     
1:33:21     
but they're building these models so like you know with cyber you're building a model of an industrial National     
1:33:28     
economy and you're building this sort of infastructure for like monitoring     
1:33:34     
everything but the data didn't really exist yet it was kind of like we didn't have the way to collect the data that     
1:33:41     
would give you the type of feedback that you wanted you didn't have that kind of continuous monitoring nowadays you may     
1:33:47     
have that actually we're getting close I think to some sort of a system where you could monitor you know and they do this     
1:33:53     
of traffic or they can do with traffic in urban areas where you can monitor     
1:33:59     
traffic flows you can get the data and then you can make up decisions sort of in real time so you know those sorts of     
1:34:07     
it's almost like in cybernetics you had a lot of ideas that were ahead of their time in terms of getting the data in     
1:34:13     
place and you know that that's an interesting theme but also you had a lot of ideas that were sort of leading us to     
1:34:21     
this uh you know to what we would see in control theory later where you had much more formalized methods for     
1:34:28     
optimization or looking at feedback or looking at these other things so it's it's really interesting how uh     
1:34:35     
cybernetics fits into this I I just had this really curious     
1:34:41     
idea and I don't know how to formulate it but I'll ask and see if it leads to     
1:34:48     
anything um and I want to use the we just hit a really nice     
1:34:54     
comparison of cybernetics with sort of broader less defined things that were     
1:34:59     
maybe less formalized or hard to formalize and didn't have the data versus what would become control theory     
1:35:06     
which is much more formalized but also     
1:35:11     
more I don't know more something perhaps and so I guess I want to ask is there     
1:35:17     
any in your understanding of either I don't even want to say history of     
1:35:22     
science or just is there any language or way of     
1:35:28     
discussing like degree of     
1:35:35     
formalization that a field has achieved or is it I don't like not     
1:35:41     
saying there's a way to measure it but almost like I don't know if there's is it more is it I guess maybe     
1:35:50     
something another way to say it is is there anything about a     
1:35:56     
field you could say maturing and formalizing more but also some Fields     
1:36:02     
just have different takes on what formalization is or how much it can be like precisely formalized some things     
1:36:09     
are much some things have to be more zoomed out and maybe less formalized is there any any ways to talk about that     
1:36:15     
that you can think of or just um I mean I don't know it's like uh I guess you     
1:36:21     
could say you could first of all Define what formalization is for a certain field like or you know there are     
1:36:27     
different ways you can formalize something so one is their terminology the terminology is very     
1:36:33     
constrained if you have like really standardized terms across different papers and you can do this using like     
1:36:39     
literature mining where you say okay this term is used and this term is used six different ways versus one one way     
1:36:46     
and if those terms are consistent across the papers I mean that might be more sort of methodologically mature Rous     
1:36:54     
because everyone's operating from the same place and sometimes that's not like     
1:37:00     
the fault of the field sometimes it's just that the field is very diverse but     
1:37:06     
you know you can they use that as an indicator another of course is mathematical formalization so do you     
1:37:12     
have a mathematical formalization for something and sometimes it's just a matter of like people have an     
1:37:19     
intuition and but they don't know how to formalize the math sometimes you have like rules or laws     
1:37:26     
that you might want to uh Implement in sometimes you have to discover those laws so it takes time so that's another     
1:37:33     
form of uh formalization is sort of the mathematics or if you have rules or laws     
1:37:38     
for things and you can just use that by looking at what the you know the State     
1:37:44     
ofthe art is the models that you have and then you know a third kind of way of     
1:37:49     
looking at maturity is kind of like um you know like maybe if you can Implement     
1:37:54     
something in codee so a lot of uh things in cognitive science you know one of the     
1:38:01     
goals of uh cognitive models is like can you take like a cognitive process like     
1:38:07     
attention or memory can you implement it in code and then simulate it so that you     
1:38:14     
can say yeah this is basically how it works we can do a simulation gives us a     
1:38:19     
lot of information about like what we can't directly observe and it predicts the data well so that's     
1:38:27     
another way for this sort of modeling and simulation looking at the     
1:38:32     
code and you know sometimes uh you know other ways you can look at it are     
1:38:37     
looking using something like algorithmic information Theory or something like that where you look at     
1:38:45     
how and this is mainly I think in computer science we're going to look at how efficient the code is so if you have     
1:38:51     
like something that's uh implemented in code how efficient is that code     
1:38:57     
sometimes you know it's like you can describe something in code but it's very you know it's very hard to understand     
1:39:04     
structure of it so you just kind of break code until you get something to work um but sometimes you know getting     
1:39:11     
it down to a short description length is good too you know getting it into few     
1:39:17     
lines of C so there are a lot of ways you can look at like the sort of maturity and the formalization of of a     
1:39:23     
field you know and again you know even like the number of papers published although that's not a great indicator     
1:39:30     
might also be a good thing because you can get a sense of like how much people really kind of explore a topic if it's     
1:39:37     
like four papers it's probably not been explored too much but if it's like 2,000     
1:39:44     
papers that just might mean they're going off in different directions it's really not a good indicator but that's where reviews come     
1:39:50     
in so reviews will kind of summarize the state of things and then you get this sense of how mature feel     
1:39:58     
is okay thank you for that yeah problem I don't know Morgan had     
1:40:04     
something to say he turned his camera on so just just just getting settled back     
1:40:10     
in the house so okay yeah     
1:40:15     
great my phone was about to die oh yeah so yeah back back home great     
1:40:24     
all right so I'm going to finish up here and I thought that was a great discussion I'm glad that I brought that paper up today um I have another sort of     
1:40:34     
article and this kind of relates back to neuro AI a bit this is from The Siam news so this     
1:40:41     
is the society for Applied Mathematics and they usually have some pretty good stuff here um this is something called     
1:40:49     
the geometry of the neurom manifold so this is an article uh research     
1:40:57     
article by kathline con and uh this is U about this idea of     
1:41:04     
a neurom manifold so this is going to talk about neural networks for septons this sort of     
1:41:11     
stuff so starts out machine learning with neural networks Works quite well for a variety of     
1:41:17     
applications even though the underlying optimization problems are highly non-convex so we come to this term again     
1:41:24     
convexity so convexity is you know where if you think about like a     
1:41:30     
surface you know it's kind of like a smooth un you know a smooth curved     
1:41:35     
surface that's a convex surface and convex surfaces represent problems that     
1:41:41     
have sort of a normal distribution you know they're kind of like normalized that uh they're you know they     
1:41:49     
have all the properties of sort of IND distribution stuff so it's like you know     
1:41:55     
if I use a standard statistical model I can describe my data pretty well now when they say nonconvex or     
1:42:02     
highly non-convex that means that it's not that sort of smooth curved smoothly curved     
1:42:08     
surface you can't represent things on its smoothly curved surface and say this is the mean and the mean describes     
1:42:14     
everything uh along its very you know every axis of its variation that you     
1:42:19     
know we can just summarize things of the mean and be done or sum things with a mean with a standard you know standard     
1:42:26     
deviation will be done it's it's much more like you know something like if you've ever heard of a rugged landscape     
1:42:33     
where you have maybe multiple mean points or you have Minima that are local     
1:42:39     
instead of global that's what they mean by non-convex and so they're going to have this distinction often times     
1:42:45     
especially when you get into statistics literature and convexity versus non-convexity and it has to do with that     
1:42:51     
sort of statistical distribution whether it's something you can describe say with an IND distribution gy in distribution     
1:42:59     
or like you know something that's going to have a lot of unique cases or outo distribution behaviors or these sort of     
1:43:06     
exponential distributions and those may or may not even describe the data well so there's a     
1:43:12     
lot to learn in that area um but in this case what she's     
1:43:18     
saying what they're saying is that machine Learning Works quite well for a variety of applications     
1:43:23     
even those that are highly known yet despite researchers attempts to understand this peculiar phenomena a     
1:43:30     
complete explanation does not yet exist so we don't know why machine learning     
1:43:36     
with neural networks Works in this way why it describes that you know different     
1:43:41     
types of data really well it's hard to say we don't really know why we just     
1:43:46     
know that you know if we do our machine learning uh use our machine learning techniques to get an answer     
1:43:53     
uh so there's a lot of like sort of theoretical work that needs to be done here but also theoretical work so a     
1:44:01     
comprehensive Theory will require interdisciplinary insights from all area of mathematics in particular and at its     
1:44:08     
core this phenomena is governed by geometry and its interplay with optimization so there's a lot of     
1:44:16     
interesting work being done in like geometric neural networks and in graph neural networks quite frankly because     
1:44:22     
graph neural network deal geometry quite explicitly you have a graph topology it     
1:44:27     
has a geometry and so that geometry then translates into the solution and this is     
1:44:34     
something again we don't know exactly the details of it but it's it's an interesting     
1:44:39     
era so the neurom manifold and this is the term that they're using here is a     
1:44:44     
key player in the optimization problem of training a neural network so take a fixed neural network     
1:44:51     
architecture and that FX neural network architecture parameterizes a set of     
1:44:56     
functions where in each choice of parameters gives rise to an individual     
1:45:02     
function so this is where we have on this image here a neural network without an activation function so we have these     
1:45:10     
uh inputs XY Z we have our parameters AB B CDE e which are the uh like from X to     
1:45:19     
this hidden layer uh neuron and then from the hidden layer neuron to the output neuron so A and D are different     
1:45:27     
parameters are just these different um paths through the     
1:45:32     
network so the parameters a b c d and e give rise to a linear function R3 R2 of     
1:45:39     
rank one Nam XYZ to this D ax b y c z so     
1:45:45     
you have this a Distributive as uh distributive aspect where you pair a     
1:45:52     
with x B with Y and C with zed and then you have um your D and E so you have D which     
1:46:00     
is distributed and then e which is distributed and so you have each of     
1:46:05     
these parameters which take in these different options so this is what they're talking about here     
1:46:11     
um so the set of functions is the neurom manifold of the network so this     
1:46:18     
representation here with this sort of distributivity is that the neurom manifold in the network for instance the     
1:46:25     
neurom manifold in figure one is a set of all linear Maps R3 to R2 with rank     
1:46:31     
one at most one so they have there's this uh mathematical representation you     
1:46:36     
can build of this neurom manifold the neurom manifold in the simple example is of all studied     
1:46:42     
algebraic variety but it suggests several tough questions for real life     
1:46:47     
networks and so those questions include the following number one what is the G     
1:46:52     
geometry of the neurom manifold when we represent it this way it only gives us a sort of algebraic description but it     
1:46:59     
doesn't give us any sort of like geometry or geometric information which is of course important if we want to map     
1:47:06     
this to a convex or a non-convex space you need to describe where this resides     
1:47:11     
in this manifold which the manifold is just the surface that describes the data so our     
1:47:18     
convexity would be this curve smooth curved surface that the describe where all the data sits on that surface and     
1:47:26     
then the surface describes variation of the data so you go from the mean at the peak of this uh convex bace to like     
1:47:33     
variation that goes down the sides and so we can describe it smoothly so we can describe all the points easy to     
1:47:39     
understand that geometry a non-convex surface might undulate quite a bit might have a lot of Peaks and valleys and it's     
1:47:46     
very hard to describe that just and reconstruct it just from from uh observing data     
1:47:53     
so that's that's the understanding so you know it's easy to interpolate the convex space but not easy to interpolate     
1:48:00     
the non okay so these these questions are     
1:48:05     
what is the geometry of the neurom manifold and two what does it look like so again you want know what the geometry     
1:48:11     
is but then what does it look like in sort of this um can we visualize it basically and then three how does the     
1:48:19     
geometry affect the training of the so if we know the GE ometry we still need     
1:48:24     
to know how it affects the training so we can optimize the training of the     
1:48:30     
network let us explore another simple example of geometry's ability to govern     
1:48:35     
optimization imagine the neurom manifold is a full dimensional subset of RN that     
1:48:41     
is closed in the usual ukian topology and that's in figure two which we'll get to in a supervised learning setting we     
1:48:48     
provide some training data that is represented as a point in the ambient space RN independent of the choice of     
1:48:55     
loss function to be minimized only two scenarios can occur during Network training so in this case in supervised     
1:49:03     
learning where we provide uh some training data that's represented in a     
1:49:08     
point in the space we have two scenarios that can occur during training so the     
1:49:14     
first is that if the data point is inside of the neurom manifold which is the thing that we're sort of trying to     
1:49:20     
predict trying to understand then that point is the global minimum or number     
1:49:26     
two if the data point is outside the neurom then the global minimum lies in     
1:49:31     
the neurom manifold found so it's basically like we can make a decision it says data point inside or outside the     
1:49:38     
neurom manifold and then is if if the data point is inside the neurom manifold     
1:49:43     
then it's the global minimum or if the data point is outside the neural manifold and theum neural manifold     
1:49:50     
boundary so this means that it's like either at the edge or it's the minimum these scenarios can have     
1:49:57     
practical consequences if we can test the point's membership in the neurom manold and we talked about membership in     
1:50:04     
the last paper where we talked about like if you have a data point that's in a set that is separable it has a degree     
1:50:12     
of membership or uh some membership Criterion right so you don't know for uh     
1:50:18     
sure maybe if it's part of one category or another but in this case you want to     
1:50:23     
know if it's part of a neurom manifold or not so if we can test the point's membership in the neurom manifold we can     
1:50:30     
hence reduce the number of optimization parameters by constructing a smaller Network it only parameterizes the     
1:50:37     
boundary of the original manifold so this is an interesting approach because     
1:50:43     
you know you have this ability to say whether that point is either on the     
1:50:48     
boundary of the manifold or the minimum and then what you can do is say okay if I add in points I can approximate that     
1:50:55     
minimum or that boundary more specifically so in either case you can     
1:51:01     
refine that surface of the manifold and so as you keep adding points you can     
1:51:07     
make assumptions about whether it's the minimum because remember you don't know what the the global Minima is you don't     
1:51:14     
know what the boundary looks like every time you add a point it gives you more information about it so that's what     
1:51:19     
you're trying to do and knowing like you know uh     
1:51:25     
something about that space or making some a priority judgments about that space and a while you to reconstruct     
1:51:31     
it so uh in this case the global minimum for data points inside the cylinder over     
1:51:37     
the dis will be in the relative interior of the neurom manifold and you can see this in figure     
1:51:43     
two where they show this sort of uh dark     
1:51:48     
orange space and this point is being added and you can have it in this volume which     
1:51:55     
is you know this lower dimensional subset which is this dark uh orange and     
1:52:02     
you can see the points being added it's either a minimum or it's on the boundary and you can eventually sample more and     
1:52:09     
more points and Sample more and more that spas and Def so this this kind of gives us some     
1:52:17     
information when we alter the loss function the cylinder changes its shape which can be challenged in to understand     
1:52:24     
so depending on our loss function we actually change the shape of this space     
1:52:29     
because our loss function is going to determine you know how this space looks     
1:52:35     
so we don't know kind of with gradient descent you know when you explore     
1:52:40     
gradient descent you evaluate the points you determine your loss function so the loss function is always moving towards     
1:52:46     
the minimum and so when we alter the loss function we alter the shape of this     
1:52:51     
unexplored manifold which is of course there's an interaction there and it's     
1:52:57     
kind of like in uh Fitness Landscapes when you alter the organisms always     
1:53:04     
alter their Fitness over time they're fit in one at one point in time and     
1:53:09     
they're more or less fit at other point in time and that can alter the fitness landscape uh topology which is a similar     
1:53:17     
thing here but every time you keep sampling points it changes the loss function the cylinder or the space the     
1:53:24     
manifold changes its shape and then it's just hard to really kind of get a handle on the true shape and maybe it is that     
1:53:32     
you know the manifold just kind of changes shape as you explore more and more of the data which is even more     
1:53:38     
depressing because it really makes it hard to approximate it and of course that's all we're trying we can do is     
1:53:44     
approximate the space we can't understand it in its totality so there are a number of interesting questions     
1:53:51     
there uh the boundary points of the neurom manifold are not the only interesting points that become more     
1:53:56     
exposed during training so we have the boundary points of course we have the     
1:54:02     
Minima but a neurom manifold often has singularities or the points at which it     
1:54:08     
does not look locally like a regular manifold so this is where you know you     
1:54:13     
have a manifold in its totality we're used to thinking about this in terms of embedding these points in a space and     
1:54:19     
knowing the space and knowing the space in Global Information locally but a lot     
1:54:24     
of times when you're at a certain point or especially when you like have only a     
1:54:30     
few points or when you have sparse data overall it's not going to give you     
1:54:37     
basically the full information about that manifold so when you're only at you know maybe a when you only have a few     
1:54:43     
points it's not going to look like the regular manifold it's going to look vastly     
1:54:49     
different and so you know this is something that's really interesting um so for instance imagine the neurom     
1:54:55     
manifold is the plane curve in figure three this is down here which has one Singularity a cusp which is defined I     
1:55:03     
think here at this uh bright red Point okay so we have this cusp     
1:55:12     
Point uh figure three always States the data points whose Global minimum is the cusp during the minimization of the     
1:55:18     
equan distance this set of data points is a positive measure which means the cusp is the global minimum with a     
1:55:25     
positive probability over the data on the contrary any other fixed non singular point on the curve is the     
1:55:31     
minimum of probability zero so this is like the point that's the minimum     
1:55:36     
everything else is not the minimum because it's right at the point it's sort of the minimum you know there's a     
1:55:42     
geometric aspect of this so for any point within the pink region here the     
1:55:47     
unique closest point on the purple curve is the red cusp so we're comparing this pink region here in the Middle with this     
1:55:54     
purple curve because this purple curve only overlaps at this one point it's a     
1:56:00     
singularity with respect to this description so it's it's you know can     
1:56:06     
actually characterize this quite well as a geometric     
1:56:11     
problem uh so in the context of these optimization properties an additional     
1:56:16     
layer of complexity arises because the network training does not occur in the space of functions where the neurom man     
1:56:22     
lives but rather in the space of parameters so different networks can yeld the same manifold but they     
1:56:28     
parameterize that manifold in distinct ways such parameterization often induces     
1:56:34     
various critical points in parameter space that do not serve as critical points of the neurom manifold and     
1:56:40     
functions space so this is interesting stuff uh with respect then to moving     
1:56:45     
towards networks and some of these aspects uh among the easiest Network     
1:56:50     
architectures to study are fully connected linear networks this is the standard sort of work course of network     
1:56:57     
science where we want to characterize sort of a random Network or you know something that's comparator to some     
1:57:04     
Network structure this architecture is no activation function and all neurons from     
1:57:10     
one layer are connected to all neurons in the next layer so there's this fully connected aspect as in figure one the     
1:57:16     
neurom manifold of such a network as an algebraic variety a solution of polinomial equations that consists of     
1:57:23     
low rank matrices it's a very easy problem it has no boundary points it does have     
1:57:28     
singularities pending on the loss function these singularities can be critical points with positive     
1:57:34     
probability and so along along with these various critical points and parameter space they play a crucial role     
1:57:40     
in the analysis of the convergence of radi okay so that's kind of talking     
1:57:47     
about kind of linking Network architecture with some of the ways that we announ analyze them or we we do our     
1:57:54     
analysis and then having these spous critical points in parameter space and     
1:58:00     
then looking at the convergence of radiate descent so how does radiate descent happen with respect to some of     
1:58:07     
these geometric features and some of these uh parameter uh parameterization     
1:58:13     
aspects okay in the linear convolutional network not only runs between neighboring layers are connected so this     
1:58:19     
is what we have this non uh you know it's not fully connected there's a sort     
1:58:24     
of selectivity of connections and several edges between neurons share the same parameter so each     
1:58:30     
Edge is sort of a parameter in this network so if you think about like networks with a really high number of     
1:58:36     
parameters all it is is it's just the connections between um neurons so when     
1:58:42     
you have a lot of connections when you have a fully connected Network that has a lot more parameters than one of these     
1:58:48     
networks that are selectively connected but the selectively connected networks you know um I guess have less predictive     
1:58:56     
power um so okay so in the linear convolutional Network we don't have this     
1:59:03     
full connectivity we have fewer parameters but sometimes several edges between Z share the same parameter so we     
1:59:10     
have sort of condensation of parameters more concretely this type of network     
1:59:15     
parameterizes linear convolutions that themselves are composed of many individual convolutions     
1:59:22     
uh one for each Network layer so this is figure four where they talk about this filter um this is they     
1:59:31     
say here for instance a linear convolution of one-dimensional signals is a linear map where in each coordinate     
1:59:37     
function takes the inner product of a fixed filter Vector with part of the input vector and that's C figure 4 so     
1:59:44     
this describes as overlap in these uh in these filters and     
1:59:49     
this kind of describes how these share parameters um so this yields a linear convolution     
1:59:56     
so if you have this overlap these three filters these overlap areas of linear     
2:00:01     
convolution because they're convolving the the signals so when you share a     
2:00:06     
parameter you're convolving the signal um and then so they talk a little     
2:00:12     
bit about convolutions and their role in neural networks and that's equivalent to     
2:00:17     
the multiplication of certain sparse polinomial and unlike the fully connected case this network's neurom     
2:00:23     
manifold is typically not of algebraic variety instead is a semialgebraic     
2:00:29     
set so this is where the solution set of polinomial equations and polinomial     
2:00:34     
inequalities and you go back to the dis in figure two and you can think about like that set of solutions for these     
2:00:41     
polinomial equations uh and how that you know how that exists as a semialgebraic     
2:00:47     
set moreover the neurom manifold is closed in the standard idity topology it     
2:00:53     
typically is a non-empty relative boundry whose relevance for Network training depends on the network     
2:00:59     
architecture and particularly on the strides of the individuals so this kind of goes through     
2:01:05     
some of this uh let's see in conclusion we can     
2:01:11     
drastically change the geometry of a neurom manifold by varying the architecture of its neural network so     
2:01:18     
this is interesting that there's this connection between this geometry of the manif and is     
2:01:24     
architectural U varying the architecture so basically when you reduce the connectivity or you change the     
2:01:30     
connectivity you're changing this manifold you're changing the number of gers in the model with the simple     
2:01:36     
architectur that we describe here the neurom manifold is a stratified manifold that consists of low rag matrices and     
2:01:43     
tensors or reducible polinomial as geometry governs the optimization     
2:01:48     
Behavior between Network or during Network training so the go the geometry governs     
2:01:54     
optimization so there's a link between the geometry and gradient descent we     
2:02:00     
have demonstrated the impact of replacing fully connected layers with convolutional layers so these fully     
2:02:06     
connected layers are where we basically have one connection and it's one parameter the convolutional layers are     
2:02:13     
where do have shared connections for the parameters like we discussed so we have     
2:02:18     
these fully con connected layers and convolutional layers as these two aspects of the     
2:02:24     
network so we've replaced these two in linear networks we've demonstrated the     
2:02:29     
impact of doing that future work will reveal the corresponding effect of nonlinear networks so this is where we     
2:02:36     
get to the nonlinear case which is you know we assume it's more interesting uh     
2:02:42     
interested researchers can use algebra geometric tools which is in the second     
2:02:47     
citation here which is metric algebraic geometry     
2:02:53     
uh and to explore algebraic activation functions such as the rectified linear     
2:02:58     
unit which is uh citation 8 tropical geometry of deep neural     
2:03:05     
networks uh and so there yeah there's a lot of interesting work in geometric uh neural networks that uh I'm not     
2:03:12     
mentioning here but it's really kind of an interesting Direction uh the study of other     
2:03:18     
activation functions or analytical techniques collaborations between several mathematical     
2:03:24     
areas so that was pretty heavy mathematical deep learning     
2:03:31     
mathematical uh stuff but but there's a really interesting set of uh things     
2:03:36     
going on so yeah Jesse had to leave thank you Jesse for attending and I think that's it for that     
2:03:44     
article now you have any comments or questions     
2:03:49     
or uh no I mean you know those are kind     
2:03:57     
of you know so many so many techniques try to exploit the let's find the the     
2:04:04     
you know reduced form in in in I'm thinking more from     
2:04:09     
data analysis standpoint from the kind more theoretical but you know um so many     
2:04:16     
of our techniques are assuming that the there's a um     
2:04:22     
you know a submanifold that can be can be determined algorithmically somehow um or     
2:04:31     
that that reduced form is is um um would be useful um you know and     
2:04:42     
and yeah just thinking of yeah more analysis     
2:04:48     
pipelines that exploit that um and then you know the the importance of     
2:04:55     
mathematical importance of differentiability which is what the what the problem of the singularity     
2:05:03     
right yes and um there's um excuse     
2:05:11     
me someone someone had this um very long     
2:05:17     
paper that they published July 4th um that was like using p torch and Jacks     
2:05:23     
and talking about all sorts of you know architectural um features but it was     
2:05:30     
called Alice's Adventures in a differentiable [Laughter]     
2:05:37     
Wonderland that yeah I did I did like the name um but yeah did you post that     
2:05:46     
in the slack I think you I didn't I I I totally I can I can all right yeah that would great it was good there there was     
2:05:53     
um um I'm trying to think of some things that I I know I meant to add to     
2:06:01     
slack uh yeah I'll drop that in data ml     
2:06:07     
datl okay um there is     
2:06:15     
yeah and um I thought that was pretty good yeah there's some the UC see     
2:06:22     
Berkeley course and large language model stuff is also in     
2:06:29     
there um     
2:06:37     
um oh yes so just one thing I know we're     
2:06:42     
kind of have time but um um in Dev AI this why did children and adults learn     
2:06:49     
differently oh yeah but um just just another you know again another reason to     
2:06:58     
to study developmental yeah developmental psychology there was an interesting     
2:07:03     
paper on Hacker News um that was like from     
2:07:09     
1985 and it was talking about um so it was like more an educational psychology     
2:07:17     
paper yeah where they were talking to kids remember this was the time of um     
2:07:23     
logo you know oh yeah right language and um um so they were teaching classes of     
2:07:33     
Kids logo and then they were kind of debriefing     
2:07:39     
afterwards and yeah which is which is already problematic yeah right so I mean     
2:07:45     
you know but again like super interesting for for developmental psychologists like how do you get     
2:07:51     
insight into how kids they are um but they were talking to them about um     
2:07:59     
recursion in um in logo like like like     
2:08:05     
they they gave them a particular problem and and you know we're teaching them this this algorithm and then we're kind     
2:08:13     
of asking them about it and they they noted that there was this this real     
2:08:20     
problem with the kids like like the kids seem to get the wrong idea about     
2:08:27     
it and and you know and this this there was     
2:08:33     
this typical psychological problem of the the kids held this view about     
2:08:40     
it even in the face of evidence you know like like it seemed it seemed very     
2:08:46     
resilient to to to being um yeah     
2:08:53     
yeah counterfactuals or something like that yeah yeah and the um anyway there     
2:09:02     
was U somebody was talking on the internet about this difference between     
2:09:07     
how children and adults learn and and of course that it's it's it's naive to     
2:09:15     
think that that's um uh um you know weakness or you know like     
2:09:23     
like like again like all these things have their place or kind of like you     
2:09:28     
know like again you know we're not we we really need to see the big picture before we we say you know this is a     
2:09:35     
problem because what we certainly know is that kids aren't learning everything     
2:09:41     
wrot right right you know like like there there's some you know incredible     
2:09:48     
uh you know induction going on and and     
2:09:53     
and so you know it's just just important to note that like not everything that us     
2:10:00     
adults the ways that we're thinking is is what's happening for you know young     
2:10:08     
developing systems and um I thought that I thought that logo example was     
2:10:14     
interesting you know yeah yeah yeah reminds me kind of I've seen a couple     
2:10:19     
talks where they talk about naive physics where physics where children will learn physics just kind of from     
2:10:25     
interacting with the world and then yeah later they learn about the rules like laws of physics and things like that but     
2:10:32     
you know you get like this naive physics that develops before there's a form more formalized physics but you know still     
2:10:38     
it's interesting because it's like if we were training a a robot how would it learn physics it would learn through     
2:10:46     
interaction but that's not really like there are a lot of things that it kind of doesn't do in the same way that we     
2:10:51     
have is from our formalized physics which is having laws and you know so we could train a robot on like laws of     
2:10:58     
physics and and or we could train it by having it sort of pick up these naive rules or these naive uh ways of thinking     
2:11:06     
about the world and so it's very different in a training regimen right     
2:11:11     
right and and you know that that's how important it is to again remember that     
2:11:18     
what we're we're actually dealing with are cont continual learning systems and not kind of a single system right that's     
2:11:26     
supposed to get everything right you know so so it's it's again if it's only     
2:11:33     
when we see this this as you know a multi-step or multiphase process     
2:11:42     
um that are we gonna start to see why like you     
2:11:47     
know these these early these early naive stages are perhaps you know really     
2:11:55     
important because again this is in a much larger context of um you know     
2:12:02     
multiple critical periods and multiple representational redescriptions uh um because you know     
2:12:10     
yeah we don't get to have a a neural net for each of these things it's just one     
2:12:16     
you know we have one brain that that's that's trying to bring all this in and and     
2:12:23     
um create interfaces between it all anyway it was nice to see my my     
2:12:29     
undergrad site was cognitive development so yeah it was a good a good reminder of     
2:12:34     
this that uh that any you find that in De AI okay yeah thanks so this paper why     
2:12:42     
did children and adults are IND differently what was the basic oh this was more of just a they did a symposium     
2:12:49     
you you know like like I think it was I think there was a     
2:12:54     
speaker um that's this particular woman who like     
2:13:01     
a brain brain imager um you know like babies and children are     
2:13:09     
fast Learners right so it's it's like again it was just kind of getting at     
2:13:14     
these you know what are the mechanisms by which they're able to do all these     
2:13:19     
things right that's seem that seem kind of superum right um oh yeah well there's     
2:13:27     
Tom Thomas Elbert I I recognize that name um but yeah it's a it's a list there     
2:13:34     
this was this was like a sosia     
2:13:40     
though Hector fellow Academy I haven't heard     
2:13:49     
that um but then in and what was the other     
2:13:54     
one well I I'll find this um this logo paper that was on H yeah yeah that that     
2:14:02     
that was that was interesting um also just nice to blast from the past in 1985 oh     
2:14:12     
yeah so I wonder yeah like well I guess with kids it wouldn't matter so much but I wonder if like you know if you had     
2:14:19     
people who knew what a concept was was more like if it's if it penetrates sort of the popular imagination more versus     
2:14:27     
something because in 1985 I'm not sure how much recursion was a part of sort of     
2:14:32     
the popular imagination I mean Computing personal Computing was rather new and so     
2:14:38     
it would be like well what is recursion you know and then you have to describe it and people wouldn't have a real frame     
2:14:44     
of reference for it I mean nowadays it's a little bit more obvious because people more people know programming it's more     
2:14:51     
part of like the world so I'm just curious as to well yeah I mean I'm I'm     
2:14:58     
curious too um well one I'm curious if if maybe     
2:15:08     
something like this is why it's been harder to break to to to to start     
2:15:15     
computer science classes earlier yeah or or like like like uh     
2:15:22     
uh there's there's just some interesting things about Computer Science Education     
2:15:28     
that have been that have been controversial and um you know yeah so I     
2:15:35     
I I don't know I mean it's it's hard for me to think back to this time I mean I     
2:15:40     
don't know if like if you were like this but you know I was one of those kids that that learn computers you know at     
2:15:49     
you know eight or 10 because I had a little hobbyist kid you know and so it     
2:15:54     
was very much like experiments I was you know in a sense experiments I was doing     
2:16:00     
yeah on my own right um probably would have been better if I     
2:16:07     
had a teacher or something see I'll put this in     
2:16:13     
De um uh yeah so it would be be interesting     
2:16:21     
to see if if um you know the Raspberry Pi Foundation does um has a seminar     
2:16:31     
series on kind of like e education of you know Computer Science     
2:16:37     
Education Theory practice you know like a combination of both you know um that     
2:16:44     
that tries to get Educators to talk about yeah like the the     
2:16:51     
difficulties as well as strategies you know for for but I I I um I think in     
2:16:59     
this case and this was more of the age range you know it's like this younger age range     
2:17:06     
is certain even in pan sense right like like they don't think like us you know     
2:17:13     
it's it's a mistake to kind of rationalize your educational strategy in the same way yeah um but but that can be     
2:17:21     
hard to do and you know this was the the the my thesis work was you know again     
2:17:27     
like carof an carel Smith kind of work where it was it was you were trying to     
2:17:33     
you were doing experiments to try and get it how how yeah that that that age     
2:17:40     
range thinking was was fundamentally different you     
2:17:46     
know anyway it would be interesting yeah well that's great     
2:17:52     
yeah all right no no it's a it's a good question     
2:17:58     
you know like I haven't I haven't looked at it but yeah um you know the the all     
2:18:04     
that kind of algorithmic or computational thinking is is kind of new     
2:18:10     
educational psych to consider     
2:18:15     
yeah yeah Food For Thought yeah all right well thanks for attending and it     
2:18:21     
was a good meeting and uh see you next week take care take care by see
