## Meeting Recording

[YouTube link](https://youtu.be/gVogJWBcCrM)

## Mastodon thread

[link](https://neuromatch.social/@OREL/114968665190053153)

## FEATURE VIDEOS
[A (modern) tale of connectionism](https://youtu.be/7Ep2Moxy96s)

## TRANSCRIPT 
0:01    
Hello. Hi. How are you? All right. How are you?    
0:07    
I'm good. Morgan.    
0:20    
So, welcome. Um, this week we had two meetings as usual.    
0:27    
We had our evil one meeting on Monday. We had u some features on things like    
0:36    
considering the different shapes of life forms. Um we also talked about how simulations    
0:44    
are used in biology and modeling organisms in particular.    
0:50    
And then we talked about developmental bio or mechanobiology. So this is where we're interested in the    
0:57    
mechanics of cells and cell populations as they form sheets and different shapes    
1:06    
like they do things like buckling, bending and migrating around    
1:14    
and that those sorts of things. So we talked about all those topics.    
1:21    
Then on Friday we had our open source meeting. Um and that we had some updates    
1:27    
on our Google summer code scholars and especially from Giadratha Gian. He    
1:34    
uh gave an extensive update on his project. Uh we also talked about documentation    
1:42    
strategies. present on some slides on that.    
1:47    
And then Jesse had a little discussion on his work on organizational literacy,    
1:54    
which is nice that he's kind of getting into that niche area.    
2:01    
He also has a number of initiatives going through what his job organization, which is this mentoring organization    
2:08    
that he runs. He's doing some uh internships in this    
2:15    
data times direction project that he's working on. And then he has some sort of contest or something    
2:24    
like that, an essay contest on technological development and    
2:30    
innovation. So I don't have an advertisement for that, but that's something that he'll be advertising on    
2:36    
LinkedIn and places like that.    
2:41    
Okay. So, that was this week. All right. So, why don't we get started?    
2:48    
So, I'm going to go over a couple things that were in the Slack this week. Uh,    
2:53    
one was posted by Morgan, the other by Bey. So, the one that Morgan posted was    
3:00    
this large language model embeddings explained article. This is from Hugging    
3:06    
Face actually from Hugging Face Bases. Um, it's from Hessimation. I'm not sure    
3:13    
what that is. That's a a user Ham.    
3:20    
Okay. Um, but anyways, this is the guide. So, this talks a little bit about    
3:25    
the embedding space. Okay. This is Hassan Rashid Kasani from University of Bala.    
3:33    
So this is the this is the article    
3:39    
um and so talking a little bit about what are embeddings, what makes a good embedding,    
3:46    
traditional embedding techniques and for that you have the term frequency    
3:51    
and verse document frequency. Uh then there's word to    
3:57    
BERT embeddings in modern large language models. embeddings in action    
4:04    
which covers deep a version of deepseek embeddings as graphs and network    
4:10    
analysis and I think that's what this is here and then let's wrap up so this is    
4:18    
an image here the embedding atlas of 50 random words and their closest tokens in    
4:23    
the embedding space of deep sea gar still 1.5 billion parameters    
4:31    
So this is uh I guess these are the different tokens here. So you have these    
4:39    
I guess words you have this network of terms and then you have similar tokens    
4:45    
to stones. So stone for example is a word that are similar token stones.    
4:54    
stones. Um I guess it's    
5:01    
different, you know, they're different um East Asian languages where you have the different characters there status    
5:08    
bad requests. So they're associated tokens. Um    
5:14    
yeah, so this just talks a little bit about the the um    
5:20    
you know how these tokens are related in this embedding space.    
5:26    
So and so he has this summary    
5:32    
generated by notebook lm. It's the podcast summary that they have which is    
5:38    
interesting. So what are embeddings? So processing text for NLP tasks requires a    
5:45    
numeric representation of each word. Most embedding methods come down to turning a word into a token or a vector    
5:54    
or a word or token into a vector. What makes embedding techniques different    
5:59    
from each other is how they approach this word to vector conversion.    
6:04    
So it's taking the words or the tokens and turning them into some sort of    
6:10    
vector. Embedding is not just for text. It can also be applied to images, audio, or    
6:17    
even graph data. In a general sense, embedding is the process of converting data of any type into vectors.    
6:25    
Of course, the embedding methods of each modality is different and unique. In    
6:30    
this article, when we talk about embeddings, we're referring to text embeddings.    
6:36    
You may have heard of embeddings in the context of large language models, but embeddings actually have a much longer    
6:42    
history. And so this is uh this figure down below is an overview of various    
6:48    
embedding techniques. So this is the word embedding and then you have traditional word    
6:54    
embeddings, static word embeddings, and contextualized word embeddings. and then    
7:00    
the model associated with that. So traditional word embeddings are represented by count vectors, TDF    
7:09    
which is term frequency inverse document frequency and then co-occurrence matrix.    
7:16    
Uh static word embeddings are represented by word tovec    
7:21    
love and fast text. And then contextualized word embeddings    
7:28    
represented by Elmo GPT    
7:33    
beep seek llama and GPT4s. Um so you you have all these different    
7:41    
methods or techniques for these to achieve these different embeddings.    
7:48    
When reading about embeddings you may come across static versus dynamic or contextualized embeddings.    
7:54    
So it's important to distinguish between token embeddings which are the fixed vectors assigned to    
8:00    
input tokens at the very beginning of a large language model and the contextual    
8:07    
representations produced by the deeper layers of the model.    
8:13    
So this is where we have these token embeddings the beginning and then at    
8:18    
deeper layers we have the contextual representations. While both are technically embeddings,    
8:25    
token embeddings are static, whereas intermediate hidden states evolve as    
8:31    
they pass through each layer, capturing the full context of the input. In some    
8:37    
literature, these contextual outputs are referred to as embeddings, which can be confusing.    
8:43    
Okay, so that's kind of a little bit about embeddings. But what makes a good    
8:49    
embedding? So this is considering terms of large language models and how good    
8:56    
that embedding actually is. So embeddings can be thought of as a dictionary of their language. Better    
9:02    
embeddings allow these models to understand the human language and communicate with us. But what makes an    
9:09    
embedding technique good? In other words, what makes an embedding ideal?    
9:14    
Here are two major properties of an embedding technique. So the first one is semantic    
9:20    
representation. Some types of embeddings capture the semantic relationship    
9:26    
between words. This means that words with closer meanings of relationships    
9:32    
are closer in the vector space than words that are less related. So we know that in terms of like    
9:39    
you have the words the vectors cat and dog which form two words    
9:46    
that have some relationship to one another in terms of similarity.    
9:51    
Now cat and dog of course are different things but their    
9:56    
similarity doesn't exist just in the relationship between those two vectors. you need additional vectors to have some    
10:04    
sort of relative similarity. So cat and dog are more similar than dog and    
10:10    
strawberry. So you might you have dog and wolf that might be closer than cat or dog but dog    
10:16    
and strawberry farther away. So you have the similarity metric that you can use and you have a dictionary of    
10:24    
words or or of vectors that give you some semantic information by which you    
10:31    
can judge how similar or different they are. Then there's dimensionality    
10:37    
um which should be the size of an embedding vector 15 50    
10:44    
characters. Uh striking the right balance is key. Smaller vectors, lower    
10:49    
dimensions are more efficient to keep in memory of the process. Of course, while    
10:55    
bigger vectors, lower higher dimensions and capture intricate relationships, but    
11:00    
are prone to overfitting. The reference GPT2 model family has an    
11:05    
embedding size at least 768. Okay, so that's uh basically if you have    
11:13    
smaller vectors, you have a lower dimensional space    
11:19    
and you have this sort of efficiency of memory because you have less data. I    
11:26    
guess bigger vectors or higher dimensions can capture more inter    
11:32    
relationships, give you more context, things like that. But then also are    
11:37    
prone to overfitting. and are less efficient to keep in mind. So that's    
11:42    
kind of the idea. So for reference, GPT2 model family has an embedding size of at    
11:47    
least 768. So that gives you an idea of what the state-ofthe-art is for this.    
11:56    
So traditional embedding techniques almost every embedding technique relies on a large corpus of text data to    
12:03    
extract the relationship with the word. Previously, embedding methods relied on    
12:08    
stati uh statistic methods based on the occurrence or co-occurrence of the words    
12:14    
in the text. This was based on the assumption that if a pair of words often appeared together, they must have a    
12:21    
closer relationship. These are simple methods that are not as computationheavy as other techniques. So    
12:28    
you have this this co occurrence matrix where you see if two words occur    
12:33    
together maybe in context you're maybe in the same document and so they might have a closer    
12:40    
relationship but maybe not maybe they don't. So you have other metrics that    
12:46    
you can use co-occurrence just being one of many.    
12:52    
So one of these methods that are more advanced is this TF-IDF    
12:59    
or term frequency inverse document frequency. Okay. So the    
13:05    
idea of TF IDF is to calculate the importance of a word in a document by    
13:11    
considering two factors and this is from Medium post called a comprehensive guide    
13:17    
to word embeddings in nonlinear programming.    
13:24    
Um the first is term frequency. Uh how frequent a term appears in a document.    
13:32    
A higher a higher term frequency shows that a    
13:38    
term is more important to the document. Okay. So this is just the frequency of a    
13:44    
term. So if a term is occurs 50 times in a document, it's probably more important to that    
13:51    
document than if it appears twice. The second is inverse document frequency    
13:57    
or the IDF component of this and that is how rare a term is across the documents.    
14:03    
This is based on the assumption that terms appearing in multiple documents are less important than terms that are    
14:09    
unique to fewer documents. So if you have terms across documents,    
14:16    
they're more important to that corpus of knowledge than terms that are maybe    
14:22    
unique to one or two documents. And so that that's you know that's sort    
14:27    
of thinking about like papers in a field of study, right? If there's a paper on a    
14:35    
specific topic, they'll tend to use certain terms again and again.    
14:40    
those terms are going to be important in terms of that document, but if they're important to the field,    
14:47    
they'll be in multiple documents in your training set and ideally have a high    
14:54    
frequency within the document as well. So that's kind of the idea.    
15:01    
So there's a formula here for TDF and it consists of two parts. So the term    
15:07    
frequency or TF is calculated using this term    
15:14    
which is the count of term T in document D over the total number of terms in    
15:21    
document D. So you just count up the number of terms, you count the term T of    
15:28    
interest and you get a ratio. And the idea is if that term T is highly prevalent, you get    
15:37    
a larger number and number should be between zero and one.    
15:43    
So for example, if a document has 100 words and the word cat appears five times the term frequency for cat would    
15:52    
be five over 100 which is 0.05. This gives us a simple numeric    
15:58    
representation of how prevalent that term is in the document. This number will increase    
16:06    
as the numerator increases. So if cat occurs more often this number will be    
16:12    
higher. Okay. Then the inverse document frequency or IDF is calculated as the    
16:20    
log of the total number of documents over the number of documents containing    
16:26    
term t. So again we're not interested necessarily in the frequency of the    
16:32    
term. We're interested in the frequency of    
16:38    
that term with the total number of documents. So let's see if we can.    
16:45    
Okay. Yeah. So but that's the log transform of that. So you get this number that's normalized. This component    
16:52    
gives higher weight to terms that appear in fewer documents. Common words that appear in many documents like the and is    
17:01    
will have a lower IDF or rare more informative words will have a higher IDF.    
17:10    
So again, if we have a large number of documents and we have a small number of documents    
17:15    
containing term T, you know, that's going to be kind of a    
17:24    
higher IDF. If we have terms like a which should be in just about every document or where the is, then it's a    
17:32    
lower IDF. Finally, the TF of score is calculated    
17:39    
by multiplying these two components. And then he gives an example where    
17:45    
they have this term cat in a corpus of 10 documents.    
17:52    
And so the score tells us how important the word cat is to the specific document    
17:57    
relative to the entire corpus. This is in reference to the IDF score. Higher    
18:02    
score indicates that the term is both frequent in this document and relatively rare across all documents making it    
18:09    
potentially more meaningful for characterizing the document's content.    
18:15    
And so now they use TDF TF on this tiny Shakespeare data set    
18:22    
and that is uh from Harpathy    
18:27    
and it's this data set here where you have uh I guess uh some of the    
18:34    
Shakespeare in a format that you can use to train a model to actually calculate this uh TF    
18:42    
IDF. F method. So this is an example using tiny    
18:48    
Shakespeare. So they're basically taking this    
18:53    
document and they're training the model or they're calculating TF.    
18:59    
And so this gives us a 10-dimensional embedding each for a document we have.    
19:04    
Now to get a better idea of the TF IDF embeddings, we use PCA to map the    
19:10    
10-dimensional space to two-dimensional space so we can visualize it better. So it takes this 10 dimensional embedding    
19:17    
which is um extracted from this tiny Shakespeare data set.    
19:23    
We've chopped the document into 10 chunks to get this dimensionality.    
19:31    
And so now then we have to transform this down to a two-dimensional space or reduce this down to a two-dimensional    
19:38    
space using PCA. So if we know we recall PCA, it's    
19:44    
basically finding the dimensions or the components that have explain the most    
19:50    
variance in the data set. And we'll plot two of those dimensions against each    
19:55    
other. So PC1 versus PC2. So we see here that we have these word    
20:01    
frequencies from these from this uh corpus which    
20:07    
we've divided into 10 documents which gave us our dimensions and then reduced that to two dimensions.    
20:16    
So as we can see there's a huge cluster of terms here around zero and zero.    
20:22    
Then we have uh terms that kind of radiate out along this principal    
20:28    
component and a bunch of terms that radiate out along this principal component.    
20:35    
And we can see that like the word Romeo with. So we have with and for but and as    
20:42    
out here these are just words that are not relevant to anything but just as parts    
20:49    
of speech. you have Warwick, uh, Margaret, God, Romeo, death.    
20:58    
Those are all kind of significant to the story. So, they're along this PC here.    
21:06    
So, you can see that you can like find words that are relevant to the text that    
21:11    
are critical to the context and it extracts those.    
21:16    
Okay, so that's that's what this basically is. This is the word embedding for this Shakespeare uh example.    
21:26    
So there are two notable things about this embedding space. The first is that the majority of words are concentrated    
21:32    
into one particular area which is here. This means the embedding of most words are similar in this approach. So we    
21:39    
don't really get that differentiation of as many words as we would maybe like. We    
21:44    
get these words here. So we do get some good uh results. We do get some    
21:50    
significant results in that sense, but we don't get maybe as much differentiation if we don't.    
21:57    
It signals a lack of expressiveness and uniqueness about these embeddings. So you know, if this embedding were more    
22:04    
expressive, we'd get more of these unique words out here and we'd be able    
22:09    
to identify. Uh but as it stands, they're in this cluster. So that's a weakness of this approach.    
22:16    
Then number two is that the embeddings have no semantic connection. The distance between words has nothing to do    
22:22    
with their meaning. So we don't really know if    
22:28    
words that have similar meaning in the in the corpus    
22:33    
um are close together. They're just kind of out here    
22:39    
and distinct but not necessarily related because TF is based on the occurrence    
22:46    
frequency of terms in the document. Words that are semantically closed such as numbers have no relation in the    
22:53    
vector space. The simplicity of TDF, TF and similar statistical methods is what    
23:00    
makes them useful in applications such as information retrieval, keyword extraction and basic text analysis.    
23:08    
So that gives us that moves us to word tovec which is another technique. So this is a more modern technique than    
23:15    
TF is can be assumed by the name and is a network that aims to convert words into    
23:21    
embedding vectors. It achieves this by defining a side goal something to optimize the network for. For example,    
23:29    
in seab or continuous bag of words, the word devec network is trying to predict    
23:34    
a missing word when it's given the neighbors of that word as input.    
23:40    
So it's it's kind of doing this uh prediction um and it's doing this uh conversion    
23:48    
into embedding vectors. So it's it's creating this embedding and then finding sort of the missing states or the    
23:55    
missing words within it. So it's actually more sensitive to context. It's    
24:00    
able to find context or or you know kind of assign context.    
24:06    
The intuition is that you can infer the embedding of a word given the words around it. So we can figure out what the    
24:13    
embedding should look like based on the input and if they're missing words or if    
24:20    
it's hard to know kind of what the word should be if we were predicting um a string of words then we would be    
24:26    
able to do that. The word devec architecture is pretty simple. One hidden layer that we extract    
24:33    
the embeddings from and one output layer which predicts the probabilities of all words in the vocabulary.    
24:41    
So we extract the embeddings from one hidden layer and then have an output layer which allows us to predict the    
24:48    
probability of words in the vocabulary. On the surface, the network is trained    
24:53    
to predict the right missing word given its neighbors. But in reality, this is an excuse to train the hidden layer of a    
25:00    
network and find the right embeddings for each word. So after the network is trained, the    
25:07    
last layer can be tossed out the window. because figuring out the embeddings is a real goal of that.    
25:13    
So this is a figure that shows the word tovec technique how it works. We have    
25:20    
these input words. This is great book. So we're missing    
25:27    
um an a in in the middle of that sentence. So we just have this is great book is    
25:34    
the inputs. Those are the input words. we have this uh notation wtus 2 wtus1    
25:43    
wt + 1 wt +2 basically w is a missing word and we're trying to find that word    
25:50    
given the words around it so this is great book goes into the    
25:57    
hidden layer this is uh j * e and this    
26:02    
represents the weights from the input layer to the hidden layer and this becomes the embedding    
26:08    
these weights. So it's a weight matrix and then we have    
26:13    
the output probabilities. So this is the these are the output    
26:19    
probabilities E * V and so then we have our SIBO example    
26:26    
down here where we have uh this is a great book which is predicted from the    
26:34    
sentence structure. So that's basically how it works. Um you    
26:39    
have these probabilities this weight this weight matrix here that becomes the    
26:45    
embedding and then you have the output probabilities and you throw those out after you get    
26:50    
the embedding. Okay. So this is um sort of the seab    
26:57    
variant of word devac. So you can use seabbow you can also use skip gram which works    
27:04    
in an opposite fashion which predicts the neighbors given a particular word as input.    
27:11    
Okay. So, but here's how see by word tovec works step by step. Choose a context window. So, in this    
27:18    
case, we have a context window of two. So, we have this is and great book and    
27:24    
those are that's the context. And then we can also put something in    
27:29    
the middle here. So, we can predict missing words. Then number two, take the two words    
27:35    
before and the two words after a particular word is input. Three, encode these four context words    
27:42    
as one hot vectors which they just you know this is a specialized type of vector    
27:49    
or pass the encoded vectors through the hidden layer which has a linear activation function    
27:56    
that outputs the input unchanged. Uh number five aggregate the outputs of    
28:03    
the hidden layer using the name the mean function. Six, feed the aggregated output to the    
28:09    
final layer which uses softmax to predict probabilities for each possible word.    
28:15    
And seven, select the token with the highest probability as the final output of the network.    
28:23    
Okay, so the hidden layer is where the embeddings are stored. It has a shape of    
28:28    
vocabulary size times embedding size. So that's this here. Actually, that's V,    
28:34    
not J. V * E and E E times V.    
28:40    
So you have vocabulary size V embedding size E.    
28:47    
So and as we give one hot vector a vector that is all zeros except for one element set to one of a word to the    
28:54    
network that's specific one triggers the embeddings of that word be passed to the    
28:59    
next language. You can see a cool and simple implementation of this in this    
29:06    
reference here. Implementing deep learning methods and feature engineering for text data. The continuous bag of    
29:13    
words model. It's from KD nuggets. So this is the word devec embeddings    
29:19    
visualization. You got the input layers here. You have the hidden layer here and the output layer here.    
29:29    
So this is actually the word tovec embedding visualized.    
29:35    
So this is tensorflow embedding projectors interactive exploration    
29:41    
of word tovec embeddings in a 3D2D space.    
29:47    
So I don't know if you can play with this. I don't yeah uh you can visualize and play with word devec embeddings of    
29:53    
tensorflow embedding projector. So that's what this was produced in. And    
29:59    
you can see here that you have this projection of these words    
30:05    
and the points in the space. So you can rotate it around as a 3D space.    
30:11    
You can see in the 2D space that they're all these different words kind of in a    
30:17    
scatter plot. Okay. So that doesn't necessarily give us a lot of clustering in like you know    
30:26    
where there's no distinction between words. You get distinctions between words now. You get distances    
30:33    
but it's still maybe not what we're looking for. So we have this other option called B    
30:41    
which is the birectional encoder representations from transformers.    
30:47    
And so if you look into in the world of NLP, you'll see BERT. It is a good idea    
30:53    
to do yourself a favor and learn about BERT once and for all. It is a sort as it is the source of many ideas and    
30:59    
techniques when it comes to large. So BERT is an encoder only transformer    
31:05    
model consisting of four main parts. There's the tokenizer    
31:10    
which chops up text and sequences of integers. The embedding which is the module that    
31:17    
converts discrete tokens into vectors. The encoder which is a stack of    
31:23    
transformer blocks with self attention. And then the task head. So when the    
31:30    
encoder is finished with the representations, this task specific head handles them for    
31:35    
token generation or classification tasks. Okay.    
31:41    
So this uh BERT was inspired from the transformer architecture introduced in    
31:46    
the attention is all you need paper which is now a classic paper. Um and so    
31:54    
it's related to attention in this transformer architecture    
31:59    
to become an encoder only transformer that can produce meaningful representations and understand language.    
32:06    
The idea was that depending on specific problems to solve, bird is fine-tuned to    
32:11    
learn about that task. These specific tasks can be Q&A,    
32:16    
question plus passage leads to an answer, text summarization,    
32:22    
classification, and other types of specific tasks.    
32:28    
So, this is an image of the Bergen architecture down here. It's kind of hard to see. There's a lot    
32:35    
going on, but basically uh you have a pre-training    
32:41    
phase and then a training phase. So in the pre-training phase, bird is trained    
32:47    
to learn two tasks simultaneously. The first is mask language modeling,    
32:53    
which is to predict masked words in a sentence. So um in other words, you mask certain    
33:01    
words in a sentence. you present that sentence to the model and it's able to predict what puts beneath the mask.    
33:09    
So it's it's similar to what we see in the previous example,    
33:15    
but instead of you know trying to predict hidden states, we're trying to predict something that's masked or    
33:22    
hidden in the sentence. We present the sentence as it would be given the hidden    
33:28    
word, but then we have to predict the hidden word. You also have next sentence prediction.    
33:35    
So given two sentences, predict if A came before B or not. So basically    
33:41    
you're trying to predict word order or or sentence order. So you're getting    
33:47    
this sort of probability that says this sentence should come    
33:53    
before the sense of a certain probability and or maybe it doesn't come before the    
33:58    
sentence of a certain probability. The special token separates the two    
34:04    
sentences and the task is similar to binary classification. So that makes sense because you're doing    
34:10    
these pair-wise predictions. And so you do this, you know, in    
34:15    
parallel and you end up with being able to predict maybe an entire paragraph or    
34:21    
an entire page of words and sentences.    
34:27    
So this there they're there these special tokens. There's SCP and CLS.    
34:34    
SEP is used in next sentence prediction. CLS is used in classification tasks.    
34:42    
As the model processes input layer by layer, CLS becomes an aggregation of all    
34:47    
the input tokens which can be later used for classification purposes.    
34:54    
So why is BERT important? BERT is among the first instances of transformerbased    
35:00    
contextualized dynamic embeddings. When given a sentence as input, the layers of    
35:05    
the B model use self attention and feed forward mechanisms to update and    
35:10    
incorporate context from all other tokens in the sentence. The final output    
35:16    
of each transformer layer is contextual is a contextualized representation of the word.    
35:22    
Okay. So that's kind of going through these    
35:27    
different techniques. Um, but then you have to think about where embeddings actually fit into the    
35:34    
larger language models that we use. And so where does the embedding fit into    
35:40    
the large language model? So in transformerbased models, the term embedding can refer to the static    
35:46    
embeddings and dynamic contextual representations. So in static embeddings    
35:53    
um generated in the first layer and combine token embeddings which are vectors representing tokens    
36:00    
with positional embeddings or vectors encoding a token's position in the sequence.    
36:06    
Then there's dynamic contextual representation. So as input tokens pass through the self    
36:12    
attention and feed forward layers of the network, their embeddings are updated to    
36:17    
become contextual. These dynamic representations capture the meaning of tokens based on their surrounding    
36:25    
context. So for example, the word bank appears in as both river bank and bank robbery    
36:33    
which are two different meanings. While the token embedding of the word bank is the same in both cases, the    
36:39    
transformations it goes through in the layers of the network take into account the context in which the word bank    
36:46    
appears. So this is an example here. So this is an overview how embeddings    
36:52    
fit into the or language model architecture. So we start here. London is the capital    
36:59    
of and the tokenized input takes these words turns them into numbers.    
37:08    
Then those numbers are passed through to an embedding layer, then passed to    
37:13    
transformer blocks and then passed as an output layer which is 5412. So it's this    
37:20    
numeric tokenized output and then that's translated into is the capital of    
37:26    
England. So it's basically    
37:31    
it's passing in this idea that London is the capital of someone. It's tokenizing the input.    
37:39    
It's going through this transformer process and then it's tokenizing an output which is then translated into is    
37:46    
the capital of input. So it has the context and it has the word order and it    
37:54    
kind of has a corresponding maybe an answer or reprisal to the thing    
38:00    
that goes into the into the model into the transformer blocks and the embedding.    
38:09    
So there's there are these corresponding tokens that kind of are connected to one another and then the output is related    
38:16    
to that. And so we have to remember that large    
38:24    
language model embeddings are trained and during this training process are optimized.    
38:30    
So he borrows from this uh resource from Sebastian Rashka building a large    
38:37    
language model from scratch. This is a from Manning publications.    
38:43    
Um, so it's a book and you can use that to kind of build your own language    
38:49    
model. Uh, while we can use pre-trained models such as wordtovec to generate    
38:54    
embeddings for machine learning models, large language models commonly produce their own embeddings that are part of    
39:01    
the input layer that are updated during training. The advantage of optimizing the    
39:06    
embeddings as part of the word and language model training instead of using wordtovec is that embeddings are    
39:12    
optimized for the specific task and data at hand.    
39:17    
So um so it helps us with our specific data    
39:24    
set and we have a generalized embedding and then that gets optimized based on what we're training the model.    
39:32    
So basically, you know, if we look at an embedding layer and how it works and if    
39:38    
we think about this example, we see that there's this connection between some    
39:44    
phrase that we feed into the architecture and a phrase that we get    
39:49    
out. It basically answers a question. And so what does that mean?    
39:56    
Well, it means that your embedding layer basically works as a lookup table.    
40:01    
So if we have some set of entries by index, we can go to the embedding matrix and we    
40:09    
can find corresponding um index values that can provide an    
40:15    
output or an answer. So our lookup table works as follows.    
40:22    
Given a list of indices or token IDs, it returns their embeddings.    
40:27    
build a large language model from scratch shows this concept comprehensively. They have figures in    
40:32    
this book that kind of walk through the lookup table aspect of this.    
40:38    
If you want to implement this in Python, the code implementation of an embedding    
40:44    
layer in PyTorch is done using torch.nbding.    
40:49    
So it's the torch.n.bing method which acts as a simple lookup    
40:54    
table. There is nothing more about this layer than a simple linear layer other    
41:00    
than the fact that it can work with indices inputs rather than one hot encoding inputs and we talked about the    
41:06    
one hot encoding inputs earlier but then you can replace those with indices    
41:12    
and it'll work from the sample. The embedding layer is simply a linear layer but it works with indices.    
41:20    
So this is where we have this uh example here. Convert indices of three training    
41:27    
examples and then multiply one hot encoded inputs with weight matrix. So    
41:32    
this is the one hot encoding. It's basically this this binary uh matrix    
41:38    
from a a vector of different values. And then down here we have multiply one hot    
41:44    
encoded inputs of weight matrix. So we have the one hot encoded input.    
41:50    
This is this uh binary uh matrix. Then we move this down to this these indicy    
41:58    
value index values here. So this just shows this process of going from these    
42:04    
training examples to one I have encoding and then these entries and a weight matrix. So then they have these examples    
42:12    
of embeddings in action. This is using deepseek and they kind of go walk through that.    
42:19    
Then they have this embeddings as graphs which is a network analysis. So this is visualizing these embeddings. So as we    
42:25    
said the embeddings have related components. We have uh tokens that are related maybe    
42:34    
semantically maybe by frequency or whatever. So we    
42:40    
have these co occurrence matrices that we can draw from. And but we want to see    
42:45    
what these look like. We don't want to just kind of um you know take the embeddings word for    
42:51    
it. We want to visualize them and see what it actually looks like. So one way to of course look at the    
42:57    
embedding layer is as a network and which tokens are the nodes. If two token    
43:03    
vectors are close then we assume their nodes are connected by an edge.    
43:09    
As an example, if we take a sentence, AI agents will be the most hot topic of    
43:14    
artificial intelligence in 2025. We can tokenize it, convert the tokens    
43:22    
to embeddings, find the 20 most similar embeddings to each of the ones we had.    
43:27    
The following was will be the embedding graph. So this is using a sequence of steps to generate this embedding graph    
43:36    
where you have um let me see okay so this is will this    
43:41    
is a hub in this network and there's similar tokens which serve as these um    
43:48    
outlying nodes. So will uh has similar tokens in the network and it's connected    
43:56    
then to in which has another I guess they share a token in common    
44:03    
most uh which has other associated tokens again and they share a couple of    
44:09    
those at will and then agent and intelligence share tokens    
44:16    
um and those are some of those are shared with each other. So there's this network where is it's I don't know how    
44:23    
to describe this network. It could be like a radial graph or you know multiple    
44:29    
radioraphs where you can see say this this uh token and related    
44:38    
tokens and then you have connections between them.    
44:45    
So you can actually see a more comprehensive example at the beginning of the article which 50 tokens or the    
44:51    
closest tokens are mapped out. So that was the thing we showed at the beginning. So in the graph examples we have ignored    
44:58    
the token variations. A token or word may have many different variations each with their own    
45:04    
embeddings. For example, the token list may have buried many different variations of    
45:10    
their with their own embeddings such as list or underscore list list with a    
45:17    
capital L and many more. So you can actually have variations on some term    
45:23    
and this is where the sort of connectivity comes in handy because the    
45:28    
term hot or the token hot has different variations. So it has dash hot    
45:35    
underscore hot hotspot cold hotter hot    
45:42    
and you know diff the hot in different languages. So it basically characterizes    
45:47    
the maybe the the phrase how it's used in context.    
45:53    
There might be oppositional terms like cold or do hot which might be like an    
45:59    
internet address or warm which is a variation of hot or    
46:05    
you know a translation into another language of the concept. So there are all these different variations on the    
46:12    
term and so this is kind of how this radio    
46:18    
representation works. So you have list and it has these different variants out    
46:24    
here. Listing list list dash list lists with a capital L. This    
46:33    
is in Russian. This is uh underscore list.    
46:39    
And so you can see if you're using it in different contexts, if it's just seeing list, it's not    
46:44    
getting any contextual information and it might not be getting any syntactic information either. You need to prompt    
46:51    
it with these kinds of relationships.    
46:56    
Okay, so that's pretty much it. I think that's a good article. has a lot of good    
47:02    
references and you know it's it's a very good I think primer to embeddings    
47:09    
and of course embeddings aren't just used in language models are used in other types of machine learning as well    
47:16    
where you have you know training sets maybe there's statistical components    
47:21    
that are embedded um you know related image features and things like that so usually a feature    
47:28    
space is similar But in large language models, we use this yeah we use this embedding as the    
47:36    
sort of place to represent distinctions between words, relationships between    
47:42    
words and so forth. Then it's all a statistical tool so that we can then    
47:49    
translate that into an output.    
47:54    
Yeah. Well, yeah, VD, thank you for your comment. Uh yeah, that was good to go    
48:00    
over that. All right. Um so that was Thank you to    
48:05    
Morgan for that. That was uh from hugging face space.    
48:11    
Okay. Um next up, so this was something that dee he put in    
48:18    
one of the channels. This was self- adapting language models.    
48:25    
And this is um the archive. This is a group of people from MIT    
48:32    
who are talking about this new type of language model called the self adapting.    
48:44    
So why don't we go through the paper and see how this differs from a typical    
48:49    
language model. All right. So large language models are    
48:54    
powerful but static. They lack mechanisms to adapt their weights in response to tasks, knowledge or    
49:02    
examples. So as we saw in the last example, we have the latent space.    
49:08    
Uh we have this embedding and this latent space can be static or    
49:15    
it can have other properties. But it's really just sort of, you know, it it its    
49:21    
weights are limited to its training set. So if it sees new examples of things, it    
49:28    
can't necessarily generalize. So this is always the sort of the    
49:34    
problem in um in machine learning and large you    
49:40    
know building large language models is that generalization is very hard. Getting that expressivity is very hard.    
49:47    
from that latencies. So we want to have some sort of way to adapt the different weights in in this    
49:55    
uh embedding that we saw in the large language models to respond to new    
50:01    
features. So if we give it a document that's totally different than documents    
50:07    
been trained on, would it be able to respond properly?    
50:14    
And so this, you know, we expect maybe this out of a human language speaker, right? I mean, human language speakers    
50:20    
are always encountering new training sets and new data and they have to    
50:26    
incorporate them sometimes not so successfully, but you have to respond to    
50:31    
them in some way. And so in humans with language, we can adapt pretty easily,    
50:38    
relatively speaking, relative to a large language.    
50:43    
So we introduce here self- adapting orange language models or seal which is    
50:50    
a framework that enables large language models to self adapt by generating their    
50:55    
own fine-tuning data and update directives.    
51:00    
So they actually generate their own sort of um you know they they generate data    
51:06    
sets that fine-tune their training set and then this allows    
51:12    
them to update directives. Given a new input, the model produces a selfedit,    
51:19    
a generation that may restructure the information in different ways, specify    
51:24    
optimization hyperparameters, or invoke tools for data augmentation    
51:29    
and gradient based updates. through supervised fine-tuning or SFT.    
51:35    
These selfedits result in persistent weight updates    
51:40    
enabling lasting adaptation to train the model to produce effective    
51:46    
self-edits. We use a reinforcement learning loop using the downstream performance of the    
51:52    
updated model is to reward s. So we've seen this before using    
51:58    
reinforcement learning in larger language models. You've seen this with the reasoning models or the reasoning    
52:03    
techniques where you're taking the large language model and you're refining it using this reward signal.    
52:10    
So, and it makes sense because if you want to find, you know, you have basically the model trained and you have    
52:17    
this uh embedding and the relationships in the embedding, how do you refine it?    
52:24    
And they mentioned some things in the last article we saw on how to refine the results. and fine-tune them. But in this    
52:31    
case, you're using reinforcement learning to kind of, you know, say this    
52:37    
is the thing we should be doing. We should look at the updated model and see how that performs relative to the    
52:45    
standard large language model and then reward the updated model.    
52:53    
Okay. So people are using a lot of and this is in terms of self-edit. So when there's a self-edit that's you know uh    
53:01    
doing well we should enforce that signal. Unlike prior approaches that rely on    
53:07    
separate adaptation modules or auxiliary networks seal directly uses the model's    
53:13    
generation to parameterize and control its own adaptation process.    
53:18    
Experiments and knowledge incorporation of fshot generalization    
53:24    
show that seal is a promising step towards language models capable of self-directed adaptation    
53:30    
in response to new data. And so they have their website and code here this GitHub site. So if we go to this this is    
53:38    
self- adapting language models. This is their some of their material    
53:45    
supplemental materials. Here you have this method where they have this reinforcement learning outer loop    
53:51    
iteration T. So this just shows the seal technique.    
53:57    
So in each reinforcement learning outer loop iteration, the model generates    
54:02    
candidate self-edits. So it has this set of self-edits that we can choose from    
54:08    
directives on how to update the weights. Applies corresponding updates. evaluates    
54:14    
performance on a downstream task and uses the resulting rewards to improve the self-edit generation policy. So in    
54:22    
this case it's generating num many self-edits and it's selecting the best    
54:27    
selfedits and then it's using that as the update for the large language.    
54:33    
So that's what is as they present the method here. Um then they do these experiments. They    
54:39    
test CO2 domains knowledge incorporation where the task is to fine-tune the model    
54:46    
to internalize the factual information from a given passage such that it can    
54:52    
answer related questions that access to the original content. So here we have    
54:57    
the passage which is title Apollo program    
55:02    
context. But even after NASA reached internal agreement, it was far from smooth sailing.    
55:10    
The self-edit then is the Apollo program faced opposition from    
55:15    
Kennedy science advisor Gerald Meisner who had dot dot dot which gives more    
55:22    
context or more detail to the passage. And then the evaluation step is is who    
55:31    
was Kennedy's science advisor that opposed man's spacecraft flights? And then the answer is Joe Wiser. So it's    
55:37    
doing this uh self-editing and then it's evaluating the self-edits and then it's    
55:43    
updating the original passage so that it's incorporating the specific    
55:49    
knowledge. So that's knowledge incorporation. Um    
55:54    
so you can get information from different passages and improve the model    
56:01    
from the self-edits. Then there's fop learning on arc which    
56:07    
is this is the arc prize that Morgan likes to talk about. This is Hshot    
56:13    
learning where the model must generalize from a small number of demonstrations by    
56:18    
generating its own data augmentations and training configurations to solve abstract reasoning tasks. So in this    
56:25    
case we have fshot examples. We feed that into our model. Then we    
56:31    
have these self-edits where it kind of talks it gives metadata    
56:36    
about the augmentations and then defines a strategy with a learning rate over three epics    
56:44    
then trains the model here and then there's this evaluation of the few shot examples and so it finds    
56:52    
the right set of few shot examples for this uh example. So this is these    
57:00    
are basically the two experiments that they do here. They test seal in these two different domains    
57:06    
and then the results here for knowledge incorporation. We evaluate seal in the task of    
57:12    
assimilating factual knowledge from textual passages in the single passage setting. After two rounds of rest-m    
57:21    
seal improves QA accuracy from 32.7 with no adaptation    
57:27    
to 47% of performing models fine-tune passages or synthetic data generated by    
57:34    
G GPT4.1. So in doing this question answer    
57:39    
accuracy you go from 32.7% to 47%.    
57:45    
And this is um you know with this seal method being input    
57:52    
in the continued pre-training setting with 200 passages. SEAL again achieves the highest performance at 43.8%.    
58:02    
indicating that its learned editing policy skills beyond the simple passage setup in which it was reinforcement    
58:09    
learning train. These results highlights Heel's ability to convert unstructured    
58:14    
text into fine-tuning data that yields lasting and efficient knowledge    
58:19    
generation. So this shows here a single pallet passage knowledge incorporation    
58:27    
for SEAL versus some of these other models.    
58:34    
Then there's few shot learning. So in a simplified subset of the ARC benchmark.    
58:39    
So this is this arc uh prize benchmark seal achieves a 72.5%    
58:45    
success rate significantly outperforming in context image has a 0% success rate    
58:53    
and test time training with untrained selfedits which is a 20% success rate.    
58:58    
This demonstrates Seal's ability to learn how to configure augmentations and training strategies autonomously,    
59:05    
enabling robust generalization from limited demonstrations. So in the fot    
59:11    
learning example, it's trying to figure out how to augment the data and how to    
59:16    
train the model in this, you know, in terms of like improvement. So the fot    
59:25    
learning is doing that. And so seal actually has a much higher success rate    
59:31    
than some of the other techniques that they try or methods that they try.    
59:37    
Um and so the limitations of course while seal enables longlasting    
59:42    
adaptation through self-generated weight updates our continual learning experiment reveals that repeated    
59:48    
self-edits can lead to catastrophic forgetting which is where you forget the    
59:54    
previous training when you especially when you switch contexts. So when you do repeated self-editing you can actually    
1:00:01    
forget a lot of things the original information. So performance on earlier tasks degrades    
1:00:08    
as new tasks are applied. This suggests that without explicit mechanisms for    
1:00:13    
knowledge retention self-modification may override valuable prior information.    
1:00:20    
Addressing this remains an open challenge with potential solutions including replay constrained updates or    
1:00:28    
representational superposition. Okay. So then their future work looking    
1:00:34    
ahead we envision models that not only adapt their weights but also reason about when and how to adapt deciding mid    
1:00:42    
inference whether a selfedit is warranted. This is why they're using reinforcement learning and because we    
1:00:49    
talked about how reinforcement learning is used for model reasoning and this is    
1:00:54    
a form of model reasoning although it's not exactly what other people are using reinforcement learning for.    
1:01:01    
So, it needs to decide when a self-editor is warranted, of course, because not all things need editing. And    
1:01:09    
so, you know, it's not doing this with any sort of human feedback, as far as I'm aware here from from their    
1:01:15    
description. Suchi such systems could iteratively distill chain of thought traces into    
1:01:21    
weights, transforming ephemeral reasoning and permanent capabilities and    
1:01:26    
offering a foundation for aentic models that continuously through interaction    
1:01:32    
and reflection. So basically these are not incorporated in plates directly but    
1:01:38    
the idea is that eventually you could incorporate these different types of you    
1:01:44    
know figuring out when to adapt and then making those changes and then incorporating it directly into the    
1:01:50    
weights of the so would be a fully    
1:01:56    
sort of autonomous process.    
1:02:02    
Okay. Okay, so I think that's enough for those two articles. I know they're related. That's why I pulled them up.    
1:02:08    
Thank you to S or thank you to Vei and Morgan for posting those in the Slack.    
1:02:17    
Okay, so looks like Sar is here. Welcome Sara.    
1:02:26    
So I should ask if anyone wants to give updates. So Sara is    
1:02:33    
comments here. The mic isn't working. So, she just said quite insightful just    
1:02:39    
saying I joined late but saw the second paper and thanks a lot for going through it. Yeah,    
1:02:49    
why don't we move on? Um, let me talk about this. This is something that I've pulled up. Um,    
1:02:56    
there are two papers here. The first one is this paper, a meta theory of classical and modern connectionism.    
1:03:04    
And the second one is this Rosetta Stone for connectionism. It's a classic complexity theory paper actually. And so    
1:03:12    
there's this history of connectionism that um is    
1:03:20    
you know I I don't think we reflect on it enough. It was an early a fundamental    
1:03:25    
part of early cognitive science. Uh and you know we we use it like ubiquitously    
1:03:31    
in a lot of the machine learning models that we use today and you know even in large language    
1:03:37    
models but we don't really reflect on what's going on in connectionism what    
1:03:43    
the theoretical claims are in connectionism and some of those more philosophical    
1:03:49    
theoretical aspects of it. So the first thing this is a quote from    
1:03:54    
Margaret Bowden in this paper here. And so, uh, what's significant about this is    
1:04:00    
that Margaret Bowden died recently and she was a pretty big heavy hitter in    
1:04:06    
cognitive science, um, having written the book that Jesse always references,    
1:04:13    
um, about sort of the history in the different strands of cognitive science,    
1:04:19    
which is, you know, a very interesting topic. And this actually this paper draws from    
1:04:28    
Margaret Bowden's book in 2006, mind is a machine, the history of cognitive    
1:04:34    
science, two volume set. So a lot of this paper draws from the work from    
1:04:41    
that. So this paper is a meta theory of classical one modern connectionism. This    
1:04:47    
is Olivia Guest and Andrea Martin and they talk about the so they want to    
1:04:55    
frame connectionism sort of as this classical phenomenon than this modern.    
1:05:01    
So it changes over time in terms of its aims and its goals. And when they say connectionism,    
1:05:07    
they simply mean like these models where you have nodes and their connections.    
1:05:16    
And those connections yield sort of patterns of processing    
1:05:21    
that are correlational that are associative and they can be    
1:05:27    
used in terms of modeling memory and modeling other cognitive aspects. In    
1:05:34    
fact, they were invented as a way to model cognition in the brain. And you    
1:05:39    
know, being very abstract models, they don't map to the brain in any    
1:05:44    
specific way. But what they do produce is this sort of um you know these sort    
1:05:51    
of outputs that maybe look like they are neural or something like that.    
1:05:58    
So we start with the abstract. Contemporary AI models owe much to the of their success and discontents to    
1:06:05    
connectionism, a framework in cognitive science that has been and continues to    
1:06:10    
be highly influential. Herein we analyze artificial neural networks or ANNS    
1:06:18    
in a couple of different ways. The first is that when used as scientific    
1:06:24    
instruments of study and then when functioning as emergent arbittors of the zeitgeist the cognitive    
1:06:31    
computational neural sciences but we'll get into what that means later    
1:06:37    
building on our previous work with respect to analogizing between artificial neural networks and cognition    
1:06:44    
brains or behavior. So they wrote a work on this sort of analogy    
1:06:50    
between artificial networks and cognition, artificial neural networks and brains, artificial neural networks    
1:06:58    
and behavior. So this is an outstanding issue in cognitive science with connectionist models, you know, how sort    
1:07:05    
of reliable are they as analogies? And we've talked about in our group    
1:07:12    
before about the machine analogy. So how    
1:07:17    
much is biology like a machine? How much is the brain like a machine?    
1:07:24    
You know, how much is a computer like a brain? Things like that.    
1:07:30    
We use meta- theoretical analysis techniques including formal logic to    
1:07:36    
characterize two distinct tendencies within connectionism that we dub classical and modern with divergent    
1:07:43    
properties. So these different classical and modern eras have different goals    
1:07:50    
mechanisms and scientific questions associated with it. So the classical connectionism would be sort of the    
1:07:57    
connectionism of maybe the 1980s when it first emerged and then the    
1:08:02    
modern connectionism is where it's been applied in machine learning and other types of    
1:08:09    
technologies. This is are two different very different application domains and of course the    
1:08:15    
scientific questions are different the goals are different and so forth.    
1:08:20    
We also demonstrate how we as a field often fail to follow important lines of argument to the end. This results in a    
1:08:28    
paradoxical practice. So this is where you know people will often    
1:08:35    
not follow through with their arguments. The field kind of evolves away from the    
1:08:41    
original intent. So it's it's it's interesting. By engaging more deeply    
1:08:47    
with meta theories surrounding artificial neural networks, our field can obiate the cycle of AI winters and    
1:08:54    
summers which need not be inevitable. Okay, so this is the quote from Margaret    
1:09:00    
Bowden. Connectionism was conceived almost three centuries ago in the 1740s    
1:09:07    
but had a long gestation. In its embryionic form, it was merely biologized introspection.    
1:09:14    
David Hartley's view of that thinking is grounded as associative mechanisms in the brain is the sort of the first    
1:09:23    
appearance of connectionism and just thinking about how    
1:09:28    
thinking is related to the brain. So according to Bowden, connectionism    
1:09:34    
has been around in some form or another for almost centuries. Even a more conservative estimate still places    
1:09:40    
connectionism's beginnings in the 1940s, making it much older than the current technology industrydriven hype cycle in    
1:09:48    
artificial intelligence research. Connectionism has gone through a boom and bus cycles, so-called summers and    
1:09:55    
winters. Thus statements such as we attend today an explosive infatuation    
1:10:01    
with this once old style but now new fashion view of cognition    
1:10:06    
which was from a paper by berscini in 1989.    
1:10:11    
Um wondrously are as applicable now as when they were written back then.    
1:10:18    
In this paper, we aim to critique and juxtapose modern connectionist stances    
1:10:23    
with those around prior to 2010 when it had when it can be argued that classical    
1:10:30    
pre-deep learning pre-widespread GPU use era ended. So basically the their    
1:10:37    
division line is about the year 2010. So the connectionism that was practiced    
1:10:44    
before 2010 is classical. The connection isn't practiced after 2010 is modern and    
1:10:50    
this is where we have our modern tools of deep learning GPUs    
1:10:57    
um and other types of tools that uh were different than the way connectionism was    
1:11:04    
applied especially in the 80s and 90s. Importantly, contemporary AI models much    
1:11:12    
of their success and discontents the connections. Um, and so there is this connection    
1:11:18    
between sort of the summers and winters of AI as we know them historically    
1:11:24    
and connectionism and its successes and failures.    
1:11:30    
So that's where they're going with this. We present a nuanced critical    
1:11:36    
perspective on artificial neural networks when used as scientific instruments of study    
1:11:42    
such as computational models of brain and behavior and used in fields like cognitive    
1:11:48    
computational neuroscience. Um and this is used in contrast to when    
1:11:54    
artificial neural networks are used as statistical and engineering methodologies    
1:12:00    
for example in non-scientific engineering oriented AI uses.    
1:12:05    
Uh notwithstanding there are important overlaps between the technology sector which is driven by profits and    
1:12:13    
engineering and science. um and so this results in important undesirable contradictions and in    
1:12:20    
conflicts of interest. The analysis presented in this paper is centered on the idea that both critics    
1:12:28    
as both critics and advocates we as a field must follow important lines of argumentation to their logical    
1:12:34    
conclusions. uh we we examine the effects of the converse for example when we take    
1:12:41    
defensive rhetorical positions too far discussing the scientific and engineering contributions    
1:12:48    
of and purported capa capacities of artificial neural networks um you know we we don't want to take uh    
1:12:56    
positions away from sort of their grounding to do this we propose a bisection of the    
1:13:03    
connectionist tendency into roughly 320 2010 which we did classical connectionism and abbreviated to this    
1:13:10    
symbol here and then post20200 which is modern connectionism with this    
1:13:15    
symbol here. So it's CNM but stylized. Uh such a distinction accommodates a    
1:13:22    
variety of related scientific events occurring as a function of so-called deep learning artificial neural networks    
1:13:29    
becoming computationally feasible and accessible to many scientists around the world.    
1:13:36    
So this is uh again I don't know I think this might be a little bit of an    
1:13:42    
artificial dividing line is artificial neural networks as they define them were used in engineering well before 2010    
1:13:50    
and people were using A&Ns as models of brain and cognition    
1:13:55    
before 2010. I'm not really sure if that's a great dividing line but whatever.    
1:14:02    
Uh so this shows sort of this mark one perceptron which was an early    
1:14:09    
connectionist model. Um you have the retina the retinal circuits or the retinal units. You have    
1:14:17    
sensory units here. This is supposed to be the sort of sensory inputs in a brain    
1:14:24    
but it's kind of represented as an engineering input. You have networks of    
1:14:29    
random connections. This these can map to association units    
1:14:34    
or a units and then those further map to this network of many to one connections    
1:14:40    
with feedback groups. So these are response units. So you can see that you have the input you have the association    
1:14:48    
layer or the association units and then you have this response unit. So it's like input hidden layer output. It's    
1:14:55    
that basic structure of a connectionist network. It's that basic structure of a    
1:15:01    
of a neural network or an artificial neural network. And it's interesting    
1:15:06    
because last week we talked about um the work of Braenberg and how his biological    
1:15:14    
cybernetics um enterprise worked. So he you know he was interested in neuroanatomy. He was    
1:15:21    
also interested in brainber vehicles and so uh you know Braenberg kind of wanted    
1:15:27    
to unite the two. So he wrote a lot of papers on neuroanatomy, sometimes very    
1:15:34    
deep and informative papers on neuro anatomy and then switched over into I    
1:15:41    
guess later Miss Cra started to find more and more commonality with these    
1:15:46    
kind of computational models and the computational structure of a lot of the    
1:15:52    
like I said insect uh nervous systems that he was studying. So it's a very    
1:15:57    
different road to this than what they're describing here. It was taking biological systems and moving towards    
1:16:04    
these artificial systems or these models. In this case, people are using    
1:16:11    
bi I guess biological inspiration very broadly to model these inputs and    
1:16:16    
outputs. Then if we think about Braenberg vehicles which came later in Brightenberg's career, we know that this    
1:16:23    
is basically the model that he used as well. and it doesn't really you know u map to    
1:16:30    
this in in the same way but you had the inputs sensory inputs in this case    
1:16:35    
visual inputs you then have association units where you have some maybe some    
1:16:41    
mapping between the sensor and aector and then you have the output or the    
1:16:47    
aector which is this simple command that goes out into the world and you can have    
1:16:52    
these feedback loops between the uh response units to fine-tune    
1:16:59    
what's going on. And if you look at some of the vehicle designs that Brain proposed, this is exactly the structure    
1:17:06    
he proposed. And so it's interesting how you know the early    
1:17:16    
receptrons, the early models of this kind of have this basic structure    
1:17:23    
uh which is biological that's extremely reduced biology and extremely reduced    
1:17:30    
cognition. Okay. So that's uh and so um    
1:17:38    
some people you know this is kind of this type of model inspired a lot of    
1:17:44    
work. So the history of it and compressing this down because there's a lot of history here. This is the    
1:17:52    
hardware artificial neural network built by Frank Rosenblad. This is the Mark1 perceptron.    
1:18:00    
And so this is sort of a parallel structure of biological neural systems.    
1:18:07    
And so you can see this um where you're not really proposing any brain    
1:18:14    
like components, but you have this sort of information processing aspect of the    
1:18:19    
nervous system that you're trying to replicate. And so it's interesting that this blurs    
1:18:25    
the line between mechanistic and functional modeling. And you know in the field they use terms    
1:18:31    
like micro structure of cognition which was something Rumbleheart coined in 1986    
1:18:38    
Rumble Hart at all and using this um in a way that's    
1:18:45    
sort of opposed to function approximation. So you know we have connectionism we have    
1:18:53    
these models and there is for a lot of different types of sort of studies. So,    
1:18:58    
one of the things about connectionism is they're very diverse in terms of how you can use them. It's like kind of an    
1:19:05    
all-purpose architecture that allows you to study a lot of different things or think about them in    
1:19:11    
a lot of different ways. And so, as a theory, it's kind of interesting because    
1:19:17    
it's gives you maybe a lot of potential predictions, but it's a very general architecture. I    
1:19:24    
guess it's interesting about it is it's sort of quai universality, right? Like    
1:19:30    
it's universal in the sense that it's capturing something in nervous systems    
1:19:36    
or in the brain that exists mainly this    
1:19:42    
associative aspect, right? You have sensory inputs. They get associated with    
1:19:48    
things and then there's this output and it can map to behaviors    
1:19:55    
or it can map to other types of perception. It's very general    
1:20:01    
and you know it's it's it's interesting how this is all kind of    
1:20:07    
how this was applied at least in this classical period.    
1:20:12    
Okay. So before we can do any of this, what is connectionism is sort of a term.    
1:20:18    
And as I mentioned already, this term is pretty general. It's just this idea of having these    
1:20:25    
connections and having these associations that map to outputs.    
1:20:31    
So according to Rumlhard at all in 1986, it is the notion that intelligence    
1:20:37    
emerges from the interactions of larger numbers of simple processing units.    
1:20:43    
So intelligence is basically the ability to make these associations internally    
1:20:48    
and then map them to behaviors. So you could imagine like if you had sensory inputs through this retinal structure or    
1:20:57    
through a bunch of sensors those inputs might take different scenes    
1:21:02    
from you know from the environment or they might be overlapping    
1:21:08    
scenes. It might be redundant scenes and it maps them to all the things that are    
1:21:16    
associated in those fields. So it could be like a feature space,    
1:21:22    
it could be you know kind of merging the scenes together in terms of the common    
1:21:28    
components and then finding every possible association    
1:21:33    
and then using every possible association to produce coherent responses. So you have this sort of    
1:21:40    
generative aspect where you have these input sensory inputs and you have then this association layer that's finding    
1:21:47    
all the possible kind of interesting things or all the possible correlational    
1:21:52    
relationships and then it's reducing it down to these coherent outputs which are    
1:21:58    
maybe zeros and ones. It could be different behavioral responses moving    
1:22:03    
forward, moving backwards, anything like that. So it's a selection aspect at the    
1:22:09    
back end. Okay. So um this framework has been    
1:22:16    
variously called parallel distributed processing neural network modeling or    
1:22:22    
connectionism and the term so Donald Taboo worked on    
1:22:28    
associative memory. Uh he introduced the term connectionism    
1:22:34    
uh in terms of the brain because he was doing a lot of things with um    
1:22:40    
associative learning and things like that and um discovered things that led    
1:22:47    
to long-term potentiation and things like that. So uh mo most broadly    
1:22:53    
connectionism is the drawing of parallels between artificial neural network models and the brain and    
1:22:59    
cognition specifically using them to model neural cognitive systems or phenomena. Worthy of underlining here is    
1:23:07    
that connectionist networks typically do not bear a transparent relation to the neurological structures that realize    
1:23:14    
them. And so the description of connectionist networks as neural nets is    
1:23:20    
somewhat misleading. So again, you know, you think about like    
1:23:25    
the work of Donald Heb thinking about associated memory and then you know,    
1:23:31    
thinking about the neuros neurological structures underlying them,    
1:23:36    
but then you think about connectionist networks being so abstract from that that it's hard to go and make    
1:23:43    
that mapping. Now like I I said there might be like um    
1:23:49    
you know I I could take the other case and say like if you look at the work of Braenberg he actually makes that    
1:23:56    
connection a little bit more explicit. And so that's you know that's a an    
1:24:01    
interesting counter example. Artificial neural networks is mathematical objects implemented on    
1:24:08    
digital computers that involve banks of units. artificial neurons moved into    
1:24:14    
layers that propagate activations to outer other banks of units or recurrently back to themselves.    
1:24:22    
Um, and then you also have to have some sort of mathematical function to process them is sort of the basis for these    
1:24:30    
types of structures. So these structures are mathematical.    
1:24:37    
They involve different banks of units. So things have to be discretized in a certain way.    
1:24:43    
You have to group things into layers which may or may not exist in the brain. But functionally the layers exist or    
1:24:50    
structurally the layers exist. And then you have these mathematical functions which operate sort of in a way    
1:24:57    
that maybe action potentials might act or multiensory integration might act.    
1:25:04    
But doing this in a mathematically tractable way that's sort of    
1:25:09    
um that's sort of universal or systematic.    
1:25:15    
Okay. So learning is achieved say for example using back propagation by    
1:25:20    
changing the numerical value of the connection weights artificial synapses    
1:25:26    
between and within layers as a function of difference between the network's output behavior and some target state    
1:25:34    
from a schematic of hardware ANNS. The mark one perceptron serves as a good    
1:25:39    
example and even in that n stage such models resemble moderate connectionist framings    
1:25:46    
with respect to input you know different types of neural structures. So your    
1:25:52    
input units are retinal cells. Your association units are you know in what    
1:25:57    
some people in neuro anatomy used to call association cortex which is neoortex.    
1:26:03    
Uh and then response units which are aectors in you know in in the organism which could    
1:26:11    
be muscles. They could be something in the spinal cord. It could    
1:26:17    
be something in the language production area. So there are a    
1:26:22    
lot of ways to map these inputs and outputs and association    
1:26:29    
uh mechanisms to the brain, but they're not explicitly neural.    
1:26:36    
Okay. So this is I'm going to go through to this table here. This is these are    
1:26:42    
the meta theoretical claims or commitments between classic connectionism which they    
1:26:49    
define as pre-2010 and modern connectionism which is close 2010. And the idea is going to be to go    
1:26:56    
through the goals, the questions, the theories, the mechanisms, the relation    
1:27:03    
to the brain and then the training that a network needs. And you're going to see    
1:27:08    
that there differences between these two arrows. just citing papers from the two eras and how this has evolved over time.    
1:27:17    
So the goal in classical connectionism is in understanding the repercussions of    
1:27:23    
our theories. So for example, models are tools for exploring the implications of    
1:27:30    
ideas. So you know again you can use say in in    
1:27:35    
classical connectionism you can use a connectionist model to understand    
1:27:41    
different hypotheses you might have about the brain or maybe more specifically about associative work.    
1:27:49    
And so in that sense, they're great tools because they give you this way to test hypotheses that you couldn't    
1:27:56    
necessarily test by using like neuro imaging or you could inform neuroiming    
1:28:03    
through these models. And in fact, in cognitive science, there's this rich history of using    
1:28:10    
computational models to augment experiments. And in you know, classic cognitive science, they didn't really    
1:28:16    
have cognitive neuroscience. I mean cognitive neuroscience relative to the    
1:28:22    
split in 2010 uh is interesting because we had say    
1:28:27    
like electrophysiology all throughout the history of cognitive science but like things like fMRI didn't    
1:28:35    
appear until maybe like the early 90s in any meaningful way. So it wasn't until    
1:28:42    
the 90s that we started getting results from say like fMRI we had EEG results    
1:28:49    
earlier but mapping those to the mind or mapping those to cognition    
1:28:55    
was you know was they were kind of disconnected almost and so you know people would use    
1:29:01    
connectionist models to think about how the theory of cognition in terms of    
1:29:09    
information processing mapped onto the brain. Some    
1:29:15    
models are used to understand the theories within connectionism which themselves are about understanding brain    
1:29:21    
cognition and behavior. Additionally, a good fit never means that a model can be    
1:29:26    
declared to provide the true explanation for the observed data.    
1:29:33    
So this is where we have the model and we have the phenomena and we have this goodness of fit or this you know this    
1:29:41    
sort of realism where the model has to be you know close or approximate    
1:29:49    
the uh cognitive system. Uh but however if we do get a good fit    
1:29:56    
it should never be declared that that's the the true explanation. So it just should inform the true explanation. It    
1:30:04    
shouldn't take the place of a good explanation or of a true explanation.    
1:30:11    
Now in the post2010 period there's a difference in terms of how people are thinking about this. So these citations    
1:30:18    
are from actually from about 2018 to 2021. Um so the in this case the goal of    
1:30:25    
science is to be able to predict what systems are going to do. These artificial neural networks get us closer    
1:30:31    
than to the goal of neuroscience. And so when we say we understood a phenomena first and foremost that means    
1:30:39    
that we can predict all the explainable variance in the data or any input in the    
1:30:44    
domain over which the model is playing to hold. So it's much more aggressively    
1:30:50    
predictive in terms of how we can use ANNS to understand neuroscience    
1:30:59    
which is which is interesting because the theoretical sort of aggressiveness    
1:31:05    
is increased in over time with models and maybe    
1:31:11    
that's because you have better hardware. um you have maybe more refined models or    
1:31:17    
refined techniques but also because we also have a lot more neural imaging data    
1:31:22    
available by 2010 and on. Okay. So then the questions so in    
1:31:30    
classical connectionism the questions um this question here can    
1:31:35    
a connectionist principles give rise to similar behavior in brain data or    
1:31:40    
cognitive capacities as seen in humans? Okay, so basically can connectionist    
1:31:46    
models replicate the the brain states or cognitive capacities of humans    
1:31:54    
and so no specific prediction requirements are imposed on the models and anatomical mappings if present or    
1:32:00    
baked in. The model is forwarded as a way to explore theory. That's from    
1:32:05    
Mcbama in 2009. And so that's kind of the way that they    
1:32:11    
think about this. And then in the modern era which is post 2010    
1:32:18    
the question is can ANN's predict here used to mean correlate with behavioral    
1:32:24    
or brain data as such they are used like inferial statistics or frame like    
1:32:29    
theoretical models. So basically you have a more aggressive predictive stance    
1:32:35    
and as a result you're using inferial statistics and ann together to make    
1:32:41    
these types of predictions. Uh and then there's a Daniel Yaman's in    
1:32:46    
this uh reference from 2021. Not only did we get good predictions but there's    
1:32:53    
also there's a kind of anatomical consistency. So there's a claim that you can get    
1:32:59    
better sort of synergy with neuroiming data with you know between these kind of    
1:33:07    
models in the brain etc. Then there's the theory. So in the    
1:33:12    
classical era theory is implemented by the model. The model is not a standin    
1:33:17    
for theory. For example, we consider a simple computational implementation of the    
1:33:23    
theory in which visual representations of objects and perceptual representations of verbal statements    
1:33:30    
about these objects interact with one another by means of an intermediating    
1:33:36    
semantic system. So we don't necessarily use the model as    
1:33:42    
theory. We're just implementing theory in the model. So if we have    
1:33:48    
an artificial neural network in the classical era that model usually model is a theory. We    
1:33:56    
have a theory about the brain. We have a theory of cognition how what occurs in    
1:34:01    
the brain. We build the model. We'll run the model and if the model fits the data    
1:34:07    
available data you know either cognitive or neuroiming    
1:34:13    
then our modeling exercise has been a success.    
1:34:18    
If not we go back and refine our model but we never necessarily go back to    
1:34:24    
refine our theory. Sometimes we refine our theory we also have to refine the model.    
1:34:32    
In the modern era in terms of theory, theory is the model. So theory is    
1:34:39    
instantiated in task performing computational models. So basically the    
1:34:46    
model is the theory. They're not separated. It's, you know, you're not using    
1:34:52    
the model to sort of represent the theory. That's actually the theory. Um    
1:34:57    
that's from u the citation in 2018. Additionally, theorizing is often    
1:35:03    
inspired by engineering systems, not nature directly.    
1:35:09    
Current computational neuroscience practice looks to AI, which has historically provided a fund of ideas    
1:35:15    
for biological theories. So yeah, this is sort of the uh idea    
1:35:23    
that sort of there's a stronger connection between models and theory and    
1:35:28    
then using these kind of models to    
1:35:33    
um you know kind of fill maybe if you're doing something like computational neuroscience using these models to fill    
1:35:40    
in the gaps. There's this um there's this    
1:35:46    
basically this lament in neuroscience that neuroscience doesn't have theories    
1:35:52    
which is probably not true but that's kind of you know the ID that that also plays into this a little bit is that    
1:35:59    
neuroscience doesn't have theories the way physics does or the way biology sometimes does. And    
1:36:08    
so it's it's like there's a lot of data generated but very little theory interpreted. Um I'm not sure if I agree    
1:36:15    
with that but this kind of speaks to the idea that theory is basically being supplanted by    
1:36:23    
models and it's an interesting kind of thing to reflect on I think    
1:36:29    
so in terms of mechanism mechanisms are proposed which the model embodies and    
1:36:35    
experiments are done to show proof of concept. So one example of this is can    
1:36:40    
connectionist principles give rise to phenomena and or capacities of interest.    
1:36:47    
The artificial neural network allows us not just to probe the response to a given stimulus test. So you know you    
1:36:54    
might do you might model some sort of cognitive experiment where you want to see what    
1:37:00    
the underlying uh neural response is. And I put that in    
1:37:05    
quotes because again we're not looking at the brain directly. We're looking at this connectionist model    
1:37:12    
but also to ask questions about the nature of the existing representations the model is.    
1:37:18    
So we can actually think about cognitive representations using model and again    
1:37:24    
this is the classical view. So they were very much interested in hypothesis testing and then having things that you    
1:37:32    
could actually probe. Um and again this was all sort of    
1:37:39    
maybe semi-independent of um    
1:37:44    
of neuroiming. Okay. And then in the modern era the    
1:37:50    
mechanism is as follows. The model was assumed to be equivalent in some way to a cognitive    
1:37:56    
or neural system and experiments are done to support this assumption.    
1:38:02    
The core idea is to treat an artificial neural network as a participant in a    
1:38:07    
psychology experiment in order to tease out the systems mechanisms of decision making, reasoning, cognitive biases, and    
1:38:16    
other important psychological traits. So this is basically where you have you    
1:38:23    
can probe the neural artificial neural network and treat it kind of like    
1:38:30    
a a cognitive entity that you know you can explore which is you know    
1:38:36    
interesting and it kind of follows from the classical view the classical view being a little bit more    
1:38:43    
um you know self-aware about being models    
1:38:49    
In this case, you're almost saying that well, the ANN can take on the properties of    
1:38:56    
an experimental participant, which I don't know if that's necessarily the case.    
1:39:02    
I guess, you know, you could criticize that view, but you probably also then want to criticize the classical view as    
1:39:08    
it seems like it's kind of the evolution of that, but whatever. Um, then there's    
1:39:14    
the brain. So linking this back to the brain, linking this back to the neuroiming of the neuroscience.    
1:39:22    
Brain regions if related to models are presented as being modeled not as    
1:39:28    
uncovered correlationally. Theory or some knowledge to be modeled    
1:39:33    
understood system comes first and these ideas are placed into an artificial neural network    
1:39:39    
model modeled purposely. So there's it's it's about like    
1:39:45    
basically modeling different brain regions    
1:39:50    
and so that associative layer is modeled. We know kind of what we want in    
1:39:56    
the model and we put those features in and then we see if it matches our you    
1:40:02    
know we run the model and see if it matches our theory or hypothesis.    
1:40:07    
Um, so I guess the point here is that you're    
1:40:13    
not uncovering anything necessarily new. And that's interesting because then the    
1:40:20    
modern view is that computational models can help infer the function of brain regions by linking model and brain    
1:40:27    
activity. Multi-layered models are particularly promising in this regard because their    
1:40:34    
layers kind of systematically mapped to brain regions. So this is also interesting because this    
1:40:41    
modern era is defined by deep learning architectures. So if we think about like classic or    
1:40:48    
artificial neural networks of the 80s and 90s, they didn't have a lot of layers. It's just like we had this    
1:40:53    
associative layer and then we had the out inputs associative layer outputs.    
1:41:00    
Later on in this modern era that they define, you have deep learning models, deep learning um representations which    
1:41:08    
have many hidden layers and you can do things in those layers. you can refine    
1:41:14    
um so you have many many different uh associative layers and so you can refine    
1:41:21    
things things can um you know they can sort of act as different brain regions.    
1:41:26    
So you can basically have a network of brain regions within the same model.    
1:41:32    
But then their claim is more about like discovering things within the models    
1:41:38    
that are analogous to discovering things within the brain and you know doing    
1:41:44    
things that are similar to neuro imaging. So this is, you know, quite interesting    
1:41:51    
from the standpoint of discovery. And one might argue maybe there's a little    
1:41:57    
bit of delusion in this and that you have this um you know, is there really    
1:42:02    
are what are you really discovering? Are you discovering things that are being generated like the brain generates them    
1:42:09    
or are you just discovering things that are um    
1:42:15    
are you just discovering things that are epitom?    
1:42:28    
Okay. So then finally training um there is often explicit awareness of the    
1:42:33    
possibility for behaviorist or associate association of stance and the load    
1:42:39    
placed on the training regime and set which in the case of artificial neural networks is statistics in the input.    
1:42:47    
So for example don't freewire structure into your mechanism if it can get it for    
1:42:52    
free from the environment. So this is where there's this aspect of    
1:42:59    
I don't want to say nature versus nurture but there's this interesting aspect of    
1:43:06    
um model complexity and uh sort of the inherent complexity of    
1:43:14    
the model. So, of course, these models were not that, you know, their    
1:43:20    
computational um overhead wasn't that great because a    
1:43:25    
we didn't have the computers to simulate them, but b they were supposed to be simple sort of toy models of of of a    
1:43:33    
brain or of a mind. And so you wanted to see what you could get out of the    
1:43:38    
environment so that you could see how that thing those things from the environment were being processed as    
1:43:45    
information. So you didn't want to pre-wire a bunch of structures into your mechanism. You    
1:43:51    
just wanted to see if the environment shape like a generic connectionist    
1:43:57    
array or an associative array. So that was the thinking and of course    
1:44:03    
this plays into a lot of things like the nature versus nurture debate and other    
1:44:09    
aspects of how you know thinking about different brain regions and defining    
1:44:14    
different brain regions and you know how they're represented in the brain and so    
1:44:21    
there are a lot of sort of parallel arguments within um cognitive science    
1:44:26    
and neuroscience that come into play here. So that was the classical view and    
1:44:32    
then the modern view kind of follows up on that claims about    
1:44:39    
statistics in the inputs. The proverbial ghost in the machine or downplay.    
1:44:46    
So this is this idea of the ghost in the machine where you have inputs    
1:44:53    
you have these correlations and the inputs that you see through associative    
1:44:59    
learning that could be spurious correlations. So we don't know if these    
1:45:04    
correlations mean anything. You can discover correlations that might be spurious and maybe don't relate to    
1:45:11    
anything behavioral, but and maybe they do relate to something behavioral. It's it's a black box in that sense.    
1:45:20    
The model's depth or architecture generally is taken as the important factor. So you know what I said before    
1:45:27    
about the ghost of the machine was that you can have these uh these sporious correlations these    
1:45:34    
outputs that are maybe not or these these patterns that are going relevant in terms of outputs and that's true and    
1:45:42    
what they tried to do in the modern era is overcome that with depth and larger    
1:45:48    
architectures and so that's where you get this important aspect. So in these    
1:45:54    
larger models you might freewire structure of the model. You can certainly simulate the pre-wired    
1:46:00    
structure and you have a wider range of relationships    
1:46:06    
with um model complexity than with the environment.    
1:46:11    
So the training set is not implicated in argumentation except to say it comprises    
1:46:16    
realistic stimuli. And so again, our training set can be    
1:46:22    
much more realistic. In a classical connectionist view, you know, your    
1:46:29    
inputs are very simple. In fact, they might resemble the retina where you just    
1:46:35    
have simple stimuli that hit the retina and then you try to figure out what the    
1:46:42    
sort of the statistical structure of those signals are and then how those    
1:46:47    
relate to the output. In the modern era, we have these much more complex models, much more computationally sophisticated    
1:46:54    
models and we can then as a result have much more sophisticated    
1:46:59    
uh training data and much more. So that's I I don't want to get into the paper anymore. I think it's a good paper    
1:47:06    
that really kind of goes very deep into the philosophy and you know it makes this distinction    
1:47:12    
between classical or classical uh connectionism and modern    
1:47:17    
connectionism. I don't think all of it is warranted, but that's the way they,    
1:47:22    
you know, divide these two up and it's it's really interesting stuff. But I    
1:47:27    
think what's relevant is that they mention in this paper another paper    
1:47:34    
and this paper is an older paper from 1990. So it's in this classical era,    
1:47:39    
it's barely within this classical era, but it's also related to complexity theory. And Jay Dwayne Farmer worked at    
1:47:47    
the Santa Fe Institute. Not sure if he's still around, but he was a early Santa    
1:47:53    
Fe Institute person and he wrote this paper in Physica D 1990 called Rosetta Stone for    
1:48:00    
connectionism. And so this this is kind of in this classical era where they're thinking    
1:48:07    
about artificial neural networks and they're thinking about this aspect    
1:48:12    
of connectionist architectures which is the associative layer right so you're    
1:48:18    
doing you're getting inputs you have this associative layer you have outputs    
1:48:24    
and it was seen as this sort of novel tool of cognitive science this novel tool of computational neural science and    
1:48:32    
then that paradigm has moved into the modern era with a lot of the artificial intelligence stuff.    
1:48:38    
But this paper actually takes a sidetrack from that and talks about sort of the structure of    
1:48:46    
connectionism, the sort of universal structure of information processing    
1:48:52    
as something that is more general to other types of models of you know maybe    
1:49:00    
nature inspired models or biological inspired models. So the abstract here he    
1:49:06    
says the term connectionism is usually applied to neural networks. There are however many other models that    
1:49:13    
are mathematically similar including classifier systems, immune networks,    
1:49:19    
autoc catalytic chemical reaction networks and others.    
1:49:25    
In view of the similarity that is appropriate to broaden the term connectionism.    
1:49:31    
I define a connectionist model as a dynamical system with two properties. So we have this dynamical system which    
1:49:39    
takes inputs and in a dynamical system you're describing some function    
1:49:46    
of which a a system will move. So you're describing some trajectory in a phase    
1:49:51    
space. So instead of describing the connections and those statistical    
1:49:57    
aspects, you're describing the statistics of say a trajectory or    
1:50:02    
set of trajectories in a space that is maybe much like a latent space    
1:50:08    
that we see in in in machine learning models which you know is that    
1:50:13    
associative layer. So I define a connectionist model as a    
1:50:20    
dynamical system of two properties. The first is that the interactions between the variables at any given time are    
1:50:28    
explicitly constrained to a finite list of connections. And then two, the connections are fluid    
1:50:36    
and that their strength and or pattern or connectivity can change with time. So the first point is that you have    
1:50:43    
these interactions that you're capturing. You have them constrained to a finite    
1:50:50    
list of connections. So you're able to trace the um the    
1:50:55    
trajectory through those uh that that connection space.    
1:51:00    
But then those connections are also fluid. So they're not deterministic. And over time those things can change.    
1:51:07    
So as your input changes your associative layer changes its    
1:51:13    
connectivity and that changes the output. So it's not a deterministic system. And    
1:51:21    
which is maybe the key to associative learning is that it's not fully    
1:51:26    
deterministic. It may be quasi deterministic and that if I have a certain input I might be able to say    
1:51:32    
well this is the set of connections that we should see and then we'd have a    
1:51:38    
output set of behaviors. But the as the the the real power of    
1:51:44    
connectionism more specifically this associative is that it isn't deterministic at all.    
1:51:53    
It's a dynamical system meaning it changes over time and then you know    
1:51:58    
according to some function then it also has this sort of itinerance.    
1:52:04    
So things get activated, things die down, the weights change over time and    
1:52:10    
for different time periods you have different weight matrices. So there's this fluidity fluidity of    
1:52:17    
connections. Okay. So this paper reviews the four examples listed above and maps them into    
1:52:23    
a common mathematical framework discussing their similarities and their differences. It also suggests new    
1:52:29    
applications of connectionist models and poses some problems to be addressed and    
1:52:34    
an eventual theory of connection systems.    
1:52:40    
So an interesting aside to this as well of course this is 1990    
1:52:47    
and the first artificial life conference I think was 1988.    
1:52:53    
So this is like around the time of artificial life too. in terms of    
1:52:58    
conferences and of course the Santa Fe Institute was very interested as they continue to be in artificial life. So    
1:53:05    
artificial life is a little bit different than artificial intelligence. It kind of thinks about how do you    
1:53:11    
simulate life and so it you know that's that broader    
1:53:16    
perspective here. So like in terms of connectionism we're not just thinking about like a neural network in terms of    
1:53:24    
modeling cognition or modeling the brain. We're thinking about neural networks maybe in terms of being a class    
1:53:32    
a subclass of a larger class of networks that model biological systems.    
1:53:39    
So yes, the the same criticisms hold true for these other types of models. So    
1:53:46    
for example, if you're modeling immune networks, you know, you're using a network model to model the immune system    
1:53:52    
to model interactions, that may be, you know, the mathematically most tractable way to do    
1:53:58    
it, but it's not an immune network. It's not an immune system. You're abstracting    
1:54:04    
away from that system to describe aspects of that system. Maybe it's the way in which that system operates.    
1:54:13    
Maybe it's, you know, how uh information is processed whatever.    
1:54:19    
So a lot of the same issues here with connection seen in the light of a larger    
1:54:26    
swath of biology. So the one of the purposes of this paper    
1:54:32    
is to identify a common language across several fields in order to make their similarities and differences clear. A    
1:54:40    
central goal is that practitioners and neural nets, classifier systems, immune    
1:54:46    
nets and autoc catalytic nets will be able to make correspondences between work in their own field as compared with    
1:54:53    
others more easily importing mathematical results pressed interdisiplinary    
1:54:59    
uh space or disciplinary boundaries. This paper attempts to provide a    
1:55:04    
coherent statement of what connectionist models are and how they differ in mathematical structure and philosophy    
1:55:11    
from conventional fixed dynamical systems models. So this is a special    
1:55:16    
class of dynamical systems model. Um I hope that it provides a first step    
1:55:21    
towards clarifying some of the mathematical issues needed for a generally applicable theory of    
1:55:27    
connectionist models. So this is actually taking this idea of how connectionist models are this universal    
1:55:34    
type model for information processing and cognition and expanding it out even    
1:55:39    
more to other types of systems. Okay. So if we go to connectionism here    
1:55:48    
um connectionism describes this mo type of model that consists of    
1:55:54    
elementary units which can be connected together to form a network. The form of    
1:55:59    
the resulting connection diagram is often called the architecture. The computations performed by the    
1:56:05    
network are highly dependent on the architecture. Each connection carries information in    
1:56:11    
its weight which specifies how strongly the two variables it connects interact.    
1:56:17    
So any two nodes are two variables that connection then is the weight in    
1:56:23    
term that describes its relative interaction. Since the modeler has control over how    
1:56:29    
the connections are made, the architecture is plastic. This contrasts with the usual approach    
1:56:36    
in dynamics and bifurcation theory where the dynamical system is a fixed object    
1:56:42    
and his variability is concentrated into a few parameters. Um the plasticity of the connections and    
1:56:49    
connection strengths mean that we must think about the entire family of dynamical systems described by all    
1:56:55    
possible architectures and all possible combinations of weights. And so in this kind of model we have    
1:57:02    
three levels at least of dynamics. So when we have a dynamical system we're describing some set of dynamics. In this    
1:57:10    
case we're describing multiple dynamics different levels of the model. So we're    
1:57:17    
looking at the states of the network. We're looking at the values of connection strengths and then we're    
1:57:23    
looking at the architecture of connections themselves.    
1:57:29    
And so these kinds of the structure of mathematical models is not actually unique to neural networks. You see them    
1:57:36    
in other types of networks. You see them as kind of kind of like analogies to this kind of networks.    
1:57:43    
Um and so it has broad applicability.    
1:57:51    
Um and then let's see if we can get into the mathematics. So they make uh    
1:57:57    
significant use of a directed graph creating a connection list.    
1:58:04    
You have dynamics. Um and so    
1:58:13    
you know in conventional dynamic models the form of the dynamical system is fixed and as we discussed this is    
1:58:20    
different from what we need in this case. Um so we have this plasticity.    
1:58:29    
So alternatively we can think of parameters as knobs that can be slowly changed in the background. In reality    
1:58:36    
the quantities that we incorporate as parameters are usually aspects of the system that change in a time scale    
1:58:42    
slower than those we are modeling with the dynamical system. Connectionist models extend this view by    
1:58:49    
giving the parameters an explicit dynamics of their own and in some cases by giving a list of variables and their    
1:58:56    
connections and a dynamics of its own. Typically this involves a separation of    
1:59:01    
time scales. Um the fast scale dynamics which changes    
1:59:07    
the states of the system are usually associated with short-term information processing which is termed transition.    
1:59:16    
The intermediate scale dynamics changes the parameters and is usually associated with learning. I would call this the    
1:59:22    
parameter dynamics or the learning rule. So we have a transition rule and a learning rule. On the longest time scale    
1:59:30    
the graph itself may change. I will call this the graph dynamics. The graph    
1:59:35    
dynamics may also be used for learning. So you have the transition rule, the    
1:59:42    
learning rule and then the graph dynamics which can change. Um and so    
1:59:48    
again we think of this system as this dynamical system of multiple time    
1:59:53    
scales. Um and talking about the information that    
1:59:59    
resides on a graph. Um and then    
2:00:05    
since the representation of the graph is intrinsically discrete, the graph dynamics usually has a different    
2:00:10    
character. Often as in classifier systems, immune networks or autocatalytic networks, the graph    
2:00:17    
dynamics contains random elements. In other cases, it may be a deterministic response to statistical properties of    
2:00:24    
the node states or the connection states. dynamical systems with graph dynamics    
2:00:31    
are sometimes called metadnamical systems. So in all models discussed here the    
2:00:37    
states of the system reside on the nodes of the graph. The system the states are    
2:00:42    
denoted as x of i where i is an integer labeling the node. The parameters reside    
2:00:48    
at either nodes or connections. Theta i reserts to a node parameter    
2:00:53    
residing at node i. WJ reser refers to a connection parameter residing at the    
2:00:59    
connection between node I and J. Um and then so this describes connection    
2:01:06    
strength. This is an important part of connectionist models. And then of course we have these connection parameters and    
2:01:14    
node parameters. And then in some cases such as B cell immune networks, this provides the only    
2:01:21    
means of changing the average connection strength. Thus, it is misleading to assume that the connection parameters    
2:01:27    
are equivalent to the connection strengths. Since the connection strength of any given instant may vary depending    
2:01:33    
on the states of the system and since the form of the dynamics may differ considerably in different models, we    
2:01:40    
need to discuss connection strength in terms of a quantity that is representation independent    
2:01:46    
and it is but which is well defined for any dynamical model. So that's where he introduces this Jacobian and the    
2:01:53    
transition rule is an ordinary differential equation of this form. The instantaneous connection strength of the    
2:01:59    
connection from node I to node J where I is an input to J is the corresponding    
2:02:05    
term in the Jacobian matrix here. So this is the Jacobian matrix. This is the    
2:02:11    
transition rule in an OD. And then a connection is excitatory if    
2:02:17    
the Jacobian matrix is larger than zero and inhibitory if it's less than zero.    
2:02:23    
Similarly for discrete time dynamical systems or continuous maps of this form    
2:02:28    
here. A connection is excitatory if the Jacobian is or one and inhibitory if    
2:02:36    
the Jacobian is smaller. And in a continuous system, the average    
2:02:41    
connection strength is the Jacobian where brackets denote an appropriate    
2:02:47    
average. In this discrete system, it is the absolute value of the average    
2:02:52    
Jacobian. To make this more precise, it's necessary to specify the ensemble over    
2:02:58    
which the average is taken. So this just kind of talks about this.    
2:03:04    
Then it gets into machine learning problems uh in neural networks    
2:03:09    
and it makes some point. There are several reasons for dropping the constraints of    
2:03:14    
modeling real neurons. One is we do not understand the behavior of real neurons. Two, even if we    
2:03:21    
understood them, it would be computationally inefficient to implement the full behavior. Three, it is unlikely we need the full    
2:03:28    
complexity of all neurons in order to solve problems in machine learning. And then for by experimenting with different    
2:03:34    
approaches to simplified models of neurons, we can hope to extract the basic principles under which they    
2:03:40    
operate and discover which of the properties are truly essential for learning.    
2:03:48    
So this and then this gets into more about characterizing neurons. But of    
2:03:53    
course, if we want to use these other models, if we want to use say like we want to look at immune networks or    
2:04:01    
something like that, we're not modeling neurons anymore necessarily. We're modeling the the nodes as something    
2:04:07    
else. And so we need to have a mathematical model of that. So there's a    
2:04:12    
lot of really good modeling work in this paper. um thinking about networks in    
2:04:19    
different ways, thinking about the properties of connectionism in different ways and all of that.    
2:04:27    
So I think that's all for that uh feature. Now we had some comments in the chat    
2:04:33    
here. So uh let's see.    
2:04:42    
Okay. So I think that's um all for that. Um    
2:04:47    
it's very interesting to look back at the history of connectionism and then look at like where kind of diverge you    
2:04:54    
know like people thinking about it more broadly where it diverged from cognitive science applications    
2:05:02    
and all that. Okay. So I think that's all for today.    
2:05:07    
Um, any other comments or questions? People want to go.    
2:05:15    
All right. All right. Well, thank you for attending    
2:05:21    
and see you next week.
