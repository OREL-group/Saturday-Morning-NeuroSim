## Meeting Recording

[YouTube link](https://youtu.be/fxFux6PBgzg)

## Mastodon thread

[link](https://neuromatch.social/@OREL/114531256747864162)

## NOTES
Jesse: current Summer of Code -- Computing, post-event, MIT Ethics of Computing Research.

* Data x Direction --> deliberation.io

* hospitals have ML teams in basement to fix things.


Algorithmic Homogeneity: lack of diversity in models, labels.

* Mathematics of Lawmaking --> smaller firms - negation-oriented.

* ChatBot is reporting reflection. Genuine information diversity.

* tech as instantiation of ethics --> Norbert Wiener.


Internships, Open-source. Direct descendent of data science.

* SET (working group), Data x Direction (project). 

* applied mentorship -- project goes with who applies.


Morgan -- lots of events, Institute of Genomics (Santa Cruz).

* Terrence Tao, LLMs. Foresight Institute.

* acousto-electric ultrasound.

* Big Brain, Human Brain Projects (high-end data collection).

* Foresight (broadness in Neurotech, Neurotech + AI safety).

* what do computation, models deliver?

* CIMC --> machine consciousness. Computational creation, machine-biological consciousness.

* Patrick Minneault -- White Paper -- NeuroAI and AI Safety.

* enigma -- Neurogenetics. Whole-brain emulation. Address AI Safety?

* Neurotech summit -- relate topics to funding and trends (Protocol Labs).

* pop-up village, talks around a wide set of topics.


What is the "great leap"? Computational model of real biology behind abstract computational modeling.

* Neuromorphic computing.

* can we fill the gaps with more data collection (molecular biology, neuro)?

* Generative AI @ Allen Institute. Foresight-funded molecular biology.

* Brain-Inspired reading group. McCullough-Pitts.

* NeuroAI Turing test. Waddington, Szilard, Wright, McCullough-Pitts, Lotka.


What is the pattern language of the Krakauer (The Complex World) book?

* 1929 -- there was no complexity theory nor cybernetics.

* what new sciences will emerge from today? 

Jesse Parent
Jesse Parent says:
üëè 
9:16

Jesse Parent says:
üëç 
9:16

Vidhi Rohira
Vidhi Rohira says:
hi everyone 
9:42

Jesse Parent
Jesse Parent says:
re brain inspired - I think they are coving this paper very soon. will mention this in their BI Complexity server again 
https://jesparent.substack.com/p/on-behavior-purpose-and-teleology
 
9:58

Jesse Parent says:
https://www.neuroai.science/
 pat mineault 
10:00

Jesse Parent says:
https://braininspired.co/podcast/209/
 
10:19

Jesse Parent says:
just invited 
10:21

Vidhi Rohira
Vidhi Rohira says:
yeah i did 
10:51

Vidhi Rohira says:
just wanted to ask
next week id like to give some kind of weekly updates
so how should i go about that 
11:11

Jesse Parent
Jesse Parent says:
Hey Vidhi-  you're always welcome to give updates - you can even say something more today if you want 
11:12

Jesse Parent says:
There's a usual call for updates at the start and you can mention something then 
11:12

Jesse Parent says:
or if ther's a specific topic you'd like to comment on, let me or bradly know ahead of time and we'll in particular center space and time to do so for you, esp if you want a longer talk or more Q&A time etc 
11:13

Vidhi Rohira
Vidhi Rohira says:
yes okayy 
thank you so much 
11:13

Jesse Parent
Jesse Parent says:
Elan! 
11:14

Jesse Parent says:
He's with Ekkolapto yeah 
11:15

Morgan Hough (he/him)
Morgan Hough (he/him) says:
https://russpoldrack.substack.com/p/why-better-code-can-lead-to-better
 
11:18

Vidhi Rohira
Vidhi Rohira says:
thank you 

## TRANSCRIPT
0:01     
hello good morning morning     
0:13     
hey good morning morning so     
0:18     
uh welcome i know uh he said something but     
0:24     
otherwise nobody um nobody was here earlier as far as I can tell although I didn't get here     
0:30     
directly attending okay     
0:35     
well yeah uh as for updates uh for meetings we didn't have a DA room     
0:40     
meeting this week we did have a an open source meeting and of course I continued with     
0:49     
the community period for GOC i talked about some of     
0:54     
the project management aspects of projects in open source i talked     
1:00     
about the open source productivity model uh and how that differs from proprietary     
1:06     
model and some other things as well so that that video is on     
1:13     
YouTube uh other than that it's been a pretty in the lab at least it's been a pretty slow     
1:20     
week been working on some different things uh I know Jesse     
1:25     
has been it looks like his blog has been pretty active i saw a couple posts i saw     
1:30     
an update on the MIT symposium and some other     
1:36     
things so I don't know if Jesse wanted to provide an update on that we can do that     
1:44     
uh yeah I've I've been I last night well was a culmination     
1:52     
of a lot of things I suppose um but I've been I've been     
1:59     
reorganizing myself and my online writings and things like that     
2:05     
um I can get into all of that sooner or later and I don't know if we have a     
2:11     
preference because it'll probably be I can go through     
2:17     
the website and and maybe um I had wanted to finish a bit more of     
2:24     
a write up on the Aventures event um where I was got to be a judge or be part     
2:31     
of that um but I don't have anything really to     
2:37     
show although I do have maybe I do have nice pictures that I can show from that because John John Mo     
2:42     
does a pretty good job of giving people nice pictures and and     
2:49     
trying to sort of document stuff which I I appreciate i say that as someone     
2:55     
who I like when I was in in college undergraduate I was I was the person who     
3:01     
was like we got to get a picture of this nobody's going to know that it happened in the sense of     
3:09     
um I was with student organizations and at that time uh it wasn't it wasn't like     
3:15     
kind of it is now i mean I say that but at the same time there are a lot of organizations right     
3:21     
now that don't really publicize or market what they do and at the time you     
3:29     
know it was a lot more rag tag and cell phones couldn't     
3:35     
quite do everything they do now as as well as they do so so it was a lot more     
3:42     
um kind of gorilla style and and just really really trying to to just make     
3:48     
things happen um so but I I appreciate sort of     
3:55     
his fundamental concern for trying to offer highlighting things that need to     
4:01     
be highlighted and effort that people do it's sometimes just doesn't um sometimes     
4:07     
it just goes away there's not there's no nothing of it memories and and things     
4:14     
are nice and important but uh otherwise you have to actually     
4:19     
document what's going on as we know on many different fronts um so yeah um I     
4:25     
don't know i'm just getting my own self set up but if you want I can launch into     
4:31     
all that stuff um like sooner than later or or even start now it'll be kind of a     
4:37     
slow start if I do that so I'll let you know so I wanted to bring     
4:43     
up a couple social media mentions of the lab and her activities so the first one     
4:50     
is this post on blue sky says Blake Richards who posts a I guess he posted a something here     
4:58     
where you know there were a lot of responses to it and     
5:05     
so it says dear neuroscientists the brain cannot generate information about the world     
5:10     
denovo it's impossible all the brain can do is number one selectively remove info     
5:18     
that is irrelevant and too remit info previously absorbed by evolution or memory the     
5:26     
brain can brains can never create information never and then uh someone     
5:31     
responded "Do you have a preferred analysis of information uh and then this other     
5:37     
person aklave said "I hate it when I define information in terms of information     
5:43     
theory and then I'm unable to prove that information is information." So they cite the biological information article     
5:51     
the Stanford Encyclopedia Philosophy they're one of our favorite references for things like that um then     
6:01     
this other person Mario God uh says "I have no idea what information     
6:06     
is but this is my favorite paper about it." And this is the Bill Warren paper from     
6:12     
2021 information is where you find it perception is an ecologically wellposed     
6:18     
problem so we've talked about you know uh ecological psychology ecological     
6:26     
um perception and things like that and this is kind of one of the it's it's an     
6:32     
important paper in this area and talking about how as an ecological problem perception     
6:39     
is well posed and so then this uh this is aka nickel Dave which raises the     
6:47     
question of how to formalize Gibsonian information is that article does looks     
6:52     
like uh this is our uh paper on Gibson one of our papers on Gibsonian     
6:58     
information have some ideas and link to the archive article     
7:04     
here so that's nice that people are kind of reading that and synthesizing that     
7:09     
with other references yeah and then I wanted to congratulate Hersha     
7:18     
Kashkarnney so Hersa Kashkarnney was a member of the lab a long time ago     
7:24     
pre209 or actually right around like 2017 to     
7:29     
2019 and we did a lot of things uh one-on-one and so he's actually got uh     
7:36     
he's received his PhD he just did his uh defense I think a couple weeks ago and I     
7:43     
wasn't able to attend online but I was glad that he was able to do     
7:48     
that i guess that's his doctoral committee there um so yeah he's says     
7:55     
happy to share that I've successfully defended my PhD dissertation set ranked in generative IR     
8:02     
addressing low resource low latency and fragmented context challenges so I remember he was like     
8:08     
starting off his PhD like right at the beginning of the pandemic so he's you know that was a     
8:15     
really tough time to start a degree program but he's he made it through so that's great     
8:21     
congratulations for Shakesh yeah that's awesome i I for I     
8:27     
don't think I did a whole lot with him but I know him and I am very happy to     
8:34     
see him go through stuff so I'll make sure that the lab um I was just doing I     
8:40     
just found like a um some other like current summer code stuff and people um     
8:46     
talking about it on on LinkedIn and wherever and I'll be sure to hit him up there yeah good to see that     
8:54     
[Music] um alumni     
9:00     
yeah okay so that was the couple items that I saw online i want to bring it to     
9:07     
our attention as a group so I don't know Jesse are you ready to to present     
9:13     
or or just discuss things     
9:21     
or Yeah um if you I know I think it's usually a bit early for Morgan but if     
9:27     
you want to go in Morgan you can otherwise I'll I'll start in just a second     
9:43     
um the biggest the biggest thing that I'll mention is probably     
9:48     
the the computing post which is actually the month of May has just been     
9:57     
um wild and and in a good way like it's just very very condensed uh for me     
10:04     
personally because I' I've had so much Um there's been a lot going on there's a     
10:11     
lot of things that actually have been able to be pushed forward and in general     
10:17     
um some     
10:24     
[Music] uh sorry there's been there's just been     
10:29     
quite a bit of um moving parts that are finally kind of settling in motion and     
10:35     
it's just been a series of boom boom boom the first thing from this month was     
10:41     
this event um     
10:46     
uh this the main one yeah I'll just go to this one     
10:52     
so yeah on on May 1st I attended I actually tried to make this into a     
10:58     
social because for some of the projects I'm I'm trying to make these social events around     
11:05     
um kind of like local meetups like there's this we do some you know this     
11:11     
lab our lab is virtual and and all of this stuff um but I'm trying whenever     
11:18     
I'm in like a relevant city or whatever or there's events to     
11:24     
offer the chance to kind of do an inerson meet up um I made a few I don't     
11:31     
I forget if they were actually from um you know     
11:36     
um the invites I have a Luma calendar that's kind of more like toward like in     
11:42     
person events and stuff but I've made a few contacts and and and is actually an amazing event um it's     
11:52     
essentially in essence it's a year-end event um for the sort of not really     
11:59     
department but but a a big theme in the computing school um at     
12:05     
MIT um but     
12:11     
um it's super relevant to a lot of things in data and direction and I I didn't even I didn't even get to say the     
12:16     
whole time I take great notes on everything but um this was it i don't think I put this     
12:23     
one to medium yet so I will send it to medium if I haven't yeah um that's been as you might have seen I'm like I'm just     
12:29     
sorting out a lot of things in Substack and getting things kind of where where they     
12:34     
should be and then going from there and kind of leaning into that one um but     
12:39     
yeah to kind of quickly go through this one um this is a really interesting talk     
12:45     
uh [Music] um and the the kind of crux of this talk     
12:53     
was about how we always focused on you know regulations and stuff like that and     
13:00     
and and and in terms of having your clinical AI things that like diagnose things that um     
13:08     
identify what's happening inside somebody and and you know trying to use     
13:15     
AI to enhance all the stuff that you can do um it is you know regulated in in     
13:20     
capacities for um clinical AI but     
13:27     
um this this particular uh post looked at or this particular talk from the Sami     
13:36     
um looked at the challenges of what about the things that are pretty     
13:42     
much adjacent to to clinical stuff um and like in this case it was sort of     
13:48     
about uh like an example was scheduling     
13:55     
um like gender like if someone is is labeled as     
14:02     
a gender female they may be slanted towards well maybe you don't need to     
14:09     
come into the office yet because of just certain biases or trainings or whatever you know um and and and ways that     
14:16     
administratively and not clinically uh things and and and sort of things that are adjacent to the clinical somewhat     
14:25     
uh more regulated clinical things but these sort of adjacent structures that also AI is being used and demographic     
14:31     
data and personal data is being used and evaluated from um and and here in Boston area like     
14:40     
there's tons of hospitals and they all use AI and sort of this Um I don't know if it was a joke or sort     
14:47     
of a just just a reality but basically all the all the machine learning teams all the hospitals have these machine     
14:53     
learning teams in the basement to try to fix problems that happen and she was basically saying well that's you know     
15:01     
that's nice to have that but how do we how do we get towards actually     
15:07     
um trying to create better models to begin with and realize that the use cases for them     
15:14     
for the models um and the biases that they have um even if they're not in a     
15:21     
quote clinical setting still have a gigantic amount of influence and are imperfect and and we need to you know     
15:28     
keep keep the pressure on making that better um so it was an interesting talk     
15:35     
especially with um I I have done some work in a similar     
15:42     
space about you know getting sort of this justosition between getting data     
15:48     
from a patient and then uh making a care plan and then the implementation of the     
15:55     
care plan uh and those are all critical areas     
16:00     
uh but but the if you will the bias on the challenge space for each of them is     
16:08     
is different and and potentially immense um so yeah this is this was just a deep     
16:16     
dive into that um I don't really recall some of her solutions um or like what     
16:22     
what we do about this other than the obvious make better products and and and you know try     
16:31     
to diffuse the biases and all that stuff uh but what I tried to do in this post     
16:36     
and I I won't go through everything is uh the a little bit of you know pictures     
16:41     
if they're there a little bit of a the session proper from from the main website and I'll try to find some papers     
16:49     
or um at least relevant links for the speakers and stuff like that in part     
16:55     
because we might come back here and a lot of a lot of the interns um that I've been interviewing for anyway if if     
17:02     
they're if they're if they're undergrads like this is a nice real world primer to say hey you know what what in this space     
17:11     
kind of looks appealing to you um and kind of give them some examples of oh I     
17:18     
like this but I don't know a lot well here's here's very clear examples of a     
17:23     
lot of the things around sort of applied data science data ethics responsible AI     
17:28     
so on u really briefly I'll mention this one's quite interesting     
17:34     
Um this is from Josh Tennibomb's lab who Joshua Tennibomb who we um I've     
17:42     
periodically mentioned and I I I get excited because I find her to be one of the     
17:47     
better folks that's sort of fighting this good fight between cognitive science matters in the world um and and     
17:55     
working to sort of justify relative to a lot of pressures from AI that studying the human mind is like So     
18:03     
you know but he's in the thick of it and he's doing good things and uh Sydney     
18:09     
Lavine is a very interesting presenter um I don't know if you can see this super well but she's basically coming up     
18:15     
with this um a triple theory of moral cognition     
18:20     
the anttology consequentialism contractualism um and I won't get into     
18:26     
that right now but it was a very interesting talk um and     
18:35     
um you know there's there's quite a bit in the alignment space and and that's a that's a worthy topic for sure um supply     
18:43     
chain is interesting i'm trying to remember what to say about this one um a     
18:48     
very I think uh a lot of these presentations were um they they sort of     
18:54     
listed the like the PI's lab and and the doctoral student or whatever going to speak about it and salesman said done a     
19:00     
lot in this space had a very interesting uh comment in uh Q&A which I did not uh     
19:09     
put here I can remember it was a very interesting talk to kind of see [Music]     
19:14     
um yeah so I feel like what are you what are you kind of doing you're looking for like regulation professional industry     
19:21     
standards organizational self-regulation government procure procurement and then contracts     
19:27     
and tors and it's sort of like well um one of the one of the things one of     
19:34     
the kind of things that was uh in the air and very on the nose for some of     
19:41     
these things like I think one of the last on the list people were saying sort of you know well what do we do in a     
19:48     
political moment like this where trying to engage the government is maybe challenging to do some of the     
19:54     
regulation work so that's a whole other can of worms that I'm just going to sidestep for now um and and talk about a     
20:02     
few more homogeneity um a critical topic but     
20:11     
um was this the one that was uh yeah this is actually a really yeah there's     
20:16     
there are a number of links here I don't think um I should probably write up more here in this space but basically it was     
20:23     
sort of um you know looking at unintended collusion between different different people using     
20:30     
the same al algorithm or the same software um impacts on creativity     
20:38     
uh the nature of competition and like how generative AI facilitates that or not and systemic risk in financial     
20:45     
systems there's a lot there's a lot going on here um and you know uh same     
20:51     
algorithm same data similar correlation but unintended coordination so and there     
20:56     
was a little bit about [Music] um different different groups basically     
21:02     
using the uh un the the the pitfalls and the     
21:09     
opportunities for good things and bad things when the same algorithm is used everywhere so to say um this is another     
21:18     
good talk and um very regional     
21:25     
focus and and should there be different standards of care and just sort of the reality that if you train a model based     
21:31     
on all this data that's in let's say America that's probably where a lot of bigger models are focused how does it do     
21:38     
when you go to the global south go to a totally different region and just looking at you know it's not necessarily     
21:47     
Perfect um this study was     
21:52     
um looking at uh basically performance across different countries I suppose um     
21:59     
Whoops yeah um and it was it it sort of used this     
22:04     
methodology referring to these these standards from UNESCO from Santiago Chile and     
22:12     
um this inter interamerican framework uh in a positive way i I mentioned this to     
22:20     
uh um Valeria Schnaki who is a society     
22:27     
ethics tech kind of alumni who's now uh graduated her degree from law and is     
22:34     
kind of getting into the law stuff there and uh she hearing about it we might talk more about it ahead this this     
22:40     
presentation that has to do with uh I think this person is founded um is from     
22:46     
Chile and centered um some of the discussion on on that part of uh Latin     
22:54     
America um and so it kind of resulted recommendations     
23:01     
of you know cooperation synergy leveraging AI and all these     
23:06     
things and nice quote of the end of uh future here but is it is it distributed     
23:12     
evenly or not and it is not going to open debate on that right     
23:20     
now um so yeah anyway uh the mathematics of love making was also very good um it     
23:27     
looked at uh yeah this is this is the key takeaway this should probably be     
23:32     
highlighted the smaller smaller firms this is about um lobby view and     
23:37     
lobbyists and the la the last two are about um law specifically so it was very     
23:44     
interesting to kind of see law legal things governments um this was a this was a     
23:51     
I think lobby view is a database i have it     
23:57     
um firm level lobby and congressional bills database from insankim and and this was this is a a     
24:04     
2018 paper but it's been in development for a long time and it's been it's just been interesting to kind of see to kind     
24:09     
of zoom out like the level of care and and dur duration that that it takes to     
24:15     
really do stuff it it you know you can see this this is a 2018 paper right and     
24:23     
there's been a lot of development on it since then you know this is a nice     
24:29     
um this is like an intro intro paper     
24:34     
um at that time but you can see how like there's just you     
24:40     
know like GPT at the level that it is now didn't didn't exist at that time you     
24:49     
know and it's just I think it's I think it's so I'm always looking for ways to convey the rapidity of what's happening     
24:56     
and how much it's like moving at warp speed or life speed and then you got     
25:02     
you're kind of riding you got you got somebody in a bicycle and you're saying "Okay how do     
25:09     
we align these two people in their travels?" And it's like well you know it's a rough show um     
25:18     
so the not I'm I'm just saying that only out of the context of it is so critical     
25:26     
and so hard to get to get moving so even though the person's on a bicycle like the distance they're covering and where     
25:32     
they're going really matters even if it's not as fast as tech is just completely     
25:39     
um let's say the word disrupting um many many many many     
25:44     
many press um but these these efforts are important     
25:50     
and we know you like especially for things that are probably very relevant now in terms of seeing who makes what     
25:57     
kind of bills and What a big takeaway from all this was um smaller firms tend     
26:04     
to be negation negativist or like negation     
26:10     
oriented like trying to challenge stuff whereas large firms um propose their own     
26:15     
stuff or have an agenda or you know are acting on behalf of a major think tank     
26:21     
that has a very large agenda that     
26:27     
um So and this was sort of an attempt to put put the surveillance powers of AI     
26:34     
back on the people with power which I think is basically the name of the game with many friends and it's like you know     
26:39     
this is attempting to regulate and document or or make bring visibility to     
26:45     
um lobbyists in this case and finally I'll briefly go over um this     
26:52     
is a very interesting topic from Sai who's I think director of the gov lab     
26:57     
and um Alex I don't know if Alex was there or not in person Alex is the Stanford     
27:06     
contingent we talk about um this group plenty but but deliberation IO which I     
27:18     
Yeah reimagining democratic engagement facilitating large scale open dialogues     
27:24     
between citizens to strengthen collective decision- making so what what is this um basically     
27:31     
it's it's an AI it's a     
27:36     
nonpartisan AI chatbot that simply directs goes through certain     
27:44     
[Music] um routines I guess you could say to engage or facilitate     
27:50     
discussion and um it has these sort of different I guess I don't know modules     
27:58     
or operations Um we have Socratic     
28:05     
dialogue Socratic dialogue facing the crowd dynamic deliberation comment ordering and preference     
28:11     
aggregation and um this     
28:17     
sorry jumping a slide here okay never mind     
28:22     
um so it would kind of be different modules and and and     
28:28     
um I'm kind of burying the headline but but yeah what we're learning so far is     
28:33     
you know can these platforms encourage more social or the social ethical political benefits of     
28:39     
separation basically um with the Socratic dialogue when when there's sort of a     
28:48     
general questioning of oh how do you know this or why do you think this way like a g a general light     
28:55     
uh regardless of who the person is or what they they     
29:02     
um where in the spectrum of of politics they are the general act of offering     
29:08     
kind of an even-handed simple questioning and     
29:14     
regulation of what's going on moves people towards the middle so it was essentially a deescalator thing and for     
29:20     
those who are not familiar with American politics um the story in the last decade     
29:27     
plus has been um the extreumification in this sense and how much there's     
29:35     
um amplification to um let's say divisiveness in in     
29:42     
particular particularly I would say in discourse um or like online discourse or or ing up stuff i will always remember     
29:49     
uh some uh non non North American folks     
29:55     
saying "Oh what do what's your take on Americans?" And well um they're really     
30:00     
nice but don't talk about politics because because they're just going to they're going to argue with you like and you can     
30:07     
kind of see the conditioning from from the outside perspective of like they can't they     
30:12     
can't have a conversation because Americans are so trained to um work as an influence to     
30:21     
um to to be very bitter and divisive and to take a point and just contend it and     
30:26     
write it and ride the contention so this is this is an interesting AI application     
30:32     
for um maybe neutralizing some of those amplified     
30:38     
antagonisms [Music] um face the crowd showing people the     
30:43     
face of the crowd demonstrates how much we agree with each other as in I think I think facing the crowd and maybe getting     
30:49     
this wrong sort of not just trying to combat the the the     
30:55     
loud loudest voices in the room and saying "Well here's what everybody actually is thinking about this kind of a deal." Um as opposed to just centering     
31:05     
uh the the potential for minorities um a moderated conversation increases nuance     
31:13     
discussion understanding and legitimacy of platform it also uncovers clear areas     
31:18     
of agreements deliberation one     
31:24     
um and then common ordering is about sort of their sense of representation legitimacy     
31:30     
uh and they're doing some field studies um and and very enthused about the results at     
31:38     
least um they are so they're getting to do this in DC and have some partners     
31:45     
and you know next steps involve of course using more LMS to do certain things more testing and and then getting     
31:52     
other field locations like they're kind of starting in DC which is you know a     
31:58     
very specific location and then getting it in these different places so you kind     
32:03     
of have um different different different arenas to test out those how well those     
32:09     
models perform there which is probably pretty useful so     
32:15     
um I guess the chat plot is supporting reflection is one thing but people realizing that it's just it's having an     
32:21     
impact there um so a lot of cool things there a lot of cool things to follow up on um     
32:28     
some some final takeaways and questions are basically what what doesn't randomization fix um but but but it     
32:36     
doesn't always uh it doesn't always work out um like um what this is sort of what     
32:44     
I said at the start what clinical adjacent spaces exist what we're using geni and a different stuff that's     
32:50     
shaping outcome but in a way that's not regulated informs that clinical areas are where     
32:57     
does general information diversity come from and who actually support it or not     
33:03     
And you know can or should AI be a supported reflection kind of the last     
33:09     
one uh be support be become a norm in online spaces rather than expect rather     
33:16     
than the exception uh when what's the line between stabilizing and depolarizing versus     
33:22     
steering people or manipulating it these are all kind of like huge huge questions that aren't going to     
33:29     
have answers but that you can kind of be explored like how is this technology essentially     
33:37     
um in the Norbert we sense um how's the technology this sort of instantiation of an application of ethics um and where     
33:44     
where is it where is it stealing exterior and what can it do and and I think I think sort of you know the the     
33:52     
the unsaid for all these these posts is sort of like and now that we're in 2025     
33:58     
because this is all this is all research people are doing now is all happening now but 2025 is a very different     
34:05     
landscape for other operating so um what is you know what does that entail     
34:12     
um any questions about that before I move on so     
34:20     
u yeah that looks really interesting Um glad you were able to go and give a     
34:27     
report on that yeah um it was a nice time and it was     
34:32     
it's it's just these are these are sort of rapid loose notes there were there are many other um very good talks there     
34:38     
and I didn't get to stay for all them uh but it's a nice sort of overview here um     
34:44     
anything else to mention um that's pretty much it for this week i     
34:50     
know I took a good chunk of time to get through all that stuff there's a few other happenings this week we are doing     
34:56     
the internships we are doing um we had a really nice open source     
35:03     
meeting i don't know if we're going to talk more about that uh later or anything but um we're kind of moving through the community period and excited     
35:10     
to get started more in a few weeks all right yeah yeah so how do you envision     
35:16     
uh a lot of what you have been doing with data plus     
35:21     
direction i mean you know you've got like the society tech and ethics and     
35:27     
you've got data plus d or data times direction and you know I know it's a kind of a     
35:33     
separate project but how do you think those two things interface oh yeah well I mean my structure for a     
35:40     
lot of this stuff is um if I basically I I've shifted my language to these things     
35:49     
um kind of in the last year in that working groups are sort of just their     
35:54     
working groups so conditioning futures society ethics tech     
36:00     
um there's sort of a fledgling working group called um mental health paradigms and perspectives which     
36:08     
uh is me and also uh Jen the master student who been here a few times in the     
36:15     
past and I might be getting a few um advisors there also sort of clinical     
36:21     
um clinical and therapeutic spaces Um and I feel like I'm missing like     
36:28     
representational brains and phenotypes I I would say as a working group it's not a joke or one that I made obviously but     
36:33     
like I feel like like that's representational brains and phenotypes sort of represents     
36:39     
um a nice collection of like it's more of a collection projects uh data and     
36:47     
direction for example is very clearly a project to me in that it's it's not um     
36:53     
it's much more applied so it's sort of Joe offers working groups and then     
37:00     
working groups can combine and and interact on projects so working groups are most about     
37:06     
people so like society ethics technology is a group of people who uh have done     
37:15     
things and a project in this space is data and direction     
37:20     
um so I don't know you mean like administratively more so or like where do I want to go next with data and     
37:26     
direction overall well I guess you know well a where do you want to go next but b they're obvious connections here and I     
37:34     
know that like you know we uh sometimes we'll get into one project     
37:40     
and then put another one aside or maybe working on a paper or some initiative     
37:46     
and stuff you've done before sometimes just kind of you don't think about it     
37:52     
and I know we do this in papers a lot where we write a paper or we have a     
37:57     
discussion it just get kind of gets buried so I want to make sure that like we have this good connection     
38:03     
between the things we've been discussing and uh society ethics tech and then some of     
38:09     
the newer stuff and just see how those things kind of what the continuity is what the opportunities are     
38:18     
yeah I mean like I see data in direction as basically I I it's it's a     
38:26     
direct say disciple but the direct descendant from my master's program and     
38:32     
from um the capstone work I've done there and then the ethics courses I've done there     
38:39     
and essentially the the goal of the project is to be really targeted at um I guess you     
38:47     
could say people in AI space specifically uh but but the data scientist data ethics responsible AI and     
38:54     
offering opportunities like I wanted to offer opportunity um not just sort of     
38:59     
like you know like in the past I guess I guess Brian Mccorpal and some of the folks essentially were interns in the in     
39:08     
the working group of society ethics technology and I mean you could say now     
39:14     
that people joining um would also     
39:20     
be I I I would I would I would phrase it like that in the sense of um let me     
39:26     
share my let me share my screen again uh yeah I'll do it like     
39:33     
this so um like in this sense of how things are     
39:41     
here um     
39:47     
D like this this was an introduction of a specific project within a working group but I kind of have it here like     
39:54     
what are the groups and what are the projects um and so here's here's attempting a project um within the group     
40:01     
so projects are a little bit more people trying to do uh certain sets     
40:08     
of things um and then the working groups are just the collection of it     
40:14     
um like I don't have a a good example to show right now here are some working     
40:20     
groups like Cognition Futures which I think needs to be updated uh but like you know this is this is     
40:26     
this is sort of like the first ever working group that I made and it's a little bit on the back burner right now     
40:32     
it's but it's not lost or anything like that it's just I'm really leaning into building out the from here to there one     
40:39     
and data and direction projects right now um and I don't know if I made Yeah here     
40:45     
we go this is a project it's it's under um society ethics     
40:53     
tech and we're you know doing all this stuff here so hosting some internships     
40:59     
there's not there's not I'm not necessarily trying uh     
41:05     
to my my current round of inner spokes are basically um attempting     
41:13     
to get it it's not so much project based as in just individual based it's almost     
41:20     
going to be more like um I don't know kind of an applied     
41:25     
mentorship where uh in part because I haven't I haven't been sure who was going to apply so I kept it kind of very     
41:33     
general at first but it it's been very interesting to see sort of where people     
41:39     
are at and what they want to do and finding a lot of interesting topical spaces and also like a little variation     
41:44     
in experience levels so um you know some people really want to do research some     
41:50     
people don't want to do research and there's like making a paper um and we'll     
41:55     
see uh we'll see where where where that goes in terms of the co what the actual that     
42:01     
cohort is ahead for this term uh this next um first round of it I suppose so     
42:09     
yeah um that's the broad overview of those things I guess um but you know nothing's     
42:17     
society ethics and tech is still there it's just um like an example of a previous project     
42:25     
for society ethics tech would be a lot of the stuff for Nickwick like the innovation panel and then a lot of the other like poperri stuff uh and I I     
42:33     
would periodically look for these things these opportunities to okay we're going to apply to this and do this and do this     
42:38     
thing happening um and that will eventually come around for data and direction two specifically um as a     
42:45     
little more centered group of Um probably the like basically the     
42:52     
people who are data scientists [Music] um reaching back out like a lot of a lot     
42:58     
of my frame framing and phrasing and background for society tech is this     
43:05     
uh a place where technical and nontechnical folks can come together and get the experience of the crossf     
43:11     
functional and and and the the being being in that space and so this data     
43:18     
direction it'll be a little more focused on     
43:24     
what from a deeper point of technical fluency about the models the data and     
43:29     
stuff where where does it connect back to society that way so that's kind of the     
43:36     
vision for those things yeah all right all right that sounds good     
43:44     
um anyone else have any questions for Jesse     
43:52     
here anyways yeah uh it's great so thank you for that update showing us the u     
43:59     
what what's been going on in that space i think it's really interesting work     
44:05     
all right um I don't know morgan did you have any updates or are you in a position you can     
44:12     
give an     
44:24     
update um sorry um let's see updates     
44:32     
um no just um things still progressing lots lots of events have been happening     
44:40     
here um but um can't say much much to     
44:46     
update in terms of progress on on projects um     
44:52     
uh there was an event at the Institute of Genomics um down in in UC Santa Cruz     
44:59     
um so I'm hoping to hear about that uh just in terms of reaching out to the     
45:04     
brain engineers um yeah usual usual updates in um you     
45:13     
know uh various various Slack channels and um it's uh it's always interesting to     
45:20     
see Terrence Tao using LLM's um to to do to do work     
45:29     
yeah um and um let's see anything coming up that's     
45:44     
um yeah just just hoping hoping to do some some events both in neurotch     
45:52     
uh ultrasound and um um there will be a online foresight uh     
46:03     
meeting or I I'll I'll drop a few few links in the chat just in terms of uh is     
46:10     
the online meeting like neuroscience related it it's it's in their neuroch group um     
46:19     
but it's you know their their neuroch     
46:25     
has a kind of you know     
46:30     
weird and wonderful um broadness to     
46:36     
it um so the the you know I think their next nerk     
46:44     
speaker is actually someone that already like spoke last month at Brain Inspired     
46:52     
um and relates to game theory i I'll     
46:59     
I'll find that um I I was actually just on my way out the door     
47:05     
um but um uh I I'll I'll find the link but you     
47:11     
know it's this um it's this kind     
47:18     
of strange connection that they make between neuroch and AI safety     
47:24     
um that that is a big you know I I know part of it is because it's a big you     
47:31     
know funding focus for them um and     
47:40     
um as I did drop a link to the     
47:45     
California Institute of Machine Consciousness proposals there is a     
47:50     
similar um new foresight around you know so     
47:56     
so CIMC um is     
48:02     
is somewhat similarly focused in terms of their     
48:08     
neuroch you know like like they don't call it neuroch but their proposals are like related to kind of um again     
48:17     
computationally creating consciousness as well as this kind     
48:22     
of to me to somewhat weird um     
48:28     
uh um you know antagonism     
48:34     
between machine consciousness and biological consciousness or you know     
48:39     
again it's it's always framed in this kind of AI safety like like are you     
48:44     
creating consciousness or are you doing something that will help us     
48:51     
to limit machine consciousness in terms of its     
48:58     
you know dangers something like that um which which yeah the the the     
49:07     
connection to biological intelligence is is not clear to me in terms of how it     
49:13     
mitigates AI safety concerns um but     
49:21     
um yeah but I but I know that's a focus and and that that'll be one of the     
49:26     
meetings there's a couple meetings that I'll I'll just highlight in that chat in terms of Patrick Mol is like up to now     
49:35     
like safety thing right no no no absolutely absolutely right so so Neuro     
49:42     
AI for AI safety is his big white paper um and Amarath     
49:49     
Foundation excuse me um which is funding the     
49:54     
um I think they call it Enigma right which is is somewhat confusing to me because I'm I'm in the     
50:02     
Enigma consortium uh which relates to neuro imaging and genetics um but Enigma for them I think     
50:10     
is supposed to be kind of like a you know a grand grand challenge project um     
50:17     
like World War II you know and     
50:23     
um and you know a very much whole brain emulation but but again the the the     
50:30     
strange part or the part that I don't get about this is how whole brain     
50:35     
emulation you know kind of addresses AI safety you     
50:41     
know um there's there's there's a a little kind of magical thinking at the     
50:48     
end which is something like by creating a     
50:55     
um something more embodied but still     
51:00     
artificial um um we we     
51:08     
we solve alignment or something like that but that there will be a neuroch     
51:14     
summit at uh at one of these     
51:19     
um again I don't know how you refer to these Jess um you know these popup     
51:25     
cities um oh like the Luma event things or No no     
51:31     
no these these I mean I I you know I kind of want to call them these kind of     
51:37     
like crypto related or Yeah you know     
51:43     
events yes like I mean you know is that a fair characterization     
51:49     
well I think that's I think I think that's a very valid like real talk like     
51:54     
real talk yeah like it's people that are kind of like Yeah     
52:00     
well not even people but like it's sort of like and and this is this is this is a ch like at at a at a at a to be     
52:07     
constructive about it like this is a challenge that when you're trying to push when you're trying to get an agenda     
52:14     
off the brand you're trying to figure out how the hell do I relate this to funding and trends that people care     
52:20     
about and so I have no idea i I'm totally agnostic and very ignorant about     
52:27     
where some of NeuroAI for safety is is touching upon and going like I've seen     
52:32     
I've seen some things that are very compelling in terms of you know [Music]     
52:38     
um uh trying to address specific problems but then there's also what Morgan just said and in terms of like     
52:47     
the events like yeah there's sort of this like hey we're we're having a a meeting space around     
52:53     
um this thing or I want to be in this location because there's stuff going on here let's have a tiein and it's it's     
53:00     
it's it's it's both useful and it's both difficult at times and also speaks to the challenges of     
53:07     
um you know getting funding and getting getting foot in the door and and getting     
53:13     
the talent and the brain power like like the kind of far side of that would be well all the top talent from the Ivy     
53:20     
League schools is going to go into consulting and and sort of the bottom up grassroots version of that like well     
53:26     
like let's do crypto and some AI safety stuff because they have sort of more cult followings right now and how do we     
53:32     
how do we associate it there so maybe I'm speaking a little too far in that direction but as     
53:38     
it's it's Yeah I mean you know I don't know this is this is exactly like     
53:46     
um I don't know how to describe edge as Moralda but it's a popup village in     
53:53     
Northern California um very very small town um and there'll be talks around you     
54:02     
know a wide set of topics um for a month um two two and a half     
54:10     
days of which will be a neurotch and and yeah so I'm I'm looking forward     
54:17     
to it patrick Weno and Sean Escola who I believe was one of the     
54:24     
Meratch co-founders um anyway but you know two     
54:31     
two people who definitely know neuroscience um are putting something together um the     
54:40     
only but I I I do imagine that it's it's somewhat around you know and they they     
54:46     
fund these you know very um     
54:53     
noble very um um frontier projects     
55:01     
uh um that that you know that said that seem to me to be very similar similar to     
55:09     
things like the human brain project right i mean in the sense that like or certainly big brain brushing you know so     
55:15     
I I I keep telling them like yes no I'm familiar with this kind of work because     
55:20     
I've already seen the European human brain brushing so I I've seen work at     
55:26     
this level doing you know this kind of you know very high-end histologology     
55:34     
microscopy and um and you know and I I I think we     
55:41     
should learn a lesson that there's only so much that you actually get from that right like like there's been a lot of     
55:48     
soulsearching after the first billion euros of of the human brain     
55:54     
um of of what what you know that these kinds of computational models actually     
56:01     
deliver you know and and I don't you know as much as I see this kind     
56:08     
of new connection with with these computational models to AI     
56:16     
alignment I I what I don't see is any new     
56:22     
um don't see any new technology or insight that says why we should expect a     
56:31     
different result this time you know yes that everything about it is is     
56:38     
incremental in terms of what you know what what microscopy has     
56:44     
brought to bear what geometric modeling has brought to bear and certainly and     
56:50     
and you know that this is what I bring up at every meeting that I go to in this is that that the kinds of computational     
56:58     
modeling that includes real biology is still     
57:03     
way behind the the kinds of um abstract neural modeling that gives us things     
57:10     
like neuromorphic computing you know and you know like like Yeah so it     
57:17     
it's it there still seems to be you know some some missing elements um and to     
57:24     
some degree you know like the little that Conrad has shared     
57:32     
with me Conrad Cording has shared with me at at previous foresight events it was just like okay you know with C     
57:40     
elegance we're going to try to fill some of those holes you know with     
57:48     
with more high-end data collection and and you know more molecular biology     
57:57     
but you know it's it's a yeah big big     
58:04     
big difference of course between that and you know the kinds of mouse brain     
58:09     
projects that most of these efforts seem to be um um centered around or at least     
58:17     
you know the the the foresight projects that I see     
58:22     
um anyway I'm very happy to be you know     
58:28     
going and talking with the people involved in this project so you know I mean it's still exactly where I want to     
58:34     
be in terms of of learning what they're doing seeing what you know what     
58:40     
resources are around and um and I had the great privilege to to talk with the     
58:48     
person who's leading up generative AI at the Allen Institute at this last meeting which was great and um really looking     
58:56     
forward to going there in June 16th 17th     
59:01     
um or 16th through the 18th um at Allen Institute meeting for using open data um     
59:10     
in undergraduate uh education so     
59:15     
um yeah I can I I'll drop some links     
59:20     
yeah I'm sharing as as as Moralda now I actually applied to some I applied to     
59:25     
some of the tracks there like I'm I'm I I'm I'm don't know how to say this i'm     
59:33     
definitely drawn to what some of what Brennan McCord and the Cosmos Institute     
59:38     
is trying to do the sort of AI to law pipeline get this was a lot of um data     
59:44     
data and direction type stuff but there's like consciousness week um     
59:51     
there's uh I think I think you were talking     
59:56     
about the probably reality reinvented and some of the the neuro Oh     
1:00:02     
of course is in there too i've been trying to follow some of this stuff [Music]     
1:00:09     
um environmental there we go neuroch stomach that's what you were talking about uh protocol labs     
1:00:17     
and so yeah um but there's like you know I if I I think I kind of got a little     
1:00:24     
bit late to this event i feel a little bit like a conference sort of     
1:00:31     
futuristic Burning Manish but but very very intellectual um and very     
1:00:37     
people trying to build out these new new technologies and and and things like that so um I'm I'm envious if you get to     
1:00:44     
go because it seems like a really fun time instrument I guess if I spend some more time     
1:00:51     
yeah no abs absolutely you know like again like I'm I'm certainly looking forward to it um you know in terms of     
1:00:59     
those those three days it's it's only an hour and a half     
1:01:04     
away for me so um I I can I can sort of justify     
1:01:10     
that and um yeah and for foresight has     
1:01:15     
been doing events at um at the Frontier Tower the the place where Josh Bach um I dropped     
1:01:25     
that um photo of of Joshua um doing his salon there um so we're     
1:01:32     
doing neuroch salons um there uh as well and um really hoping to expand that to     
1:01:41     
you know cover kind of the the kind of molecular biology and nanotechnology     
1:01:46     
that foresight funds as well um you know and which is certainly part of the     
1:01:54     
the Ed Bdon Conrad Cording Institute if     
1:02:00     
if it can be called that um I I I still haven't seen more details     
1:02:07     
but I assume it's just trying to to implement what     
1:02:12     
Conrad's um vision for what's missing with with C     
1:02:17     
elegance data yeah you mentioned the project from     
1:02:23     
voided in courting before too that I still but if you if you if you see     
1:02:29     
anything about that or like think about it feel free to nudge me and whatever so     
1:02:35     
yeah he he he there was a slide about it on the um at this AI     
1:02:43     
bio meeting that was in Berkeley um but but again no no no real     
1:02:50     
details except that foresight has has you know certainly given the initial     
1:03:00     
funding and I I'll drop a few more links but I was just just heading out yeah     
1:03:09     
okay that's that's interesting stuff that was a good discussion     
1:03:15     
um yeah I know you mentioned at yesterday's meeting     
1:03:20     
about Conrad and uh Ed Bden and they     
1:03:25     
were forming an institute i didn't know it sounded intriguing but I didn't know what you meant by     
1:03:31     
that so I don't know if you know any more about if that's just institute by     
1:03:38     
name or if they're actually planning on doing something no I mean that's that's how that's how they're     
1:03:45     
I mean the     
1:03:50     
research referring to it and you know and I would just also     
1:03:58     
point out that these are both they're both researchers who     
1:04:05     
um receive funding from these kinds of     
1:04:10     
organizations you know I don't just mean foresight but that are definitely actively seeking like protocol labs     
1:04:22     
funding that are definitely you know involved in crypto and and funds     
1:04:30     
lots of um this kind of AI safety project     
1:04:42     
it's just the the leap that I I don't get is the how they how this helps with safety     
1:04:51     
oh yeah yeah i guess it's the old uh I need funding aspect     
1:04:58     
so where you need funding you make an argument or something and it's not necessarily a good fit but that's how uh     
1:05:07     
how people Sorry did I get disconnected yeah yeah oh sorry     
1:05:14     
i think I left the Wi-Fi sorry i was just saying that they both get they both     
1:05:20     
get funding from foresight and they both are hoarding this this more     
1:05:28     
um again like AI safety related funding from protocol labs yeah uh that uh that     
1:05:36     
provides a lot of foresight uh axle money yeah     
1:05:44     
okay no no details yet but I'm I'm impressed i'm digging yeah yeah i mean     
1:05:49     
it looked like from their white paper and we reviewed the white paper a long time ago with Diva Worm uh I know that     
1:05:57     
was an early iteration what they're trying to do but like it looked like that was something that would have to be     
1:06:02     
like a uh and like an organizational level effort in other words you'd have     
1:06:07     
to have an institute to do it because you'd have all these collaborators and moving parts and you'd need to have     
1:06:14     
something an organization to make that happen I guess yeah yeah and and Yeah     
1:06:23     
but and a lot of equipment oh yeah yeah uh you know but these these all strike     
1:06:30     
me as as like this just seems like a European brain project but about perhaps     
1:06:36     
a a more appropriate model system i mean     
1:06:42     
am I going crazy here yeah yeah so I mean I I love it because     
1:06:50     
of course you know this is this is the model system what maybe somebody should     
1:06:57     
have started     
1:07:02     
so uh Jesse said with respect to brain inspired I     
1:07:08     
think they're uh covering his paper very soon they have their I think they're brain inspired uh they they review     
1:07:15     
papers and in a in a paper I think you'd mentioned this in the slack you had     
1:07:22     
taken pictures of the the complexity group just did the mullen pits paper     
1:07:27     
yeah yeah and and that that was that was super interesting just because it is such a I     
1:07:34     
mean it's a paper that has so much um     
1:07:42     
so the the woman leading the discussion there was a piece in in mathematics was     
1:07:49     
talking about just how difficult this paper is to read because it requires this this uh level of well that you you     
1:08:00     
need to know Whitehead and Russell's Principia Mathematica's     
1:08:06     
terminology notation and then you have to know Rud     
1:08:14     
Rap like logic work as well for its for its notation to     
1:08:21     
just just to be able to translate it into kind of a modern     
1:08:26     
uh representation     
1:08:32     
but it's it continues to be a wonderful group and then the last um the last     
1:08:38     
speaker uh Jessa if you can find the brain     
1:08:45     
inspired talk that includes um game theory it's been in the last month maybe     
1:08:50     
even like the last couple weeks is it is it is it like from brain inspired pop     
1:08:56     
proper proper yeah brains by proper it's like     
1:09:02     
um first name is like orin     
1:09:10     
Yeah aron and Na yeah yeah what's the What's the title sorry     
1:09:18     
the neuroi touring tests yes there you go there you go so so you can you can     
1:09:26     
you know it's funny because it's like if you look over kind of the latest brainspired talks this is the one that     
1:09:33     
would be you know if you had to guess which one would be the most mo of most interest and foresight     
1:09:40     
this is this is their next speaker next oh okay     
1:09:45     
but this will be on the online Zoom um group okay     
1:09:57     
yeah this is bright inspired proper and they have a complexity group going through all the papers from the crack     
1:10:03     
complexity book that's what I was showing here um     
1:10:09     
so this group like what are the papers that they've done so far do they have like a list of     
1:10:15     
like     
1:10:20     
um They're all uh it's in if you go to the     
1:10:29     
server I can I'm in the server it's not I don't think it's like very gay character     
1:10:36     
it's 1922 Lka 19ard     
1:10:42     
1932 right 1942 Wington and then 1943 so     
1:10:47     
they're really starting like century ago you and um build it out from there     
1:10:55     
well it sounds like uh you've got uh     
1:11:01     
the uh what is it i guess thermodynamics evolutionary landscapes     
1:11:09     
uh neural networks uh epigenetic landscapes and     
1:11:14     
then ecology or like I guess early dynamical system stuff it's an     
1:11:20     
interesting set of like topics that they're trying to to get at have they been drawing a lot of connections like     
1:11:27     
between the papers or I what's I wonder what their goal is picking the papers that they picked     
1:11:38     
no no no i I mean I I attended a couple and I've attended a     
1:11:44     
couple of the the um     
1:11:49     
gonna set up for the meeting and you know this certainly I wouldn't say that there's a big attempt on trying to put     
1:11:56     
them in a proper order or something like that yeah um but but I I've it's definitely been     
1:12:05     
impressive to see how people have have certainly some people are bringing     
1:12:10     
historical context and some people are just bringing you know a great knowledge of the the particular field or topic     
1:12:18     
that to to to make a a rough a rough comment     
1:12:24     
that is lacking nuance and polish i think the majority of the     
1:12:30     
discussions that I've seen and I haven't been deeply involved in all of them have basically been people just struggling to     
1:12:36     
get through what did these people mean at the time okay and it it more or less with varying     
1:12:45     
degrees of how fruitful that is and I've seen a few comments saying you know it's nice to try to go through this paper but     
1:12:51     
it's been even better to see the community trying to talk about it it's sort of like if you're in a grad course and the teacher's kind of like so so and     
1:12:57     
the material is hard to read you got great great classmates they're like "Hey did you know about this article there's     
1:13:04     
this oh there's this great video that explains this." It's kind of like that vibe and I think that's that's fair i     
1:13:10     
don't I think that's literally like not I don't this not a denigration of     
1:13:15     
anything that's just that's what it's at and I think when you're looking at the origins or the fringes of of things like     
1:13:22     
that's that's kind of you know we get we get a little bit you know um prisoner of     
1:13:28     
the moment where you know and everything right everything right now is in this     
1:13:33     
agental AI LM geni speak that people may not     
1:13:39     
give as much of a care about in the same degree in 10 years and though you do that you know a hundred years ago It     
1:13:47     
just amplifies the sort of vernacular the distance to the vernacular of time right so I mean I think I think it's     
1:13:53     
quite humbling i I always I personally love environments and maybe this is just part of my weirdness where very very     
1:14:00     
very I use the word smart or intelligent people that really care about trying to understand things is a nice way to say     
1:14:07     
it too are struggling to decipher things in this context because     
1:14:12     
I think I think it I think it it I think I always I think I think the most like useful lens for all this stuff for me     
1:14:19     
always is like we aren't that far maybe 100 years ago But in a lot of ways it's everybody just     
1:14:26     
sitting around in like caveman times with with some sticks and a few rocks     
1:14:31     
and trying to scrape them on the wall and make this diagram to say "Well here's kind of what I think I need." And     
1:14:37     
we have all this rigor and development and stuff but like at the end of the day that's that's kind of what it is and     
1:14:44     
even even in this case like we don't you know I I I'm sharing this link that I     
1:14:49     
just sent for this big deep dive on the teiology paper which I think is pretty cool and honestly honestly I think I     
1:14:56     
think one of the cool things is like compared to these other papers uh the teiology paper is like crisp it has very     
1:15:03     
clear points they may not be the best points in the world but like it's a well-written paper that to me like okay     
1:15:10     
like I know what they're saying here but it's it what it carries over into is what it is but like I I'm so like I I     
1:15:18     
really like that the review that we I put together about it because it's but     
1:15:24     
it's also like great material to work with so So that that makes it a little     
1:15:29     
bit uh more palatable maybe as a as a um St john's College graduate     
1:15:38     
uh I I just I love hearing people doing what we did which is you know go and     
1:15:45     
read these original papers and and also be you know both both pleasantly     
1:15:53     
surprised of what's actually in them yeah yeah yeah you know like like is     
1:16:00     
that you know when you've been citing a paper for you know your entire career     
1:16:06     
and then you you look at it and you're like wait this is this is what was um     
1:16:13     
and as well yeah just just how um incredibly     
1:16:20     
yeah some of these are short papers some of these are long papers     
1:16:27     
It's it's the the the struggle you have putting them into their historical context     
1:16:35     
yeah yeah yeah it's it's what I did for for so I like I like listening to     
1:16:44     
neuroscientists do the same thing with these complexity it's wonderful     
1:16:50     
yeah and and and just as a moment for like people a little bit less deep in the weeds like complexity     
1:16:58     
science is yeah it's it's complex but like it's     
1:17:05     
complexity science is is not it's it's hard to convey for folks who     
1:17:11     
haven't really waited in the space maybe you proudly your work can speak for this and we have other things to do but like     
1:17:17     
complexity science and this this paper series this I still be up so sharing my     
1:17:23     
screen this this book okay is David Kirkower is the head of the Santa Fe     
1:17:30     
Institute and he's put together a accessible kind of tour guide of his     
1:17:36     
take on complexity and it's and he's it's it's in in this book here but it     
1:17:42     
sort of draws from these four volumes that are foundational     
1:17:49     
papers in complexity science and I don't know how to really best convey that but     
1:17:57     
like in 19 29 uh there wasn't quote unquote complexity     
1:18:04     
science as we call it now you know for one thing and there really wasn't even cybernetics     
1:18:11     
uh which also doesn't really exist anymore but kind of does So it's just this this sort of um it's a very     
1:18:19     
interdiciplinary attempt to synthesize kind of a new perspective and there's a lot of debate in the complexity science     
1:18:25     
space about what it is and and like should you should you incorporate this     
1:18:30     
into the theory or not so it's definitely sort of a a new science um     
1:18:36     
and and and a lot of interesting debate in this sort of interdisiplinary field that's trying to look at things     
1:18:42     
differently um so it's not just it's not just like an established field that the     
1:18:48     
early papers are hard to read it's this synthetic attempt which kind of is what     
1:18:53     
you know what happened to cybernetics is something that I followed upon like you know     
1:18:59     
it does either you want to add anything to that um just for people who have no idea what complexity science is or uh     
1:19:07     
well I mean it you know it's supposed to be the study of complexity which is sort of taught logical but uh basically you     
1:19:15     
know a lot of science has been reductionist which means you drill down     
1:19:21     
and you find the parts that work together you find sort of the fundamental units of things and then try     
1:19:27     
to explain them through experiments or through some sort of uh you     
1:19:34     
know some sort of causal relationship or something like that     
1:19:40     
basically you know you can reduce a system down to its component parts and and those explain everything with     
1:19:47     
complexity science the uh sort of the the goal is to find higher order     
1:19:54     
structures uh things like networks or emergent structures and explain them in different     
1:20:01     
ways than uh we do with reductionism so with emergence we like to say the sum of     
1:20:10     
the parts is in the whole is greater than the sum of the parts that's a you know you can show that mathematically     
1:20:17     
perhaps you can show that statistically there are a number of ways you can arrive at that same with networks     
1:20:24     
networks are interacting components so you know just because you can boil things down to their fundamental     
1:20:30     
components doesn't explain everything because those things may interact and so you can describe these interactions of     
1:20:36     
the network and again it's you know there are behaviors that are sort of     
1:20:42     
meta behaviors or things that are greater than just describing the parts     
1:20:47     
and how they fit together and then the other thing about complexity theory is that you talk about universality     
1:20:54     
so like in you know if we can go from like psychology to computer science to     
1:21:01     
um you know mechanical engineering or you know electrical engineering and we     
1:21:07     
can describe systems in those different fields using the same techniques so networks are a great example because you     
1:21:15     
have uh biological networks you have uh technological networks you have social     
1:21:22     
networks and they all operate according to the same principles you can draw a     
1:21:27     
network you can get statistics out of the network you can describe things with     
1:21:33     
you know different network laws and things like that so it's very much like very inspired by physics in that you     
1:21:41     
have universality you have this quantitative view of things and you have kind of this     
1:21:49     
um uh way of kind of thinking about how     
1:21:54     
thing you know how things uh emerge u and you know so when we say     
1:22:00     
physics you know we kind of mean there's this old term I guess a natural philosophy of physics as being kind of     
1:22:07     
like you know physical objects but also this kind of approach it's almost you     
1:22:12     
know mathematical or has this sort of like in economics they talk about     
1:22:18     
physics or sometimes in social sciences I'll talk about social physics it just means this regularity and universality     
1:22:25     
of things so that's that's kind of where where complexity theory comes from what     
1:22:31     
it you know what they're trying to do and you know it's as an interdisciplinary interdisciplinary     
1:22:38     
enterprise it's useful because you can go between fields and you have a unifying framework that describes things     
1:22:45     
um um oh no that's that's about all I wanted to say     
1:22:52     
um I know we're kind of I'm sure you have other things to say but very quickly I'll I'll conclude by saying at     
1:22:59     
a really macro level I mean it's sort of a and I I'll reference this this this     
1:23:06     
paper from uh whatever I'm supposed to be here i'll reference this paper from     
1:23:11     
like um this is a 1943 paper I think before     
1:23:17     
the term cybernetics really came about but it was sort of this these sort of big big big thinkers of the time with     
1:23:24     
Weiner who kind of pushed cyber and resol too and and Big Little as well and     
1:23:30     
in 1943 you know this this was sort of a     
1:23:39     
um this is a microcosm of trying to reclaim a concept in philosophy around     
1:23:45     
teology and purpose um and and a lot of what complexity     
1:23:52     
science at large is doing is sort of I don't know if it's a reclaiming but there's sort of a major the     
1:23:57     
enlightenment and and the big scientific revolution all the stuff in Europe in the 17 whatever hundreds throughout kind     
1:24:05     
of you know to be a scientist meant you find you break things down you find you     
1:24:10     
were to find empirical data and if you couldn't find the data and you couldn't reduce it to     
1:24:16     
the data you couldn't you can reduce it to a measurement then what are you doing because you might be doing too much airy     
1:24:23     
fairy god stuff and we got to get that stuff out of town so there's a big there's a big pressure um to to like     
1:24:30     
there's there's a reason why reductionism and all this stuff uh took place and took hold and and yielded     
1:24:37     
things and and like it's this tough line between saying oh empiricism is     
1:24:43     
uh problematic or reductions are problematic to it just doesn't cover everything we need it to do um or it's     
1:24:50     
an incomplete perspective and there's a lot of tension between people saying oh yeah complexity is really important and     
1:24:56     
and other people saying like if you maybe everything boils down to to the flow of neurons in the brain like maybe     
1:25:02     
that's what it is like we just have to reduce it to it fundamental parts and then it's all physics from there everything else is just details kind of     
1:25:08     
thing um or not you know and there's there's contention in the development of the field and this paper which to kind     
1:25:15     
of complete the circle for the brain inspired complexity through u it's a fun     
1:25:20     
paper i'm not going to go through it now but um it's a fun paper because it's     
1:25:25     
basically these guys trying to reclaim this concept of any good pictures stuff     
1:25:33     
but trying to reclaim basically teology and and this idea of     
1:25:39     
purpose and then using it as a way to sort of categorize complexities of behavior and     
1:25:46     
that has implications for prediction and sort of feedback dubtales into     
1:25:51     
cybernetics and a lot of other modern fields so um I can leave it at that and     
1:25:57     
we can move on or anything else people want to say what we do     
1:26:03     
um yeah I think that's a good discussion i mean we can come back to it later but     
1:26:10     
I was diagramming out so those papers that we've been covering in the group     
1:26:16     
that we talked about I was diagramming that out on my note board so I I have     
1:26:22     
you know the reason I asked that original question was because it's fascinating to me why they're hitting     
1:26:28     
those specific papers what's the endgame where are they going     
1:26:34     
i think what they're I think right now it's honestly they were just trying to figure out they should go in order or     
1:26:39     
not so they're just the way the papers were presented in the book um and they're just they're literally just going through the book um okay that's     
1:26:46     
why but but there's been some discussion within the server and     
1:26:52     
I'm not I'm not I'm not super active in the server to to to disclose that but I     
1:26:57     
believe there's been discussion of like do we want to try to be thematic do we want to just go through them what do we try to do and I think it's kind of we're     
1:27:04     
trying to I think I think for everybody's I think that simplicity went in some chronological order and then are     
1:27:10     
just trying to find people that kind of feel qualified to sort of share the discussion about the the paper     
1:27:17     
so I know it's rag tag but but maybe the way forward I know that in the book uh     
1:27:23     
and we've talked about the book before that they have that he has a very useful sort of typology of different papers     
1:27:30     
that he considers to be the foundations of yeah and and it's I'd be cur like that's     
1:27:38     
one thing I don't quite understand like how does the book that David Kauer put of his own work the complex world     
1:27:45     
book compared compared to the the series like because we're kind of just going     
1:27:51     
through the series of the the raw papers right now so because I can see a you know I can     
1:27:57     
see like uh connections between the papers myself i'm just wondering if     
1:28:03     
that's what they're thinking that's why I asked the question so like you know right in Wington there's a lot of     
1:28:09     
connection there wright's 32 paper talks about fitness landscapes here introduces     
1:28:15     
the concept and the way he does it is by drawing out like these landscapes where you have areas of high fitness areas of     
1:28:22     
low fitness and it's like you know drawing from population genetics and     
1:28:28     
putting that into a a draw basically a diagram that describes it spatially or     
1:28:34     
you know in a way that's like multi-dimensional space and what's interesting about that is that you know     
1:28:40     
that is there are connections between that and like     
1:28:46     
uh like energy gradients and some of the stuff that people are doing machine     
1:28:52     
learning and even like evolutionary computation things like that and so that     
1:28:58     
whole concept is interesting he doesn't really frame it in terms of like like energetic energetic landscapes which     
1:29:05     
people were talking about back then but it's a it's a concept that's really interesting and it's the first time     
1:29:12     
people kind of made that connection and then people developed uh fitness landscapes you know later on more     
1:29:19     
computationally more more statistically then also talks about     
1:29:25     
landscapes but from an epigenetic standpoint so epigenetics you know of course is     
1:29:31     
development and the idea behind an epigenetic landscape is that can we     
1:29:37     
find regions in a landscape that give you stable phenotypes in development so     
1:29:42     
there's this concept of buffering or channelization where you find these     
1:29:49     
basically these channels in the landscape where development always proceeds down these trajectories because     
1:29:56     
they're the most stable places so if you look at like a any sort of embryo     
1:30:02     
um you know you have a great potential to sort of if you have like a bunch of     
1:30:07     
eggs like maybe in um insect eggs or other types of eggs the offspring could     
1:30:14     
look very different but they don't they're often you know very they're all basically of the same they maintain     
1:30:20     
their species identity most of the time they maintain the same phenotype and so     
1:30:26     
the question is how does that happen how does that get maintained so you know and then of course he's talking about     
1:30:32     
landscape so that connects to right so there you know there's this landscape aspect then the Xylard and LKA papers     
1:30:40     
locka talks about um you know a lot of it kind of comes     
1:30:46     
from a physics standpoint but a lot of what his work did was inform ecology and     
1:30:53     
how you have these predator prey models which are these co-evolutionary models     
1:30:58     
where you get a lot of predator you they predate on the prey and the population     
1:31:04     
of prey crashes but when that happens then the population of the predators also crashes which allows the pre     
1:31:12     
population of the prey to increase again and so on and so forth and you get these     
1:31:18     
uh inner connected cycles um these predator prey dynamics and so     
1:31:24     
it's dynamical systems but it's really kind and has a lot of implications for     
1:31:29     
biology um then Xard talked about um you know he is famous for the Xyard     
1:31:37     
engine um you know which is this idea that you can kind of escape the ravages     
1:31:43     
of entropy uh you can build these you know uh this engine that has like basically     
1:31:49     
breaks some of thermal dynamics so he talks about that [Music]     
1:31:55     
um yeah and this has a lot of relevance to the Maxwell's demon which we've     
1:32:01     
talked about in the group a lot which is where you have this process that's     
1:32:07     
anti-entropic it doesn't result in greater entropy over time it results in less entropy and it gives you order     
1:32:13     
instead of just kind of randomness so I could see like where you have like all     
1:32:19     
these papers kind of very mathematical in in their own way and they kind of fit     
1:32:24     
into a larger theme um of kind of like thinking about landscapes thinking about gradients     
1:32:31     
maybe and thinking about populations thinking about like entropy and thermodynamics and then     
1:32:40     
makulla pits is where you have this neural representation of some type of connectionist     
1:32:45     
representation with some biohysical properties this is like you know the foundation really the foundations of     
1:32:54     
uh connectionism but it's the foundations of like take you know     
1:32:59     
processing units kind of modeling those putting     
1:33:04     
them together into networks and then you know we get later we get     
1:33:09     
connectionism which you know isn't as biohysically inspired but we get a lot of     
1:33:14     
models we can model what's going on in the brain     
1:33:19     
there's been a lot of ink spilled about like the con these connections uh people have written papers about like neural     
1:33:26     
models and other types of models and you know like networks and connections so     
1:33:32     
there are all these like themes that are very nebulous out there that people have made and I'm just wondering you know if     
1:33:40     
that's kind of what people were thinking when they did this i don't think it maybe it was but um I'm I'm I have to     
1:33:47     
imagine that uh Brian what what what's his name again i     
1:33:55     
keep forgetting his name i I'm in literally in the server with the person who leads Brainsburg     
1:34:02     
um uh     
1:34:08     
it's Paul paul Middle yeah there we go geez I don't know why i think this Brian Brain stuff thank you Morgan for saving     
1:34:14     
me um anyway um there isn't anything like that yet     
1:34:20     
i'm sure I'm sure I'm sure they're going to do some kind of a synthetic work at least I hope they do that said I mean     
1:34:27     
Bradley you just sort of freestyle wrapped on four early foundational     
1:34:32     
papers in complexity science so maybe we should put something together for that um I I I've been looking at     
1:34:39     
[Music] um basically pattern langu languages for other things too so maybe there's some     
1:34:45     
some some some patterns and and some things to at least at least to map it     
1:34:51     
like you can map map at a high level conceptually map out what these papers are getting at     
1:34:57     
and maybe show uh I'm sure I'm I'm certain that if we     
1:35:03     
just summon the the inevitable future the inevitable future act of summoning     
1:35:10     
the David Crackower Gen AI agentic AI bot to convert of this this call which     
1:35:16     
you know is going to happen even like 11 bots just you know pre I'm sure he'd be like yeah here's     
1:35:23     
here's some you know some some some themes and concepts and ways to to do     
1:35:29     
this and blah blah blah blah blah blah blah blah blah but um the materials and artifacts are to be made so     
1:35:36     
um maybe we'll do something there that way it's a it's a nice it's a nice way     
1:35:42     
to push back rigorously ly or attempt to find a rigor to push back against some of the     
1:35:48     
the inadequacies of reductions and they're on this one     
1:35:55     
but uh whoops um I just found this again and posted it     
1:36:00     
big bots this quote always gets me but basically how but also the development     
1:36:06     
of ethical social frameworks that will become essential in the future forms of agents or subversive part of I think the     
1:36:13     
the higher appeal of attempting complexity scientists realize reductionism as it has been there's     
1:36:20     
there's a constraint on certain critical spaces of I don't want to say identity     
1:36:25     
and I think Lean's talking like way you know uh totally changing the game living     
1:36:32     
machines are not 20th century machines paper and all that stuff but I found this again and next I'm sure we should     
1:36:38     
move on to other things at this point so so let's do that Yeah uh let's see you had one more thing     
1:36:47     
in the chat the neuroai science this is not not not related to this but when     
1:36:53     
you're talking about Levvenbot I just wanted to say definitely check out um     
1:36:59     
recent post by Russ Pre um he's working on a new book but um he's also he he's     
1:37:07     
always had very interesting uses of of     
1:37:13     
um new technology and as example projects and he's got uh one of his     
1:37:21     
former posttos now works at um at Anthropic and I I I'm not going to     
1:37:29     
say that's why he's biased towards Claude but um but he's been posting     
1:37:36     
about his his use of um his use of Claude uh uh separately from the book     
1:37:46     
that he's working on but but both both should be should be mentioned     
1:37:53     
where where do you follow his stuff is it like his blog or like I Yeah I I so     
1:37:58     
so um um like like everyone um these     
1:38:03     
days uh he seems to have started a stuff stack okay okay so he is there i'm not     
1:38:10     
condoning that but uh um and uh     
1:38:16     
but honestly I get uh I get his updates on LinkedIn okay yeah okay good to know     
1:38:23     
okay thank you for your patience Bradley i feel like we have many other things to cover     
1:38:28     
falling off the leg called releasing the crack hour so     
1:38:35     
that's great that's great yeah all right so then I     
1:38:40     
wanted to talk about this neuro neuroi archive which was the Patrick's     
1:38:47     
uh I guess substack or something um and so this is the uh what we that Jesse     
1:38:53     
posted in the chat so this has a lot of really interesting articles     
1:38:59     
here and just kind of merging neuroi with that AI safety stuff and um yeah     
1:39:07     
just kind of going over some other like brain scores which are these tools that     
1:39:13     
they use to compare artificial neural networks and biological neural networks     
1:39:19     
um uh looking at automation of science and different tools we can use in     
1:39:25     
science foundation models for neuroscience we've talked about foundation models and how you know they     
1:39:31     
there are these large scale models but beyond that the definition is kind of     
1:39:37     
ky and then uh yeah so there are a lot of interesting articles here to look at     
1:39:44     
so I wanted to finish up with two papers this is under a category called bioinspired methods um I believe this is     
1:39:52     
a paper that VD posted I think in the uh     
1:39:58     
organoids channel and the reason it is in the organoids channel is because it talks about dish brain which we've     
1:40:05     
talked about before so this is a good dish print paper from the     
1:40:11     
archive and cortical labs is involved in this and um I don't know some other     
1:40:19     
organizations uh so this is biological neurons compete with deep reinforcement learning in sample efficiency in a     
1:40:27     
simulated game world so they're using dish brain and I guess with comparing that with deep reinforcement learning     
1:40:33     
which we've talked about in past meetings um so let's go over the     
1:40:41     
abstract uh so the abstract reads how do biological systems machine learning     
1:40:47     
algorithms compare on the number of samples required to show significant improvements in completing a task we     
1:40:55     
compared the learning efficiency of in vitro biological neural networks to the     
1:41:00     
state-of-the-art deep reinforcement learning algorithms in a simplified simulation of the game pong so they're     
1:41:07     
looking at the the invitro biological neural networks are these uh you know you have a bunch of neurons you have a     
1:41:14     
micro uh electrode array which records the activity and there's some feedback     
1:41:20     
to that network from the game so if you have like you know you want to send a     
1:41:25     
closed loop control signal the neural network will produce some signal it will     
1:41:33     
have some act action in the game and then there's feedback from the game you know either you hit your target or not     
1:41:40     
and that's a stimulation of the network so uh people have built these for for     
1:41:45     
quite a number of years where you're you know you're stimulating the the neural network in a dish you're sending uh some     
1:41:53     
signal out of the dish recorded from the electrodes and you're sending the signal into the game so it's this closed loop     
1:42:00     
control that mimics kind of you know kind of mimics a human player of Pong of     
1:42:06     
course Pong is a low complexity video game so you only have like a very small     
1:42:12     
number of moves and the strategy is basically there's a single strategy     
1:42:19     
which is to hit the ball with the paddle and to match that so it's kind of like     
1:42:24     
in in robotics they have something called the pull balancing task which is     
1:42:30     
where the whole point of the control it's like this one-dimensional control where you just need to a pole upright if     
1:42:37     
the pole starts to fall in one direction or another you just move in that to correct it along that one-dimensional     
1:42:43     
axis and you can keep it upright and that that's that's sort of a sign of robust control so that's what we're     
1:42:51     
doing with I guess with dish brain but we're also trying to do this with deep reinforcement learning to see if that     
1:42:58     
can be done efficiently or how dishrain might compare     
1:43:04     
so using dishb brain a system that embodies in vitro neural networks with incilico     
1:43:10     
computation which is just to say that there's a computer that you know or some     
1:43:15     
program that's uh I guess augmenting what happens the signals that come out     
1:43:21     
of the uh the dish brain um using a high     
1:43:26     
density microeler array which we discussed we contrasted the learning rate and the performance of these     
1:43:32     
biological systems against time matched learning from three states of the art of state-of-the-art     
1:43:39     
deep learning uh reinforcement learning algorithms so they're looking at DQN A2C     
1:43:45     
and PO we talked about PO last week i think we talked about DQN in the past     
1:43:52     
um you we kind of talked a lot about reinforcement learning because we're of course was interested in reinforcement     
1:43:58     
learning for some of our projects but I think moreover reinforcement learning um     
1:44:04     
is a good sort of comparison to these kind of in vitro biological neural networks because it really compares I     
1:44:12     
mean in one sense it's like comparing apples and oranges reinforcement learning I mean reinforcement learning     
1:44:18     
has its roots in the brain because um     
1:44:23     
people propose that our brains do reinforcement learning but on the other     
1:44:28     
hand the way reinforcement learning tends to happen is you have these policies that you select from so if     
1:44:34     
you're talking about a reinforcement learning algorithm you're talking about uh choosing between a set of policies     
1:44:41     
and so yeah I mean you know deep reinforcement learning has its own kind     
1:44:46     
of features but it is an interesting comparison um I think it's     
1:44:52     
really kind of a almost like a useful I mean if you approach it from one     
1:44:58     
perspective it's a nice kind of way to look at like you know neural information     
1:45:05     
processing but from another perspective it is like comparing apples and oranges     
1:45:10     
um because you you were like using kind of different ways to do this uh this     
1:45:16     
allowed a meaningful comparison between biological neural systems and deep reinforcement     
1:45:21     
learning which is the way they did it the comparison here we find that when samples are limited to a real world time     
1:45:29     
course even these very simple biological cultures all perform deeper enforcement     
1:45:34     
learning algorithms across various game performance characteristics implying a higher sample     
1:45:40     
efficiency so this means that uh deeper     
1:45:46     
reinforcement learning algorithms underperform these dish brain uh     
1:45:52     
constructs these uh biological networks um and this is where we have a real     
1:46:00     
world time course meaning that we have real world information or real time information coming in I guess and you     
1:46:07     
you know it's hard to predict what the next state is um ultimately even when tested across     
1:46:14     
multiple types of information input to assess the impact of higher dimensional data     
1:46:20     
input biological neurons showcased faster learning than all deep reinforcement learning     
1:46:26     
agents so this is you know biological neurons are doing learning of course they're doing heavy learning they're     
1:46:34     
doing some maybe some sort of reinforce biological reinforcement learning which     
1:46:39     
means that when they get a reward like if they do you know if they're able to say hit the ball with the paddle they     
1:46:46     
kind of know or I wouldn't say they kind of know but they have this     
1:46:51     
uh reinforcement of that thing so you get happy learning you get a     
1:46:57     
reinforcement of a certain action by the network and certain connections are     
1:47:04     
strengthened and you end up with this um you know sort of natural     
1:47:10     
reinforcement learning based on heavy or scaffold them on heavy whereas in your     
1:47:15     
deep reinforcement learning agents you're having to learn policies or choose between policies sometimes those     
1:47:22     
policies are emergent sometimes you have to decide what those are in advance so     
1:47:27     
there are a lot of in the design of a reinforcement learning agent you have to kind of think about all those things um     
1:47:35     
so that's that's what they're saying and basically the lesson here is that you know as murky as the sort of behavior of     
1:47:45     
a you know a dish brain or a biological neural network is we can that maybe is     
1:47:52     
faster than what we can design in agents so that's kind of the lesson the only     
1:47:59     
problem is of course is it's hard to do things like credit assignment in a     
1:48:04     
biological neural network because we don't have you know we can't trace back     
1:48:10     
every sort of information processing step in a network like that     
1:48:16     
and sometimes you there aren't even discrete steps sometimes there are a lot of things that are emergent or there are     
1:48:22     
a lot of things that can happen simultaneously and so we can't disentangle the effects of those things     
1:48:28     
so that's that's kind of what they're getting at this paper so you know we've talked about     
1:48:34     
dish brain in the past and how they've used it to play games and we said "Yeah this could be you know less impressive     
1:48:42     
than it seems because I can't remember exactly what     
1:48:47     
the criticisms are were but um let's put that into let's put that into context     
1:48:53     
maybe we can go back and think about what those criticisms were and kind of     
1:48:58     
bring you know that to this paper but not maybe not right now um so the     
1:49:05     
introduction reads both biological and machine intelligence systems demonstrate the ability to learn and achieve goals     
1:49:13     
although the complexity of and drivers behind these tasks may differ     
1:49:18     
comparisons between these types of systems can yield valuable insights     
1:49:23     
uh even definition of what traits artificial intelligence should demonstrate are heavily informed by     
1:49:29     
traits observed in biological intelligence in other words when we talk about artificial     
1:49:35     
intelligence we use biological intelligence as sort of a biased comparator so we said well you     
1:49:43     
know we know what intelligence looks like in the real world and we're going to model it uh using an artificial     
1:49:51     
algorithm and if it exhibits things like intelligent behavior that aren't     
1:49:57     
biological we won't necessarily be able to recognize them so you know that     
1:50:03     
that's I know it sounds kind of weird because you're like well that's what intelligence is right like it's just     
1:50:08     
something that we can see in nature and it they should all you know intelligence should look the same across every     
1:50:16     
context well we had this discussion about the definitions of intelligence how we have many definitions of     
1:50:23     
intelligence across different fields and how those are very different um so     
1:50:29     
intelligence may mean one thing it may mean many things and uh we don't really know i     
1:50:37     
mean basically intelligence is this comp complexity of behavior so it's you     
1:50:45     
know it might be some learning it might be some memory it might be some prediction you know and it's a lot of     
1:50:51     
things that result in something we may or may not be able to identify as     
1:50:56     
intelligence i know it almost sounds also like you're saying well if I build     
1:51:01     
an artificial intelligence whatever it produces is intelligence and you know I     
1:51:07     
can't say I'm never wrong about that but uh but anyways it's it's a tough thing     
1:51:13     
to kind of disentangle um but that being said what we're actually interested in     
1:51:20     
are these comparisons between biological and machine intelligence and so that's     
1:51:26     
why we're interested in kind of benchmarking artificial intelligence on     
1:51:31     
biological intelligence because ultimately we want to compare them and you know we want to say maybe which is     
1:51:38     
better or if we're getting towards sort of intelligence in our machines and our     
1:51:44     
representations so it's been notoriously difficult to sort of compare biological machine     
1:51:51     
intelligence as the scale of connections in even simple biological organisms far     
1:51:56     
exceeds those found in artificial neural networks or comparable machine learning     
1:52:02     
algorithms however by taking a systemsbased approach we aim to compare data gathered from a biological neural     
1:52:10     
network or BNN using the dish brain system and     
1:52:15     
we're doing this against time match learning from deep reinforcement learning algorithms we're comparing DQN     
1:52:21     
A2C and PO with dish despite the inherent differences between silicon and biological systems     
1:52:29     
such as power consumption and network size this approach makes it possible to explore learning performance and sample     
1:52:36     
efficiency so we have much larger networks in artificial     
1:52:43     
systems and much greater power consumption than artificial systems and     
1:52:48     
yet that doesn't necessarily produce the kinds of behavioral complexity that we see in biological systems that's     
1:52:55     
basically the problem and so we want to know first of all why that is and second     
1:53:00     
of all how can we overcome these limitations so if we could build like     
1:53:06     
you know artificial systems that not only did     
1:53:11     
things that were similar to biological intelligence and performed at a similar level or a     
1:53:17     
similar rate of information processing but do it with smaller networks and     
1:53:23     
lower power consumption um this is not trivial because deep learning is running into this problem     
1:53:30     
where we're building these massive networks we're doing massive amounts of     
1:53:35     
power consumption and we would like to make those systems more efficient if possible     
1:53:44     
so such should compelling differences be found would further support extended     
1:53:50     
efforts to understand key differences in the information processing dynamics unique to each system so they are making     
1:53:56     
the argument that artificial systems might have in you know unique features     
1:54:02     
that you don't see in biology but again you know how do we know that's intelligence or how do we know that's     
1:54:09     
you know just an artifact of the artificial system it's a it's it's really kind of a hard thing to sus out     
1:54:16     
um but but basically uh they talk about reinforcement learning how this is like     
1:54:22     
a very popular way to do uh simulated     
1:54:29     
intelligence um and so uh you know we have to kind of think about     
1:54:35     
reinforcement learning in the artificial sense and then kind of map that to     
1:54:40     
biology so reinforcement learning implies learning the best policy to     
1:54:46     
maximize an expected cumulative long-term reward through many steps in     
1:54:52     
order to achieve objectives or goals a deep reinforcement learning approach integrates artificial neural networks     
1:54:59     
with a reinforcement learning framework that helps the system to achieve its goals it maps states and actions for the     
1:55:07     
rewards that they bring com combining function approximation and target     
1:55:13     
optimization so this is where we have these states and rewards that are or states and actions that are these     
1:55:20     
representations things happening in the network but those are the things that we want the network to learn and to     
1:55:27     
optimize so that's and of course in brains we don't know if they're necessarily optimizing for that but     
1:55:35     
that's you know the outcome is basically to produce these things that we can     
1:55:40     
recognize as states and actions um and so you know reinforcement     
1:55:46     
learning has allowed us to build systems that have beat human experts in     
1:55:51     
different types of gameplay like poker and multiplayer games complex board     
1:55:57     
games go chess and the suite of Atari video so we talked about that last week     
1:56:04     
where we had these uh deep reinforcement I think it was PO that was performing on these different benchmarks some of them     
1:56:11     
are Atari games some of them are other uh benchmarks so this is a common thing     
1:56:18     
in the field uh reinforcement learning still faces real challenges including but not     
1:56:24     
limited to complexities in the selection of hyperparameters and word structure sample     
1:56:31     
inefficiency reproducibility issues and catastrophic forgetfulness so we have     
1:56:37     
these features of these uh artificial systems that are brittle we     
1:56:43     
have forget uh catastrophic forgetting we have this sample inefficiency which     
1:56:50     
means that you have a lot of samples and not a lot of learning uh where you know and we also have these hyperparameters     
1:56:57     
that we need to tune that we need to figure out how to sort of optimize a     
1:57:03     
reward structure to make sure that those get tuned optimally now in biological     
1:57:08     
brains we probably may or may not have hyperparameters but that's not something that we typically can identify or we     
1:57:17     
don't know what the biology is doing to optimize its performance so hyperparameters aren't really a concept     
1:57:24     
in biological networks except that you know maybe if we modeled it we could     
1:57:30     
identify different things that could be optimized um sample you know biological     
1:57:36     
networks are very sample efficient which means that they use maybe a small number of samples to learn things extract     
1:57:43     
things from stimuli and then uh biological networks can encode memories     
1:57:50     
there's forgetfulness in biological systems but not this catastrophic forgetting where they     
1:57:55     
forget you know like if you switch context uh if you switch to a new task it     
1:58:02     
forgets everything from the old task and those are the things that you know for now we really don't understand how those     
1:58:08     
work in biological networks so they talk about dish brain and I don't know if     
1:58:15     
this is like the way they approach this is that dish brain is the second coming of uh greatness but like there's     
1:58:24     
this uh you know definitely they want to make the case that biological networks     
1:58:30     
are superior to uh artificial networks and maybe to understand why so dish     
1:58:36     
brain of course um has this closed loop control system your deep reinforcement     
1:58:41     
learning algorithm also does this closed loop control but it has to measure all     
1:58:47     
these aspects of the environment and then put them into this uh deep network     
1:58:52     
and then you have to extract policy the optimal     
1:59:00     
policy okay so I'm not going to get into this stuff i don't think there are a lot of nice     
1:59:07     
kind of results here where they talk about they make the     
1:59:13     
electrphysiological sort they use electrophysiological measures to look at     
1:59:19     
uh the performance of dish brain and then you're looking at paddle and ball position in the reinforcement learning     
1:59:26     
algorithm case so you're making comparisons there between the two     
1:59:32     
systems and you know they they have these analyses so I don't want to get     
1:59:38     
into that too much um but just to say that I guess dish brain outperforms     
1:59:43     
these deep reinforcement learning algorithms um see if we can get down to     
1:59:50     
the discussion that might use I guess we have all these tables i     
1:59:57     
won't get through any of those [Music]     
2:00:05     
um well I um this is a very     
2:00:12     
long results but let's see if I can f okay uh results and then we have     
2:00:24     
our discussion okay so let's get into the discussion so the advantages     
2:00:29     
vantages and disadvantages of biological versus machine intelligence are often discussed the technical limitations     
2:00:36     
prevented some comparisons and they do the comparison uh and uh so while direct     
2:00:44     
comparisons between these systems are naturally constrained even what is referred to as a neuron is inconsistent     
2:00:50     
between fields of research so that's a problem that we have to think about you know maybe we need better     
2:00:56     
representations of neurons and artificial networks maybe it's you know that's what the secret sauce is and why     
2:01:03     
we can't make this direct comparison or can we make performance jump um the aim     
2:01:09     
of this work was to determine whether meaningful performance differences would arise between learning systems as     
2:01:16     
contained systems so we have these this contained neural network in a dish and     
2:01:21     
then we have this contained algorithm on a computer and you know we we have we know what the limits of those     
2:01:28     
systems are so we can kind of you know figure out what inside them is is     
2:01:33     
contributing to this difference um and so you know this says     
2:01:38     
you know maybe we should explore biological networks as computational     
2:01:44     
machines so that's that's the argument     
2:01:49     
um and so yes this is all consistent with past research um and they actually used the     
2:01:57     
biologically inspired algorithm to explore some of these um some of the     
2:02:03     
benefits of the biological network so they implemented an active inference agent that uses counterfactual learning     
2:02:11     
and they report these uh results in the supplemental materials improve learning rates observed in the biologically     
2:02:18     
inspired learning protocol supports the potential of active inference agents to     
2:02:23     
provide valuable insights into optimized learning strategies thereby enhancing our understanding of these dynamics     
2:02:30     
however these active inference algorithms are still highly dependent on the chosen hyperparameters require     
2:02:37     
relatively higher power consumption compared to biological systems nonetheless these results highlight the     
2:02:44     
value of further exploring biologically inspired systems of learning and support     
2:02:49     
the notion that SBI systems or I guess uh I don't know what SBI systems are may     
2:02:56     
offer a useful pathway to do this in the future so that's uh that     
2:03:05     
paper we had a bunch of uh discussion in the chat here um     
2:03:13     
uh this is directed at VD from Jesse you're always welcome to give updates you can say     
2:03:19     
anything um yeah this is just kind of talking about updates okay so why don't I get to the     
2:03:26     
other paper this is another archive     
2:03:34     
paper and this is the role of bioinspired modularity in general so this is talking about this     
2:03:42     
bio inpired aspect of learning and and     
2:03:47     
modularity um this is Baron Holtz who heads up echolapto so this is an coming     
2:03:54     
from I was just gonna say that he's at Galapto and there's a lot of stuff in the cognitive space it's good to see the     
2:04:01     
paper yeah yeah i've been checking out their work and they have they're doing a     
2:04:06     
lot of theoretical work um that kind of ties together AI and the brain that's     
2:04:12     
kind of their space so they're doing a kind of a similar thing to the last paper but they're approaching it from     
2:04:19     
more of a theoretical     
2:04:27     
standpoint okay so the abstract reads one goal of     
2:04:33     
general intelligence is to learn novel information without overwriting prior     
2:04:38     
learning so we want to be able to again not have these catastrophic forgetting     
2:04:44     
events where the system forgets everything it learned if you switch     
2:04:49     
context um you want to be able to learn from novel stimula you don't want to have to sample it 40 times or 50 times     
2:04:57     
you want true oneshot learning and when we talk about oneshot learning we talk about like some of that is like     
2:05:04     
very general features or very easy to to reach features we want to have this sort     
2:05:11     
of system where we can learn novel information that isn't sort of consistent with uh the distribution that     
2:05:18     
we're sampling from but we also want don't want to use that as sort of like the mean behavior we want to have put     
2:05:27     
that sort of exception in our database and we don't want to overwrite     
2:05:33     
everything we've learned about the world prior to it so it's a very delicate     
2:05:38     
balance between learning novel information but not thinking that it     
2:05:43     
represents you know the mean information of the world it's kind of like if you     
2:05:49     
taste new foods you know you don't think that every new food you taste is sort of the way food tastes you dis make the     
2:05:56     
distinction between different types of tastes and different types of foods and     
2:06:02     
you make exceptions for some foods over others you make comparisons between     
2:06:08     
everything tastes like chicken say then you have like different foods that you can describe very distinctly so that's     
2:06:15     
something that um is a general intelligence feature but something that artificial intelligence can't     
2:06:21     
necessarily do the utility of learning without forgetting or CF is twofold first the     
2:06:28     
system can return to previously learned tasks after learning something new in     
2:06:34     
addition so they can you know you can switch contexts you can come back to the old context and you can compare it with     
2:06:40     
that new context and you can make a generalized statement or you can make a statement that what's what's unique     
2:06:47     
about a specific context that's you know it sounds kind of simple but it's really     
2:06:54     
not uh in addition bootstrapping previous knowledge may allow for faster learning of a novel task previous     
2:07:01     
approaches to CF and bootstrapping are primarily based on modified modifying     
2:07:07     
learning in the form of changing weights with a tune to tune the model to the current task overriding previously tuned     
2:07:14     
weights in the previous task so this is again what I was describing with catastrophic forgetting or the reason     
2:07:21     
why that happens and the reason why happens is because networks especially like deep     
2:07:27     
learning networks um always have to kind of encode everything in their     
2:07:32     
weights because of that they have to change their weights every time they learn new things or switch contexts and     
2:07:40     
so those old weights get overwritten so you'd have no way of preserving the weights from the previous context you     
2:07:46     
could you know save the weights and then average the new weights at the old weights but that is also not sufficient     
2:07:54     
because that just gives you a big smear of information it doesn't give you any     
2:07:59     
distinctions or comparisons for that so this is the problem um it's definitely     
2:08:07     
an artifact of artificial neural network biological neural networks probably use     
2:08:12     
some mechanism a lot of different mechanisms for encoding memories aside     
2:08:18     
from just the weight structure so we know that like you know we have supporting cells we have different     
2:08:25     
neurotransmitters we have uh different places when memories are encoded we have     
2:08:31     
networks where you know we can although we can interfere with memories     
2:08:38     
pharmacologically we also have distributed memories and those distributed memories are robust so that     
2:08:45     
mean may mean that they have multiple different types of encodings in the     
2:08:50     
brain so there are a lot of ways that biological networks can overcome or compensate for this problem overwriting     
2:08:58     
weights or averaging weights or whatever um however another critical okay however     
2:09:06     
another critical factor that has been marginally overlooked is the initial network topology or architecture so this     
2:09:14     
is where we have the structure of the network and that architecture is important because it does give you     
2:09:21     
information the network topology if it's a big circle of neurons or if it's like     
2:09:26     
a layers of neurons that have like this random connectivity or this full     
2:09:33     
connectivity those are all sort of you know you can each of those connections     
2:09:39     
have weights but if there's some sort of structure to the topology the structure     
2:09:45     
matters so it's you know it's not just as simple as having like these fully connected layers and just weighting them     
2:09:52     
accordingly and producing sort of a structure you can actually have physical changes in the structure of the network     
2:10:00     
or physical arrangements of the network structure that can actually aid learning     
2:10:06     
or aid that encoding of that information as I guess you could say weights but     
2:10:12     
like I'm thinking more about heavy in learning where you need to make you know comparisons of is my neighboring neuron     
2:10:23     
active and if I'm active and they're active active then maybe there's a connection there um there's this dictim     
2:10:30     
in heavy learning which is um neurons that fire together wire together and     
2:10:37     
that's kind of you know if you do that you get you know weight that's high um     
2:10:43     
but you also it's not as simple as just because you can imagine that if you had a if networks were just comparisons of     
2:10:51     
pair-wise neurons that would be one thing but you have all sorts of other partners that you have to listen     
2:10:58     
So it's not as simple as just saying you know we're going to find some weights     
2:11:05     
those weights to describe learning and then we can you know those are just an easy uh heristic for learning there's     
2:11:12     
there's probably more going on probably to do with how the networks are connected what partners are uh     
2:11:19     
preference you know get preferential treatment over other partners and so forth here we argue that the top     
2:11:27     
topology of biological brains likely evolve certain features that are designed to achieve this kind of     
2:11:33     
informationational conservation so what they're saying is that in biological brains you have structures those     
2:11:39     
structures have specific types of connectivity if you go to the cerebellum for example it's connected in a very     
2:11:46     
different way than the um than the medial temporal or the hippocampus     
2:11:53     
you also have the neoortex which is wired in a different way or structured in a different way than the phalamus and     
2:12:01     
of course functionally they do different things and so there's but there's a reason for that it's because in     
2:12:07     
evolution they've you know acquired a function specific function and now     
2:12:14     
they're interacting with one another but not all to all there are specific     
2:12:19     
connections between say the phalamus and the cortex so in the phalamus locally and in the     
2:12:25     
cortex locally there's information processing going on there's structure that's unique to those structures but     
2:12:31     
then there are interconnections that link those functions and link together for larger scale functions and those are     
2:12:38     
all kind of put there by evolution in particular we consider that the     
2:12:45     
highly conserved properties of modularity and modularity is just where you have compartments of things so     
2:12:52     
instead of having like a a brain is like this undifferentiated structure of     
2:12:57     
connections and you just get this huge network that and just expand the size of     
2:13:03     
the network and you get more computational power you split that network up into pieces or compartments     
2:13:10     
those compartments are specialized were structured in different ways and then they're connected independently of that     
2:13:18     
local structure to give you this large network but the the large network has different pieces that kind of fit     
2:13:25     
together so that's what they mean by modularity and there's this whole history it's kind of notorious in     
2:13:31     
cognitive science about cognitive modules and modularity which I won't get     
2:13:36     
into here so they're kind of treading in this sort of uh dubious space with     
2:13:42     
respect to the history of cognitive science but I think the idea of modularity is     
2:13:47     
basically interesting and somewhat correct in that you know you have to     
2:13:53     
have this sort of local structure that translates into this global structure     
2:13:58     
and the global structure gives you properties that the local structure can't solve on its own it is a bit like     
2:14:05     
emergence in the complexity and in fact in complexity theory a lot of times     
2:14:10     
people will talk about local and global effects those local effects are conditioned upon modularity so that's an     
2:14:18     
interesting connection there so modularity which is conserved in     
2:14:24     
evolution a lot of your structures like the phalamus go back you know the they're phoggenetically ancient they     
2:14:31     
have common ancestry that's deep in the tree of life uh the cerebellum is the     
2:14:37     
same way so there's this modularity that's highly conserved and that may     
2:14:42     
offer a solution to the weight update learning methods that adhere to learning without catastrophic forgetting and     
2:14:49     
bootstrapping constraints so in other words we can overcome catastrophic     
2:14:54     
forgetting due to weight update learning methods by this uh principle of     
2:15:00     
modularity by this principle of linking together modules into this information     
2:15:06     
processing machine that's very phoggenetically distinct very you know     
2:15:12     
uh heterogeneous final considerations are then made on how to combine these two     
2:15:18     
learning objectives in a dynamical general learning system so you're thinking now about learning as a     
2:15:25     
dynamical system and what these models give you in terms of     
2:15:30     
enabling so you know this is a theoretical paper kind of walks through their argument um and so this is all     
2:15:39     
very general stuff it doesn't draw I don't think from any specific well I guess they're examples     
2:15:46     
from neural imaging um you know and then links to     
2:15:51     
computation but it doesn't really give you uh you know I don't think they're trying to implement anything here where     
2:15:58     
they don't give you any specific biological details beyond like     
2:16:04     
talking about this idea of neural architectures and where they come from     
2:16:09     
phoggenetically So complex neural architectures are thought to have emerged in the Cambrian     
2:16:15     
explosion or about 540 million years ago with records of both camera eyes and the     
2:16:21     
emergence of animals with complex behaviors in contemporary species the     
2:16:27     
four basic brain structures the telenephylin the cerebellum the     
2:16:32     
dianphylin and the mezzanine are preserved across nearly     
2:16:37     
all mamalian species so those four structures um you know you'll find this in like in     
2:16:45     
fish brains you find this three-lobed brain in mammals you find these four     
2:16:51     
structures those all have sort of a common ancestry so the common ancestor of mammals have these four uh     
2:16:59     
structures uh the common ancestor of fish has three structures and you know     
2:17:05     
you're building upon these basic this basic architecture and so in mamalian species     
2:17:12     
you find um different versions of these four structures they've been     
2:17:17     
differentiated into different other structures but you can trace them all back to these four and you can trace     
2:17:23     
them back to a common ancestry so this is where you know you get this deep     
2:17:28     
conservation and you don't see this sort of structured insects they have their own highly preserved modular nervous     
2:17:35     
system structures and that has a deep common ancestry as well so the idea is     
2:17:40     
you evolve these basic modules and then they differentiate across the evolution     
2:17:46     
of of that uh cate or that that taxonomic grouping and so you know     
2:17:54     
that's so then they get into how this works in humans the modularity that we     
2:17:59     
see in neural imaging and all of that so that's their argument so that's a nice I think it's a     
2:18:06     
nice kind of overview of that um okay so yeah um I think that's     
2:18:13     
all for today morgan put a link to Russ Cold Substack and why better code can lead to     
2:18:20     
better science I guess     
2:18:25     
as well as it's got a link for his book okay the book is working all right yeah     
2:18:31     
so check that out um so any other comments or     
2:18:45     
questions all right so I guess that's all for today um     
2:18:52     
thanks for attending see you next week take care take care
