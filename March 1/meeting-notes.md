## Meeting Recording

[YouTube link](https://youtu.be/bsE1rYXDnck)

## Mastodon thread

[link](https://neuromatch.social/@OREL/114090985682763664)

## NOTES
Vidhi -- interested in contributing to RL. MARL.

* attend meets, Jesse (easing back into meeting schedule).

* NYCWiC-WeRobot. MetaScience. EI Workshop --> formal acceptances, keep an eye on deadlines.

* small recording for NYCWiC session? Poster acceptances for WeRobot.


Present Shock --> tech enabling things to come really fast. Notirfications --> attention economy,
collapse of narrative.

* universe of cannon vs. storytelling --> temporal incoherency.

* Edu, pedagogy, knowledge generation (Norbert Weiner).

* heavy philosopher --> 


intelligent Soft Matter --> ActInf Discord. Michael Levin's levels of matter --> passive, active, agential (redescription).

* Friston's Theoretical Neurobiology. NeuroTechX --> VR.

* OthelloGPT --> don't know how to ask. Model about intense reasoning.

* Bender/Marcus --> linguistics people analyzing people analyzing LLM output.

* see artificial cleverness, we're not equipped to reveal limitations.

What would you expect given LLM structure? Emergent misalignment. Existing benchmark work. Intelligent probing.

Difficult to assess. PyMC developer.

* LLM output not just db retrieval.

* is imagination the right word for what is going on here?

* tournament selection (https://en.wikipedia.org/wiki/Tournament_selection).


Rise of reasoning models --> hybrid models --> get squawking box to squawk correctly.

* "Insights of Transformer Models". Leaderboards --> suggest progress every release, is it the progress we need?

* Affordance Learning (contrast with other forms of learning).


LLM --> LLM --> LLM. Works if you know the reasoning landscape better (sue RL techniques).

* frontier LLMs (another marketing term).

* only as good as our benchmarks (in terms of understanding).

* meta-benchmarks --> know it's moving target. "Between Circuits and Chomsky".

* pre-pre-training --> emergent realignment paper --> bad behaviors.


Organoid meeting in Boston --> OI Slack (Celsius). Elevate Bio (based in Boston).

* Curt Jamogal -- "Theories of Everything" podcast.

* Manolis Kelis @ MIT. TFUS society + organoids.


Emery Brown -- anesthesiology. Earl Miller -- Bard Ermentrout group. 

* achieve universal computation. 

* Logan Cross --> agentic theories of mind (ToM).

* see symbols the way someone else saw them. 

* how do we get all this tech to drive us towards positive goals?


PFIL --> functional imaging group (London). BRIC in Plymouth.

* neuromodulation in primate models. Show from behavioral data.

* conscious tech collective --> messaged Adam Safron.


Talk: strange loops, AI, empathy. Get at Hofstadter's point --> symbols pushing symbols. 

* feed LLM back into itself. process symbols from itself (strange loop).

* strange loop would be conscious (Godel) --> tension between Wolfram (Irreducibility) and 
Hassibis (approximate reality).

* "Unreasonable Effectiveness of Mathematics" --> Eugene Wigner.


Huggingface, LeRobot (sub-500 dollar robot + support).

* ALOHA --> Stanford robotics (bimanual manipulation). What vocabulary do Roboticists use?

* past researchers are thinking about affordances --> but did not use the language.

* "Secret History of Silicon Valley". 


Pile (serial) and Packed (clutter) grasping dataset: Jiang et.al 2021. 


Jes (OREL)
Jes (OREL) says:
gsoc channel will be particularly of note 
9:29

Jes (OREL) says:
https://bsky.app/profile/dcameron.bsky.social/post/3lizlp62o5224
 related to this ? 
9:51

Jes (OREL) says:
'empathy ai' but this is probably different 
9:52

Jes (OREL) says:
boston shout out 
10:04

Jes (OREL) says:
would be curious to join that slack if possible , bio intelligenc one? 
10:04

Jes (OREL) says:
please share these papers in the slack yeah 
10:28

Vidhi Rohira
Vidhi Rohira says:
yeah would be really helpful to go through these again 
10:29

Vidhi Rohira says:
My slack has only 3 channels "general", "gsoc-open-source" and "random"...how can i join the remaining channels as well. 
10:52

Vidhi Rohira says:
okay got it 
10:52

Vidhi Rohira says:
thank youðŸ˜ƒ 
10:52

Jes (OREL) says:
A VERY obscure reference " A very crude water color painting, of a man in a bishops. hat"  - Bishop Brennan of Father Ted 
10:58

Jes (OREL) says:
https://www.linkedin.com/in/janet-johnson/
 Someone who may be relevant to affordances and also technological spatial processing of affordances  - remebered her. she also co -designed my human centered AI course. 
11:01

Vidhi Rohira
Vidhi Rohira says:
I actually have to head out as I have some university work to take care of. However, itâ€™s been great joining this meetâ€”Iâ€™ve learned a lot! Looking forward to seeing you all next Saturday as well.

Also, any GSoC guidelines you can share for Project Idea 5?

Should i go ahead and introduce myself on both neurostars and the general channel in slack? 
11:05

Jes (OREL)
Jes (OREL) says:
Yes, to introductions 
11:17

Jes (OREL) says:
would be interested in those papers, too 

## TRANSCRIPT
     
0:00          
hello Hi how are you good morning good morning so how are you          
0:06          
today it is it is still breakfast yeah all right good good          
0:14          
yeah so I got a number of things today uh to talk          
0:19          
about uh once again we had our Diva War meeting uh last week and uh mul went          
0:25          
over a lot of the stuff he did with ra neural network Works uh last year and          
0:32          
he's also been working on it since so he gave some updates on that um so that was          
0:40          
good and we still haven't had any other meetings um we still haven't uh picked          
0:47          
up our cybernetics meeetings yet but we'll be doing that when we can so          
0:53          
wanted to start by mentioning that the Google summer of code projects for 2025 are up on neurostars          
1:01          
uh we have project four which is the openworm dorm D graph project so this is we're sponsored          
1:09          
by openworm and our dorm group and this is the graph neural networks project for          
1:17          
biology um this is uh an advanced skill so you know they have the sort of          
1:22          
outline now where you have the skill level required skills of description so it's advanced          
1:30          
uh you know we have a repository which is linked down here but you also need to have pytorch          
1:38          
tensor uh flow workflow experience you need to be able to work with data sets such as segmenting video          
1:45          
and generating graph visualizations uh you need to have the ability to build web interfaces UI          
1:52          
design basic knowledge of biology open- Source practices and applied          
1:57          
mathematical tools so these are all kind of the things that you know that's not like the extent of the skills but it's a          
2:05          
pretty technical area doing a lot of machine learning graph Theory and          
2:10          
working with biological data uh it's a full full-time commitment          
2:15          
so 350 hours which means it's 40 hours a week for the duration of the          
2:21          
summer and then you know this is the description here where we have this          
2:27          
repository D graph which is uh been we've been working on          
2:33          
this for a couple of years uh a good number of participants          
2:38          
in this repository uh we've had gauw contributors mentors and additional          
2:44          
contributors probably numbering about seven or eight um and so we've been          
2:51          
working on this for a couple years it's also integrated with our D learn platform which is a uh a machine          
2:58          
learning deep learning platform for segmenting images and extracting you know          
3:05          
segmenting cells out of other types of data sets so this is what we uh we've          
3:12          
been building this for a lot longer but uh that integrates into the          
3:19          
Evo graph repository which is here and you know you can take a number          
3:25          
of directions on this we have the hyper Devo graph which was what the whole          
3:30          
worked on last year we also have some topological data analysis and          
3:36          
topological graph analysis work that Pocky worked done last year and so there          
3:43          
are a number of ways you can go with this project the second one is the open          
3:48          
source Community sustainability that is also an advanced skill level uh and the skills required          
3:55          
here you know we just really need people who can develop and integrate multiple development          
4:03          
environments uh for this year probably some working knowledge of large language          
4:09          
models and you know some knowledge maybe of agent base models or some          
4:15          
reinforcement learning so if you have a nice robust skill set and well-rounded          
4:20          
skill set then you'll be all right uh this is 350 hours as          
4:26          
well and you know we've been working on this for a while uh we worked on these two last          
4:33          
year uh uh the LL MC which is the larger          
4:40          
language model um and open- Source sustainability          
4:45          
um hybrid model which is where we had large language models as          
4:51          
agents that were living on an agent based modeling grid and they were          
4:56          
solving tasks and creating projects and doing that sort of thing we also had          
5:04          
multi-agent reinforcement learning last year we've been doing multi-agent reinforcement learning in the space for          
5:09          
a couple of years now so it's been developing and uh so that there a number          
5:15          
of directions you can take with this project as well so that's an update on          
5:20          
the Google summer of code stuff okay now I'd like to talk about a          
5:25          
paper this paper is intelligent soft matter for embodied intelligence          
5:31          
basically this paper is and the abstract reads intelligence oft matter stands at          
5:37          
the intersection of Material Science physics and cognitive science promising          
5:42          
to change how we design and interact with materials this transformative field seeks to create materials that possess          
5:49          
like like lifelike capabilities such as perception learning memory and adaptive          
5:56          
Behavior unlike traditional materials which which typically performs static or predefined functions intelligent soft          
6:03          
matter dynamically interacts with its environment it integrates multiple sensory inputs retains experiences and          
6:11          
makes decisions to optimize its responses inspired by biological systems          
6:17          
these materials intend to Leverage The inherent properties of soft matter those          
6:23          
are flexibility self- evolving or self-evolution and responsiveness to          
6:29          
perform functions that mimic cognitive processes by synthesizing current research Trends and projecting their          
6:36          
evolution we present a forward-looking perspective on how intelligent soft matter could be constructed with the aim          
6:44          
of inspiring Innovations and Fields such as biomedical devices adaptive Robotics          
6:49          
and Beyond we highlight new Pathways for integrating design of sensing memory and          
6:55          
action with internal low power operations and discuss challenges for practical implementation of materials          
7:02          
with intelligent behavior these approaches outline a path towards robust vers un scalable materials that could          
7:10          
potentially act compute and quote unquote think by their inherent intrinsic material Behavior Beyond          
7:17          
traditional Smart Technologies relying on external control so this is you know          
7:23          
you may have heard of like smart materials and things like that and it's kind of moving in that Direction and          
7:30          
that's what they're getting at in this paper so let's go down uh this is a new field that's          
7:36          
emerging called intelligent soft matter it aims to develop materials with Advanced          
7:42          
capabilities such as perception memory learning and adaptive responses teachers          
7:47          
traditionally attributed only to living organisms so this is uh something that          
7:53          
allows us to have a material that autonomously interacts and adapts to its environment          
8:00          
through self- learning and adaptive mechanisms using stochasticity of a large number of units so the material          
8:07          
has a number of units and they would be able to you know behave basically like          
8:13          
biological units if that makes sense the application domains are things          
8:20          
like nanoscale robotics uh different types of adaptive          
8:25          
systems that optimize performance in real time materials capable of not only sensing          
8:32          
but also altering their surroundings surroundings and neuromorphic platforms from Material cognition and          
8:39          
thermodynamic Computing so this is kind of the application domains for this          
8:47          
while the initial context of ism is rooted in the quest to mimic biological system complexity and efficiency such as          
8:54          
neurons and they talk about neurons here in 10 through 13 it may go well beyond          
9:00          
Mir biomatics the ultimate Vision encompasses the creation of soft matter          
9:05          
agents possessing fully synthetic intelligence capable of functionalities          
9:10          
that may not even have biological analogs so similar to macros skill Soft          
9:16          
Robotics soft matter transitions from passive adaptability towards exhibiting          
9:21          
sophisticated levels of material intelligence characterized by emerging agency and evolving functionalities resp          
9:29          
responding to environmental cues so the idea is that you have these cognitive like          
9:34          
behaviors that emerge through these macroscopic demonstrations so if you're          
9:40          
familiar with shape changing robots or self-healing materials or at least those Concepts that's what they're getting at          
9:47          
so you have all these small units that configure themselves in different ways where you have uh robots that change          
9:54          
shape or materials that can heal themselves they have a lot of things on applications specific examples so the          
10:02          
specific examples are things like active liquid crystals um networks of responsive nanop          
10:11          
particles self assembling polymers and dynamical Lal assemblies such as          
10:17          
magnetic droplets okay so this is their one of          
10:24          
their diagrams here in the paper          
10:31          
okay so this is uh main capabilities of intelligent soft matter for soft matter          
10:37          
system to exhibit material intelligence it must possess a minimal level of structural complexity shown in figure          
10:44          
one so in this figure we have decision making          
10:49          
actuation perception and sensing and memory and learning so decision making          
10:55          
is the process of making autonomous decisions based on inputs          
11:00          
actuation is the capability to perform actions based on internal stimuli perception and sensing is the          
11:07          
ability to sense and respond to environmental stimuli and memory and learning is the          
11:13          
capacity to retain information and adapt over time so those are the four          
11:19          
capabilities and intelligence oft matter so when they talk about perception and          
11:25          
sensing this is where the matter as a whole          
11:30          
and each individual unit must sense its environment interpret stimula and prioritize actions          
11:36          
accordingly so this includes some sort of Global Response integration where you need to process          
11:43          
multiple types of sensory input such as pressure temperature and chemical          
11:48          
gradients and then synthesize these into a coherent response so for example soft robots that          
11:55          
utilize continuous fluid flow for integrated control process multiple          
12:01          
sensory inputs and give an appropriate response and they call these flow Bots          
12:07          
because they can achieve robust control without moving parts and exhibit This Global Response          
12:15          
integration you also need some sort of selective attention mechanism so this is where the material          
12:21          
selectively responds to certain stimuli while filtering out IR relevant signals          
12:27          
then prioritizing those Reg signals over noise focusing on significant changes in          
12:33          
the environment so materials that can learn mechanical behaviors by tuning their          
12:39          
stiffness uh is what we want and this is similar to how artificial neuron suggust          
12:47          
weights so the ability to learn multiple behaviors simultaneously suggests a form of          
12:53          
selective attention and where the material prioritizes relevant stimuli          
12:59          
to provide an Adaptive response third is dynamic sensory          
13:05          
actuator coupling and so this is where you have this Dynamic coupling between sensory          
13:11          
and actuator components via feedback groups for example a gel-- like material          
13:17          
exhibit significant stiffness changes in response to Thermal stimula showcasing          
13:22          
how Dynamic coupling can enable adaptive behavior in soft materials          
13:29          
this allows for continuous monitoring of environmental interactions and iterative adjustment of actuation allowing the          
13:37          
material to dynamically modulate its response based on observed          
13:45          
outcomes so number one is perception and sensing then we have memory and learning          
13:51          
which is this section for soft matter to exhibit cognitive functions Beyond reactive          
13:57          
behaviors it must possess ays physical memory and be capable of memory and          
14:02          
learning this allows the material to retain information about the past and          
14:08          
adapt to Future responses so we need some sort of memory encoding and          
14:14          
recall we need predictive modeling through self-regulation we need self- reparent          
14:21          
learning and we need adaptive pattern recognition so this is where I know          
14:27          
these are things that of course we know from uh a lot of the AI stuff we've          
14:33          
talked about and some of the biological systems um memory encoding and recall is          
14:40          
particularly interesting because Mater we already have materials that do that but this idea that we can have a more          
14:47          
sophisticated form of that mechanism is intriguing so materials          
14:53          
could employ phase change elements or deformation based memory systems toore and recall specific ific states          
15:00          
hydrogels can retain and forget information based on thermal stimulation showcasing how soft materials can encode          
15:08          
and retrieve information based based on past interactions a thermomechanical local          
15:13          
probe technique has achieved ultra high storage density and thin polymer films          
15:19          
allowing data storage retrieval and Erasure um you also have memory and          
15:26          
entropy generation and non- equilib polymers where polymer chain confirmations stochasticity and          
15:33          
fluctuations retain historical information and impacts their current behavior so this is all you know sort of          
15:41          
stuff that we have now but we're just improving upon uh predictive models are important          
15:47          
as well this allows for your material to adjust preemptively maintain stability          
15:53          
and resilience there are these self repair mechanisms that allow material to mend          
16:00          
itself and develop resistance to recurring stressors effectively learning from repeated          
16:06          
exposure and then adaptive pattern recognition is where the soft matter can          
16:12          
dynamically adjust Its Behavior or structure to align with predictable environmental          
16:18          
Cycles then we have actuation of course because we're dealing with uh materials          
16:24          
that might lead to robotic systems um and so this approach enables          
16:31          
adaptive and self-regulating behaviors that fundamentally differ from existing          
16:36          
implementations so the first of these is soft robotic actuation current soft robotic actuators          
16:43          
while offering unprecedented flexibility and adaptability in shape movement often          
16:50          
require complex external control systems and high power inputs future          
16:55          
directions in ISM actuation should minimize dependence and such external          
17:01          
controllers the emphasis should not just be in designing better actuators that perform motion under stimuli          
17:08          
pre-programed patterns but in designs that have inherent self-actuating properties viop physical and chemical          
17:14          
components of the material structure then there are these programmable morphological          
17:22          
Transformations uh these are where you have these uh today we have these single mode          
17:29          
mechanical deformations and responses for basic motions and linear actuator          
17:34          
displacements or volume expansion contraction and or some sort of          
17:40          
stiffness which but of course in this case what they're advocating for is something          
17:46          
greater which is a greater repertoire of dynamic morphological Transformations so being able to sort of          
17:55          
couple these things or respond to multiple modalities of input          
18:00          
further research directions must therefore seek to engineer materials capable of more sophisticated and          
18:06          
controlled shape changes with a wider range of accessible kinematic behaviors          
18:11          
that are tunable according to desired complex functional actions by external          
18:17          
interventions so uh we want to be able to uh make          
18:23          
better use of reversible material transitions which is what we observe in liquid crystals          
18:29          
with electrical Optical control or magnetically actuated polymeric systems          
18:35          
where the material can deform into multiple shapes in response to external stimuli rather than pre-programmed          
18:42          
single input output responses for each input signal          
18:49          
okay and we have self-organizing communication Pathways and integration          
18:54          
into networks all right so that's now we have          
18:59          
this uh idea of self-organization and how we might          
19:04          
characterize material intelligence and so they have these four principles of intelligent soft matter so these are          
19:12          
self-referential evaluation self-organization active inference and          
19:19          
emergent activity so self- referential evaluation is where you can make an internal in internal assessment and have          
19:27          
proactive decision making so the units and have come to decisions on their own instead of having someone on the outside          
19:34          
program it self organization is where you get spontaneous structure          
19:40          
formation from units interacting instead of based on external templates active inference is where you          
19:47          
can adapt to minimizing prediction error and surprise and then emergent agency is          
19:53          
where you have collective decision making and go directed Behavior          
20:00          
and so they kind of lay these out here and then of course there are these challenges and opportunities of          
20:07          
design and so the challenges of design and intelligence soft matter are          
20:13          
durability and stability so being able to regenerate the material when it's          
20:20          
damaged or or destroyed or have some sort of control          
20:25          
degradation in cases maybe where you use it as a bioscaffold where you wanted to dissolve          
20:31          
at a certain rate but you also want it to be able to be this sort of intelligent          
20:38          
material um complex behaviors and cognitive          
20:43          
functions we have SIMPLE mechanisms and higher order functions so this is these are the things we talked about with          
20:49          
memory attention these other types of adaptive behaviors these emergent          
20:56          
behaviors with some with simple mechanisms some with higher order          
21:01          
functions material complexity where we have stochasticity and experimental experimental paradigms so stochasticity          
21:09          
is where you get this sort of random behavior and if all the units have this          
21:16          
sort of stochastic behavior um it can make it hard to          
21:21          
control but it also can provide the opportunity for emergence or other complex behaviors          
21:29          
integration complexity is interoperability material Integrity          
21:34          
basically being able to Plug and Play this material into any system you want          
21:39          
scalability is where you have fabrication methods and functional Integrity so you know being able to          
21:46          
build larger structures from the basic design of these different          
21:52          
materials and then energy management of course where you can manage the energy          
21:57          
usage of the Mater material or manage it within a system so you have autonomous          
22:03          
functioning and Energy Information          
22:16          
tradeoffs all right so then this is uh let's          
22:23          
see okay so yeah they have a number of references here uh they don't have the          
22:29          
titles of the papers but they have the references down          
22:35          
here so this paper is going to be a perspective and soft matter          
22:43          
Journal okay all right so that's a tour of that paper um I guess it's not quite in          
22:53          
a in an issue yet but you know it's interesting          
22:59          
um yeah so it looks like we have two new people here uh VY rira and Jess or I          
23:07          
mean just old person old hand here but he's back so what uh vid would you like to          
23:16          
introduce          
23:21          
yourself is um I'm second year student in computer science so this the first          
23:29          
meet that I'm attending forc I've actually seen this project          
23:34          
like idea number five sometime back and I worked on reinforcement learning          
23:40          
previously so I was interested in contributing          
23:50          
okay well thank you yeah that's something that we just actually talked about before you came into the meeting          
23:58          
so we have uh our descriptions of the Google summer of code projects          
24:04          
on uh neurostars which is in cf's platform for advertising projects and we          
24:13          
have a repository in our GitHub um our orl group GitHub and so we          
24:19          
have the uh Google summer of code repository there and we have the multi-agent          
24:27          
reinforcement learning directory which is I don't know I don't know if it's an          
24:32          
actual Standalone repository but we've been working in multi-agent reinforcement learning for several years          
24:40          
and yeah welcome abore you know we have our slack if you're in our slack um please you know ask questions          
24:49          
there yeah I join this LA but I don't think          
24:57          
anyone has m anything like there are two channels General and random but I think          
25:04          
General is more about where people are sh something [Music]          
25:10          
butand yeah uh General you join the general Channel and just introduce          
25:16          
yourself I'll introduce myself in the general one okay yeah that sounds          
25:21          
good but uh nobody has done that like before me so is it all right          
25:29          
oh yeah yeah I don't          
25:37          
know I mean I I yeah I haven't checked it in like a day or so          
25:45          
so yeah a lot a lot of the activity is in other channels yeah we usually use          
25:51          
the random chair Channel sparingly um so it's you know we just          
25:57          
kind of use it for like up like advertising meeting the weekly meeting things like          
26:09          
that so if I want to contribute to this project this year like in          
26:15          
2025 how should I go about like there are three prequisites as far as I'm aware the first one was to attend the          
26:23          
meets and the other two are just to understand the Cod base right yeah yeah          
26:29          
so we have a couple of iterations of reinforcement learning and they're on          
26:35          
the GitHub repository um and yeah I mean un attending the meets is useful for seeing          
26:42          
what's going on in the lab we also have open source meetings which will happen          
26:47          
closer to the Summer where we'll kind of have topics and open source talk about          
26:54          
some of the projects and so on and so forth but if if you have questions about what's in          
27:00          
the repositories or questions about what we do with reinforcement learning uh please          
27:06          
get in touch through the slack um just just say like that you're what you're interested in and you know I          
27:14          
can I or someone else can point you in the right direction so we don't I mean we you know          
27:20          
we have I guess a paper on it but it's not uh it doesn't Focus just on          
27:25          
reinforcement          
27:33          
yeah all right well thank you um Jesse welcome back I know you had taken          
27:40          
a Hiatus yeah um hello everybody I am I'm          
27:45          
just stopping in today and kind of easing into um more normal functioning again but          
27:53          
it's good to be here and I I didn't get to see what was discussed earlier in meeting but          
28:00          
um yeah once you know in          
28:05          
in normal terms or regular terms there's there's things coming up there's          
28:12          
um the Nick quick is is happening soon there's a panel for that uh I was we          
28:19          
were accepted to we robot stuff which I'm just getting time to really flush out one now I think medic science stuff          
28:27          
has happened too um embodied intelligence is coming up soon and I think it's going to be          
28:32          
basically like last year where you just sort of register and have a talk if you want          
28:39          
um there's a bunch of other con um uh conferences and events that are          
28:48          
sprouting uh as well and I am slowly reorganizing myself to focus          
28:56          
on on different part them at different times um I'm still not really going to          
29:03          
be at 100% until a few more weeks when I'm I I want to finish my my course work          
29:09          
I'm in my last part of my program so          
29:15          
um I won't be doing quite as much but          
29:23          
um there'll be a lot of things to talk about and even even in terms of          
29:28          
there's things that have kind of come up over the last I don't know four or five months even that I would like to talk more          
29:36          
about here things in my course that have been really interesting          
29:41          
um and kind of dub tailing some projects here but between that and and and Jo Pro          
29:47          
and some other stuff um you know things are kind of rolling along but um I don't          
29:55          
have any major updates right now outside of mentioning those things but if there's anything          
30:03          
um like you might have questions about Nick or something like that anything you want me to try to speak to I can do that          
30:09          
but otherwise it's kind of just a soft restart right now back          
30:16          
here oh yeah thank you for um giving us that update and yeah I I think the          
30:24          
conferences we have to keep on top of those um yeah I haven't heard much about          
30:29          
Nick Wick and we robot yet um that's where I've been hands off but meta          
30:34          
science I submitted a metascience abstract a couple weeks ago uh for the          
30:40          
group we talked about it in the slack and that is on uh uh project Ops          
30:48          
research Ops something like that I can't remember the title um at the moment but          
30:54          
it's basically kind of going over that in the context of reproducibility of          
31:00          
metascience uh and then uh the EI Workshop I submitted an abstract for          
31:05          
that that's something maybe that other people might be interested in submitting an abstract to so you can uh you can          
31:12          
submit your own abstract if you register um the thing I submitted was          
31:19          
basically a continuation of our work on the developmental neural simulation stuff so          
31:26          
it's just kind of continuing down that road and um I yeah haven't gotten the I          
31:33          
have some slides on it but it's still pretty formative so yeah we'll have that we          
31:40          
just want to keep our eye on like the I guess all of those have been submitted          
31:45          
I'm not sure if the metascience one has been accepted I don't know when the acceptances are sent out for that but um          
31:52          
EI Workshop of course is just accepted and then I guess we robot we have a          
31:58          
formal acceptance for nck Wick we have a formal acceptance for yeah um it's interesting because          
32:07          
Nick Wick kind of became I don't even know if I got an acceptance for it but it just became          
32:13          
it's almost like it sort of just became a I mean it's happening but it kind of          
32:21          
became absorbed by like the main program like it's like a program thing not like          
32:26          
that I that I submit it is sort of like I don't want to say irrelevant but          
32:32          
the latest on that is basically they're trying to Center as much as possible like I'm curious how to incorporate you          
32:37          
and Cally Morgan if you want to or the people here in the lab uh because what they're trying to do and rightfully so          
32:44          
is Center people that can be there in person and also especially um          
32:50          
like uh um like I don't know I don't really I          
32:56          
don't really know if I want want to say like early career but like people who are early career and can make it and          
33:04          
attend in person um because I was originally wasn't sure if it's going to be the big virtual thing or          
33:10          
not so I've actually reached out to two people that have worked with the LA          
33:15          
before that may be able to do it one of them is um Min and she did a very um she          
33:22          
contributed to one of our like group inqu things a few years ago and also          
33:28          
Sam Samantha carollo okay um who actually works with GE now and is I          
33:34          
think somewhere in New York state and she's interested in coming to be on the panel and she's done a lot she's done          
33:39          
stuff with us she's she works in some I think an innovation space for G to so          
33:45          
hopefully those two will be able to be on the panel in person and um maybe also Jen but we'll          
33:54          
see um from from other things here in the lab uh but those are kind of like them plus          
34:01          
maybe another person might form like the core of the imperson panel but in terms          
34:06          
of other things like I'm not like I wonder I'm trying to figure out that if          
34:12          
they would be like you know [Music] um maybe kind of like          
34:21          
a if you could depending how much anybody wants to be involved I'm I I          
34:27          
kind of included you're brly in it um by default and you don't you don't have to necessarily but if you wanted to          
34:33          
contribute to the panel it could even be like a small recording and then maybe a          
34:38          
potential virtual like question and answer session as part of the panel too I you don't have you won't have to like          
34:44          
be you know like the main speaker at the panel or anything like that right um but          
34:50          
kind of fit it like I'm not entirely sure yet um what          
34:58          
uh like I don't I don't know the room it's going to be in yet and what that room can do technically because we're at          
35:05          
a different location this year which is both a good thing and has some some challenges we usually done it in one one          
35:11          
town more much more you know more Upstate and now we're kind of          
35:16          
going to be in the middle of between New York City and and Upstate between New York City and Alan whatever yeah and          
35:24          
um so I don't I don't know if that room is going to be like          
35:30          
internet capable to hold a functional like you know jitsy call or something          
35:35          
like that um but at the very least depending on how much you wanted you know I don't know what what          
35:43          
your um capacity or bandwidth for for some of          
35:48          
it is going to be so that that's kind of what's on my mind about it so far but otherwise I'm going to try to be there          
35:54          
and moderate a panel between me and then and hopefully Sam maybe a few other          
36:01          
people in person and then um I've been you know I've mentioned their sort of um          
36:08          
like you and and maybe some other people might want to have a virtual component and they're okay with that so far but we          
36:14          
don't we're kind of figuring out logistical details for that          
36:21          
okay um we robot is a poster acceptance and          
36:29          
um I would really love to go there in person I may have some          
36:37          
travel block blockages or obstacles that may prevent it but I really want to I really really          
36:43          
really really want to do it um but we'll see I can where is it being          
36:50          
held it's in essentially outside of um just outside of Detroit into Canada          
36:57          
at Windsor College okay of law okay yeah          
37:03          
so Bas it's in Canada yeah proper um          
37:09          
but um yeah that that's where it will          
37:15          
be and uh I don't think there's a virtual component to it they usually don't do a virtual like thing but um the          
37:24          
theme the loose theme for that that poster is          
37:30          
um um well          
37:37          
what one of one of the I kind of wanted to round out the poster with a set of          
37:42          
case studies about essentially um I think I think the larger the larger          
37:49          
scale theme for the poster is essentially around um I don't know the easiest way to say it is is present          
37:55          
shock uh which is a that's kind of a spiritual successor to Future Shock and you guys you guys          
38:02          
probably know but just to say present shock Future Shock sort of this idea from the 70s about you know technolog is          
38:08          
arriving technolog is enabling things to come very very very fast and it's not          
38:13          
sort of not really being fully processed not really being fully integrated in the stuff and so sort of this the future is          
38:19          
arriving faster and faster and faster and present shock is a book from a couple years ago that's saying now it's          
38:26          
not even Future Shock it's that we almost have this collapse of          
38:35          
um the you know um it's not present          
38:42          
shock isn't isn't spoken about from a a cognitive science perspective per se          
38:47          
although it does address it and it it kind of gets that the imense pressures that we have          
38:53          
on notifications on like the attention economy basically um and also um kind of like it deals          
39:01          
with things like the collapse of narrativity and and how even like TV shows sort of and and modes of          
39:09          
Storytelling have changed in in this new environment where it's less about certain classical forms of development          
39:16          
of narrative and more about um understanding the universe that the          
39:23          
fictional universe that it's in and and not really about development of you          
39:30          
know certain certain conventional things but it's overall the present shock with          
39:35          
the sense of being you know um I think one of the you know prevalent one one of          
39:40          
the key number of key terms one was like I think Digi G frenia or something like that or some I don't forget the proper          
39:46          
term but basically um the sense of self being split across          
39:53          
multiple areas of focus it's like being spread too thin um because there's so          
40:01          
much immediate tension to be look at this look at the screen answer your notifications you got an email you got a          
40:07          
phone call uh there's Al you know all this stuff happening at once and the the          
40:12          
the spreading toin of that and the sort of temporal incoherency of it          
40:18          
perhaps um and that fits interestingly with all the stuff happening in the world today too for sure but so I          
40:24          
mentioned this in the present shock sense and what I was kind of curious about and and what inspired the poster          
40:30          
is a bit of like commentary on it what's happening and then also um I'm not          
40:37          
really trying to offer like Solutions about it per se but I'm I'm have a number of projects I'm kind of          
40:43          
developing in the space and I probably will talk about them more ahead but even          
40:49          
using like one of the things I wanted to do kind of in reference to like what's being done about it is          
40:56          
like I was exciting one of the case studies I wanted to mention was basically this lab and maybe also plot          
41:03          
Twisters as like sort of inherently narrativity focused but this lab even in an academic sense of like was one of the          
41:09          
few Labs I've been around where we pretty openly talked about philosophy and history science like we talk about          
41:17          
how things change over time we talk like pretty directly about like developmental psychology and developmental biology          
41:23          
right but also just like that it's sort of baked into some of what we talked about and and how there's a space for it          
41:32          
that's sort of one of the things that I wanted to mention in in in orud to in the poster um and and I think that's you          
41:40          
know um in terms of education and pedagogy and knowledge generation and          
41:46          
and exposing people to um dealing with things in a way where          
41:52          
there's a there's a narrativity and a sense of trajectory and development as opposed to just this massive collapse um          
42:00          
is is uh I think an interesting um interesting set of things          
42:05          
to look at so that that's a brief overview of it and there'll be          
42:11          
more ahead on that too that sounds good yeah I look forward          
42:16          
to seeing that um sounds interesting we talk we talked about future shock in          
42:23          
cognition features at some point you know we had an ark on that yeah and and I mean like          
42:32          
there's there's there's I don't know I don't even know if I'm going to go there fully with what I'm trying to say to          
42:37          
that audience but like also the Norbert weiner stuff um and and a lot of a lot          
42:42          
of the I mean you could say nor weer is like like almost like the precursor like          
42:50          
at least the human use of human beings is kind of a precursor to some of the stuff like like like just his his          
42:56          
perspective on on that and the whole like you know um I don't know a lot of the core          
43:04          
things in like Society ethics technology straight up fit fit that um so we could          
43:09          
we could go like heavy philosophy which maybe would be good for that audience um or or or not and I'm not          
43:17          
quite sure yet but there's a lot of there a lot of a lot of interesting threads here so it's nice to kind of be          
43:23          
back and talk about some of them again Yeah well yeah welcome          
43:30          
back um yeah thank you um Morgan I don't know if you had          
43:37          
anything to say about the previous section          
43:43          
um did you have anything or any          
43:49          
updates the um well as you can imagine the intelligence s matter paper was posted          
43:57          
the active inference Discord oh yeah um so I guess um is          
44:08          
it um I think he might be a he might be an active inference fellow uh yeah I think          
44:16          
so okay um and of course yeah just          
44:24          
just reminds me of um          
44:29          
Michael 11's levels          
44:34          
of levels of matter yeah um I'm trying          
44:41          
to remember you know I feel like I feel like this is the          
44:50          
um I forget didn't Michael have four levels like passive          
44:58          
active or did he have three passive active          
45:03          
and um and then certainly agential right          
45:11          
right but I can't remember now if he had          
45:17          
a a fourth in there but um you know this          
45:22          
it it certainly sounds like an agential matter kind of          
45:30          
redescription and um          
45:38          
uh great to see the GX um project          
45:44          
posted um this week let's see          
45:49          
[Music] the          
45:55          
um pre previous week been a bit more active inference um I don't think there          
46:00          
were actually any um friston's theoretical neurobiology group          
46:07          
didn't meet this week um          
46:13          
and uh let's see good meetings          
46:19          
um yeah just a lot of neur attack this week          
46:25          
um uh and I think yeah I think there's maybe          
46:30          
something something relevant there in virtual reality Nur Tech um but          
46:41          
um no I I I can't can't think of          
46:47          
any you know the it'll it'll come come to me          
46:53          
later I I might need to look through the through the slacks to see um SL channels to see what          
47:03          
um what might be good to talk about this week or you know things that have come          
47:11          
up right yeah I think that's just yeah you          
47:18          
want to look through there and well it was a good um sorry on          
47:23          
Tuesday it was a good meeting um so conscious Tech Collective is a group          
47:29          
that meets here and in a different um kind of Cooperative          
47:36          
space [Music] and it reminded me I I I messaged Adam          
47:42          
saffron about it just because it it was Tim Hansen speaking um          
47:48          
who was one of the neuralink co-founders and but he was he was          
47:54          
talking specifically about empathy Ai and it it          
48:03          
was you know I've I've he's joined meetings before so I I you know I think          
48:10          
he's probably the person that I think most overlaps with kind of like um          
48:18          
orthogonal uh you know interests and and you know          
48:26          
is he's very eclectic um so he he          
48:32          
was trying to his talk I'm not going to do it          
48:38          
justice but it was something like strange Loops Ai and          
48:44          
um an empathy something like that and          
48:49          
and I think he was trying to get it          
48:54          
um uh he was trying to get at some points of          
49:02          
hosters um about symbols well like he kept          
49:09          
referring to this this line from I am Strang Loop that like symbols symbols          
49:15          
pushing symbols or something like that and I and I I think he was he was trying to relate it          
49:21          
to you know would you would you when we achieve          
49:30          
point where we can feed a large language model back into          
49:37          
itself and and it can start          
49:43          
to change its programming to alter how it processes          
49:51          
those symbols that it's inputting back onto itself right yeah          
49:57          
that that that would be the hofstead or's strange Lo          
50:05          
okay and and that at that point          
50:12          
um I I think he he was also trying to I mean he he'd be the first to say that          
50:18          
he that this was a very kind of like um still you know not not fully baked          
50:27          
yeah talk right but but that that that          
50:34          
was general intelligence or that that that was the no no sorry that that would          
50:43          
be Consciousness yeah in a in a like like and what he liked about it          
50:52          
and he was definitely trying to ground it originally with with          
50:58          
um the historical context was girdle and and then he wanted to          
51:07          
say that we now live in this this tension between a kind of wol from and          
51:14          
um and I can't I don't really know how to pronounce his name right but it's like hbus what's Denny yeah yeah          
51:23          
hus something anyway de de Minds          
51:28          
founder no Nobel Prize winner right yeah          
51:34          
and um so I I I I liked that or you know          
51:40          
like like again like um what is the tension between what wol          
51:46          
from and well sorry not the not the tension I I I actually had a problem with the he was like you know Wolfram is          
51:54          
always pushing this competitional uh irreducibility and and          
52:01          
Demi's is saying that like hey we can we can basically approximate          
52:06          
reality okay and I was just like no that's that's not the T you know that's          
52:12          
not attention yeah like like wol for you know I was just like hey you can't you          
52:18          
can't Skip chayon and jump to wol and then be like what's wolf talking about          
52:24          
like like there is this compositional irritability right it comes out of some          
52:30          
really grounded work and um and yeah so          
52:36          
algorithmic information Theory stands there          
52:42          
um what          
52:47          
um the other side is that yeah like reality or certainly you know          
52:54          
computational physics certainly certainly is very effective you know I          
53:00          
mean who's who wrote the the effectiveness of          
53:05          
um it's like man I can't reable Effectiveness in          
53:11          
mathematics yes yes okay yes uh I don't remember who wrote that but I I know that yeah it's a classic but yeah it's          
53:19          
is classic right and there is this that that if you will is the is the the other          
53:25          
side right is that like mathematics is remarkably effective but I think it would be the the same          
53:32          
like you know um it's it's not trying that the          
53:38          
computational mathematics in the form that that deep          
53:44          
mind is using to try and solve Fusion problems and things like that is is not          
53:50          
trying to address the issues that girdle and and shapon are trying to address          
53:55          
right yeah um and yeah so I mean there's          
54:01          
if if anything you know you're getting closer to Hoffman in terms of just you know the          
54:10          
the AL algorithms start to push the limit of of what our understanding of          
54:16          
SpaceTime and things like that and and that yeah anyway but the I like the loop          
54:25          
in terms of just coming up with a different coming up with a computational way that you          
54:33          
could you know perhaps redescribe something like Consciousness right where it's this this          
54:39          
symbols pushing symbols like like again like I think he was trying to open up          
54:44          
that large language models could achieve something that was kind of universally          
54:53          
conversational um uh without it becoming you know without it developing Soul or          
55:00          
something like that right and so that that was good and and his          
55:07          
point was um what he was trying to said what he          
55:13          
was trying to link this to that I thought was really interesting with regards to Logan cross um and that group          
55:21          
at Stanford was agentic theories of mind right right so what does it mean for um          
55:31          
an an agent to have a theory of mind and he he was trying to make it          
55:37          
where you were able to          
55:42          
um you know again kind of essentially see see your symbols the way somebody          
55:48          
else saw them um see see these symbols the way somebody else was was was seeing them          
55:55          
which was kind of like being able to run their code run their algorithm um and uh          
56:06          
anyway it's it was his end goal was the same as kind of          
56:11          
Adam aons which was um how do we how do we get all these all this technology to          
56:19          
be driving us towards you know positive um          
56:25          
um uh yeah positive goals and you know          
56:32          
anyway that was that was just a nice it was a nice discussion at um at a level          
56:38          
that didn't yeah still tried somewhat to stay          
56:44          
grounded in in algorithms and yeah mathematics          
56:51          
yeah so that that was that was really nice um and then you know neurotech          
56:59          
meeting this week or the the Phil in London functional Imaging lab in London          
57:04          
had a great presentation on um uh          
57:10          
uh from a woman who really pioneered this kind of transrenal um focused          
57:18          
ultrasound and that that was nice Elsa excuse me I forgot her um I forget          
57:25          
her last last name let see maybe Italian um          
57:35          
uh foran foran okay um anyway she's now the          
57:42          
director of The Brick the brain research Imaging Center in Plymouth and um has          
57:51          
definitely pioneered a lot of these techniques that are now kind of standard          
57:56          
focused ultrasound groups which is um how to how to collect you know kind of          
58:03          
like a an MRI data set very quickly that actually tells you something about          
58:08          
person's skull um and she had some some good some really          
58:15          
interesting uh examples that you know very unusual like          
58:23          
she's biomedical engineer but I see that she was in Matthew rushworth's Lab at          
58:29          
Oxford which is a very like you know it's real decisionmaking lab right so          
58:36          
she she had examples where she's doing kind of          
58:42          
neuromodulation but in the context of these you know kind of well understood          
58:47          
primate models um and yeah so was was more showing the          
58:57          
effects of neurom modulation from behavioral data as opposed to you know          
59:04          
trying to like Stanford's approach of trying to collect this kind of data inside an MRI          
59:12          
where you can actually you know lit literally like visualize the the          
59:18          
displacement caused by the the ultrasound um hers were were much more          
59:24          
like hey I'm gonna I'm gonna Target these subcortical structures that should          
59:30          
cause this Behavior Behavior or decision-making task to change in in          
59:36          
some biasing in some direction um that that that was um that was very          
59:42          
interesting and um sorry there was there was one more          
59:50          
that I thought of but I now I'm blanking on it okay um yeah          
59:57          
it'll it'll it'll come up later okay yeah well that's all good stuff          
1:00:04          
um glad to hear that like you enjoyed the sorry oh I I I I just remembered          
1:00:10          
sorry so um there was an organoid meeting in Boston and um um          
1:00:19          
so apologies because the you know the number of kind of organoids SL          
1:00:26          
is is grown and um so most of the          
1:00:32          
organized stuff I'm I'm doing at Celsius but um we now have an organoid          
1:00:37          
intelligence slack right and this is this is started together with um a a guy          
1:00:45          
now yeah that's right um at um Elevate          
1:00:50          
bio in in um Boston um          
1:00:56          
so but interestingly This was um I I don't know if people follow Kurt jongle          
1:01:04          
um but his theories of everything H podcast he has some he has some great um          
1:01:11          
he has some great guests uh I really really enjoyed the the um quantum          
1:01:18          
physics talks that he had just recently as a moderator when he was in          
1:01:24          
Boston last last week he had manalis um kis and          
1:01:31          
menis runs the comp bio. mit.edu          
1:01:37          
um site you know and course course is          
1:01:42          
online um I've I've tried to watch videos from it um          
1:01:51          
and so Kellis just had a meeting in Boston on organiz and Consciousness and          
1:01:58          
I'll I'll post the video in          
1:02:03          
um brain organiz yeah yeah that would be good but          
1:02:10          
it was it was very it it was exciting because um he joined our Kell joined our          
1:02:17          
slack okay after the meeting so some somebody from interestingly somebody from the TFA          
1:02:24          
Society uh uh was in Boston for the organized meeting and had shared the the slack          
1:02:33          
link so I thought that was cool yeah so you know it's it's I don't think          
1:02:41          
of the T FL you know that is related to the organoid work um but but this week          
1:02:48          
there there was a connection yeah heuss is what the uh trans cranial focused          
1:02:53          
Ultras oh right right right yeah yeah and and although I will say that um          
1:03:00          
they've been linked together before which is in how Open Water uh Open Water          
1:03:07          
Health which is Mary L jeffson company that's that's making these super low          
1:03:13          
cost um Advanced Imaging devices and and folus ultrasound devices um she was the          
1:03:22          
one who's doing uh cancer cell destruction with          
1:03:27          
Focus ultrasound and leaving the healthy tissue intact and she demonstrated that          
1:03:34          
using organoids so it was brain organoids gleo          
1:03:39          
blastoma and then showing how you could lice the um you could lice the uh the          
1:03:48          
cancerous cells and and I think that was a big part of of how she was able to raise so          
1:03:54          
much money in the p last year yeah so super super interesting connection          
1:04:00          
there yeah so yeah thank you for that um there's interesting stuff definitely um          
1:04:08          
a lot of good things going on there um yeah yeah I just I just dropped a link          
1:04:15          
to that uh that talk with um Emory Brown so you know since Amanda's not here I'll          
1:04:23          
say you know emry Brown is the very famous anesthesiologist that works together you          
1:04:31          
know does um a lot of you know what I          
1:04:36          
would call the the the really important work in Consciousness studies yeah uh          
1:04:43          
understanding the physiology and um so he's he's also had a multi-year          
1:04:51          
collaboration together with Earl Miller there um uh          
1:04:56          
which is a shout out to oh to a meeting next week wave Club will be meeting next          
1:05:01          
week March 4th so that's the B man trout          
1:05:07          
group it covers traveling waves in in neural systems          
1:05:13          
um uh yeah so Emy Brown and then Ken Kos          
1:05:18          
who I wasn't familiar with before but um is uh conscious researcher s Barber and          
1:05:27          
um so he had a paper that something like why aren't organoids conscious          
1:05:35          
yet and that was the that's yeah so you can find the video of braids all right          
1:05:43          
yeah all right that's great I did want to talk          
1:05:50          
about so we've been going through this arc on large language models and          
1:05:56          
reasoning um you know we we've been posting a lot in the slack and last few meetings we've been talking about this          
1:06:04          
so I did want to go over some readings that I kind of collected from the slack uh well the first one actually is          
1:06:11          
this article I I think this was posted by Hussein or maybe Morgan this was on          
1:06:18          
the or no actually I think it was zura who posted this um then this is uh          
1:06:23          
Google research it says they Co scientist that they're working on and so          
1:06:29          
I guess um Zora right memory serve Zur uh posted and said have you tried          
1:06:36          
this yet and I don't think there's actually anything to demo but they have some          
1:06:41          
papers so this is uh this this is a blog post that kind of goes over what Google          
1:06:47          
is doing in this area so this is accelerating scientific Breakers with an AI          
1:06:54          
[Music] they here they introduced the AI Co scientist a multi-agent AI system built          
1:07:01          
with Gemini 2.0 is a virtual scientific collaborator to help scientists generate          
1:07:07          
novel hypotheses and research proposals and to accelerate the clock speed of          
1:07:13          
scientific and biomedical discoveries always at the computational um          
1:07:19          
analogy um so they have these they have the AI Co scientist paper the gene          
1:07:26          
transfer Discovery paper and a transfer rediscovery paper so we'll we'll get          
1:07:31          
into those in a minute and this just kind of you know this is kind of semi-          
1:07:36          
marketing uh this blog post but basically you know they're trying to          
1:07:41          
empower scientists and accelerate Discovery and so um given a scientist          
1:07:49          
research goal that has been specified natural language the AI Co scientist is designed to generate novel research          
1:07:56          
hypothesis a detailed research overview and experimental protocols to do so it          
1:08:01          
uses a coalition of specialized agents generation reflection ranking Evolution          
1:08:09          
proximity and meta review so those are the six agents and they're all          
1:08:15          
specialized for these different uh phases of uh I guess research evaluation          
1:08:22          
that are inspired by the scientific method itself these agents use automated feedback to iteratively generate          
1:08:29          
evaluate refine hypotheses resulting in a self- incurving cycle of increasing          
1:08:34          
high quality in novel outputs so this is the GIF here let me see start here with          
1:08:40          
I guess you can stop it okay um this is a graph that says test time          
1:08:47          
compute versus research ideas quality and Novelty so they're going to be plotting that the scientist here has          
1:08:54          
this set of agents at their disposal so I guess the scientist is a supervisor          
1:09:01          
agent or there is a supervisor agent here and then we have the proximity          
1:09:06          
agent The Meta review agent ranking agent the evolution agent the generation agent review agent so you have something          
1:09:13          
that generates a hypothesis something that reviews the hypothesis something that ranks the          
1:09:20          
hypothesis something that involves the hypothesis and then reviews it at a          
1:09:25          
higher level and then they do this tournament selection of research ideas so you have you put a bunch of research          
1:09:32          
ideas in a list and you evaluate them and then you do generate this graph here          
1:09:39          
which is I don't know kind of data this is but it just kind of shows the um you know the increase in quality          
1:09:48          
I guess uh and Novelty so this is uh the design here          
1:09:55          
the cociena with the system by specifying a          
1:10:02          
research goal natural language they can also suggest their own ideas and proposals provide feedback and reviews          
1:10:10          
and interact via chat interface to guide the          
1:10:16          
cociena with the agents there's a input of a research goal so the scientist          
1:10:22          
describes a research goal along with preference es experimental constraints and other          
1:10:28          
attributes and then you have these uh this goes into this multi-agent system          
1:10:34          
so there's a plan configuration there's this ranking agent tournament so it          
1:10:40          
means that you know you have these different rankings of of          
1:10:45          
ideas um so research hypothesis comparison and ranking with scientific          
1:10:50          
debate in tournaments limitations and top win loss patterns are summarized and          
1:10:56          
provided as feedback to other agents this enables iterative Improvement in quality of research hypothesis          
1:11:02          
generation creating a self-improving loop uh so you have the generation agent          
1:11:09          
the reflection agent Evolution agent proximity check agent better review          
1:11:15          
agent so the generation agent do literature exploration simulated          
1:11:20          
scientific debate reflection agent does full review with web search          
1:11:25          
simulation review tournament review deep verification then the evolution agent          
1:11:31          
does inspiration from other ideas simplification research extension then          
1:11:37          
the meta review agent does research overview formulation that all comes to this          
1:11:43          
output which is research proposals and overview top rank research hypotheses and proposals are summarized into a          
1:11:50          
research overview and shared with the scientist so that's the co- scientist design system so they're I don't know if          
1:11:56          
they're they're not trying to be like an AI scientist so much as a sort of an extension of the scientist so you can          
1:12:04          
actually you know put in ideas put in hypotheses and then there's this          
1:12:10          
evaluation by the cociena and overview          
1:12:18          
structure um yeah so then this is kind of some          
1:12:23          
performance in here why don't we go up to these papers here the AI Co scientist paper this is          
1:12:31          
towards an AI Co scientist um this is a technical          
1:12:36          
paper and let look at the          
1:12:42          
abstract uh scientific discovery relies on scientists generating novel          
1:12:47          
hypotheses to augment this process we introduce the AI go scientist a          
1:12:53          
multi-agent system built on Gemini .0 so they're using Gemini 2.0 for this and          
1:12:58          
they're using these different agents within that framework the AI Co scientist is intended to help uncover          
1:13:04          
new original knowledge and to formulate uh demonstrably enable research          
1:13:09          
hypotheses and proposal building upon prior evidence and aligned to scientists          
1:13:15          
provided research objectives and guidance the systems design incorporates a generate debate and involve approach          
1:13:23          
to hypothesis generation so this is where they have the the evolution and          
1:13:29          
the evaluation so you have you generate hypotheses you debate them you rank them          
1:13:35          
you have this tournament and then you evolve them over time so this is inspired by the scientific method and          
1:13:42          
accelerated by scaling test time compute key contributions include one a          
1:13:48          
multi-agent architecture with an asynchronous task execution framework          
1:13:53          
with flexible compute two a tournament Evolution process for          
1:13:59          
self-improving hypothesis generation and then automated evaluation          
1:14:04          
show continued benefit of test time compute improving hypothesis          
1:14:10          
Vol so they actually implement this and validate this in three biomedical areas          
1:14:16          
rug repurposing novel Target Discovery and explaining mechanisms of bacterial          
1:14:22          
Evolution and antimicrobial resistance for drug repurposing the system proposes          
1:14:28          
candidates with promising validation findings including candidates for acute mid leukemia that show tumor inhibition          
1:14:36          
in vitro at clinically applicable concentrations for novel Target          
1:14:42          
Discovery the AI Co scientists propos new epigenetic targets for liver          
1:14:47          
fibrosis validated by antifibrotic activity and liver cell regeneration          
1:14:52          
human hepatic orgs finally the the AI Co scientists recapitulated on published          
1:14:58          
experimental results via parallel in silico discovery of a novel Gene transfer mechanism and bacterial          
1:15:05          
Revolution these results detailed in separate co- Ed reports demonstrate the          
1:15:10          
potential to augment biomedical and scientific discovery in Asher and era of AI empowered scientists so this kind of          
1:15:18          
goes through this this is the AI co- scientist paper and then they did this          
1:15:24          
Gene transfer Discovery paper which is on the bioarchive this is chimic infected part          
1:15:33          
infective particles expand species boundaries inage inducible chromosomal Al mization so this is a very specific          
1:15:40          
paper in um uh I guess it's in looking at mobile          
1:15:47          
genetic elements so this is you know I          
1:15:52          
guess this is applying this methodology to study and then this other run transfer          
1:16:00          
rediscovery which is this paper here AI mirrors experimental science to uncover          
1:16:07          
a novel mechanism of Gene transfer crucial to bacteria          
1:16:12          
Evolution this paper utilizes the platform as well so let's look at this paper the summary AI models have been          
1:16:20          
proposed for hypothesis generation testing their ability to drive high impact research is challenging since an          
1:16:27          
AI generated hypothesis can take decades to validate here we challenge the ability of a recently developed large          
1:16:34          
language model platform AI Co scientist to generate high level hypotheses by          
1:16:40          
posing a question that took years to resolve experimentally but remains unpublished and that is how could capsid          
1:16:47          
forming Fage inducible chromosomal Islands so this is from the last paper          
1:16:52          
uh kind of summarizing the AI contribution here spread across bacterial species so remarkably AI co-          
1:17:01          
scientist top ranked hypotheses matched our experimentally confirmed mechanism          
1:17:06          
that is cfis hijack diverse spage Tales to expand their Host range uh we critically          
1:17:14          
assess its five highest rank hypotheses showing that some open new research Avenues in our laboratory we benchmarked          
1:17:21          
its performance against other large language models now outline best practices for integrating AI into          
1:17:27          
scientific discovery our findings suggest that AI cannot just act as a tool but is a creative engine          
1:17:34          
accelerating Discovery and reshaping how we generating test scientific          
1:17:39          
hypotheses okay so this is a paper on so the second paper was on this topic          
1:17:49          
of Fage inducible chromosomal islands and this paper focuses on the AI contribution of that          
1:17:56          
okay so that's uh something about uh doing          
1:18:04          
cociena scci uh like an AI scientist concept uh there's another paper uh that          
1:18:12          
kind of critiques the whole area of AI SC having an AI scientist I don't have          
1:18:17          
that paper here I don't know if we've covered that in the previous meeting but um and then this is the cociena          
1:18:24          
uh sort of a cognitive support tool this is something I think that uh          
1:18:30          
Morgan posted in the slack so this is the frames of Mind          
1:18:37          
GitHub repository this is animating R1 thought so this is the R1 Larch language model          
1:18:44          
and this is kind of talking about some of the things that they're doing here uh          
1:18:49          
the thought process for R1 so this is um          
1:18:55          
kind of the repository where you can clone the repository and run this um          
1:19:03          
environment and so they want to visualize the thought process of R1 so          
1:19:09          
we can visualize the thought process of R1 by saving the chains of thought as text converting the text to embeddings          
1:19:16          
with the open AI API and then plotting the embedding sequentially with T so          
1:19:21          
they're using the tne method to visualize ize the results of the          
1:19:27          
embeddings so this is where we're taking these change of thought as text converting the text to an embedding and          
1:19:33          
then plotting the embeddings of visualization and so here's what it looks like when R1 answers a question in          
1:19:40          
this case describe how a bicycle works so you have this prompt describe how a bicycle works and then you have so this          
1:19:48          
is going to be I guess in in sequence here okay okay so I need to describe how a Bicycle Works let me start by thinking          
1:19:55          
about the basic components of a bicycle there's the frame wheels pedals chain          
1:20:01          
handlebars brakes and gears how do all these parts work together to make the bike move and so this is where you start          
1:20:09          
with this point here we have consecutive distance it might be useful to get a          
1:20:14          
sense of how big each jump from thought I the thought I + one is then this graph          
1:20:20          
would show the difference between consecutive steps so this is consecutive cosine          
1:20:26          
distances transition number versus normalized cosine distance uh then by default we calculate          
1:20:33          
cosine similarity between the embeddings normalize across the set of all consecutive steps to Zer one so over          
1:20:43          
that interval I'm interested in seeing when the bigger or smaller jumps happen in the thought cycle so then there's          
1:20:49          
this combined plot where you have you know kind of goes through through the          
1:20:55          
problem and then you get these aggregated distances which is the position in percent versus the value so          
1:21:02          
these are this is a normalized sequence analysis you get these individual sequences which are highly variable then          
1:21:08          
you can uh find the mean sequencer you know right here in blue which averages          
1:21:15          
across all of these sequences um and so I don't know how useful the normal the average sequence          
1:21:22          
is but like you know you get this uh smooth version of the thoughts and so          
1:21:30          
the graph above shows the aggregated distances for 10 samples in my eyes it looks like a search phase where the size          
1:21:37          
of the step is large Al by a stable thinking phase followed by a concluding          
1:21:42          
phase so you have the search thinking and concluding I guess this is the          
1:21:47          
search this is the thinking this is the concluding okay so that's that's this          
1:21:54          
reposit here frames of mind and then finally this paper here uh          
1:22:01          
this is from Trends and cognitive science this is a featured review I think this was also in          
1:22:06          
slack um and this is structures not strings Linguistics is part of the          
1:22:12          
cognitive Sciences so this is where they talk about Linguistics and the cognitive          
1:22:20          
sciences and kind of how that you know how that sort of in these kind of          
1:22:25          
generative grammar and how that's fitting together so um this is a little          
1:22:31          
bit different this isn't focused on large language models but it does talk about like how we think about language          
1:22:37          
and cognitive science so uh uh Shamsky is on this          
1:22:45          
and number of other people so there are many questions one          
1:22:52          
can ask about the human language it's dis distinctive properties mural representation characteristic uses          
1:22:59          
including use and communicative contexts variation growth in the individual in          
1:23:04          
origin every such inquiry is Guided by some concept of what language is sharpening the core question what is          
1:23:11          
language and paying close attention to the basic property of the language faculty and its biological foundations          
1:23:18          
makes it clear how Linguistics is firmly positioned within the go of Sciences here we will recent          
1:23:25          
developments in generative grammar taking language as a computational cognitive mechanism seriously allows us          
1:23:31          
to address issues of unexplained in the increasingly popular surface oriented          
1:23:36          
approaches to language so there have this Trends barks here on the right          
1:23:43          
which is of course because it's Trends and cognitive science it's sort of describes the trend here the          
1:23:48          
computations of the Mind rely on the structural organization of phrases but are blind to the linear organ          
1:23:54          
organization of words that are articulated and perceived by input and output systems at the sensory motor          
1:24:02          
interface and this is where we have we generate speech and we produce signs the          
1:24:08          
computational procedure that is universally adopted is computationally much more complex than an alternative          
1:24:15          
that relies on the near order linear order is not available to the systems of syntax and semantics it          
1:24:22          
is an ancillary feature of language probably a reflex of properties of the sensor M system that requires it for          
1:24:30          
externalization and con stream by conditions imposed by sensory motor modalities it follows that language is          
1:24:37          
primarily an instrument for the expression of thought language is neither speech SL sign externalized          
1:24:44          
expression nor communication one of its many possible          
1:24:49          
uses says paper goes through so there's uh uh box one which is the basic          
1:24:56          
property of language talking about how you use          
1:25:01          
merges uh there's also this study of language          
1:25:07          
which focuses on three questions we have this glossery which has a number of terms that we use in the          
1:25:15          
study of language uh let talking about syntax and the          
1:25:22          
syntax of semantics talking about simple rules in          
1:25:28          
language uh showing sort of this idea of negative polarity so this is I think a valuable          
1:25:35          
article for thinking about how people study language and then going back to          
1:25:41          
this frames of Mind repository uh where they kind of go          
1:25:46          
through animating the thoughts of a large language model and then the          
1:25:54          
thinking about human constructs of language and descriptions of things that          
1:25:59          
are more abstract          
1:26:05          
ideas okay let me see what we have in the chat here so Jess made a couple of          
1:26:11          
comments here uh mentioned the gof channel we have a gof channel in our          
1:26:18          
slack uh so going back to that part of the meeting yeah we we have that you should join the gck channel through          
1:26:24          
interested in Google sumar code uh there's this uh Blue Sky post uh          
1:26:33          
and this is the empathy a reference to the empathy AI thing which is where okay          
1:26:40          
darl Cameron says Apparently one of my papers on AI empathy with this person          
1:26:46          
here was put up as an example on a talk at spsp yet it's one in which we use          
1:26:52          
interdisciplinary dialogue between Psych ology and philosophy to take an open-minded Broad View on empathy          
1:26:59          
exploring cost benefits across modes alas I don't really know what the          
1:27:04          
context for that is but um oh maybe this was in uh the society          
1:27:13          
for yeah I I think uh I think he put it up as like was this real you know was          
1:27:19          
this what um uh um Tim was talking about Tim          
1:27:25          
Hansen okay yeah and and and Adam          
1:27:30          
zaffron they both talk about empathy as it relates to um artificial intelligence          
1:27:37          
but okay I'll I'll yeah I'll check that out I was just gonna say that that some          
1:27:44          
of the papers that you're covering you can find in cognition Futures okay Channel yeah          
1:27:51          
um yeah and um          
1:27:58          
or data Sai data ml yeah          
1:28:04          
uh it's perhaps in Dev neuro AI I didn't          
1:28:10          
I haven't Tracked Down um oh yeah yeah yeah          
1:28:19          
well yeah Dev dev has between circuits and          
1:28:24          
jsky free training on formal languages and parts linguistic bi okay we can go          
1:28:30          
back yeah we can go back to those channels in a little bit yeah it's it's          
1:28:36          
but it that is kind of yeah yeah yeah all right there's a lot of channels          
1:28:44          
there is is          
1:28:50          
that I can't oh yeah I can't read them BD yeah yeah          
1:28:55          
sorry got I've got my glasses on I still can't read okay all right uh yeah so this this this          
1:29:03          
post I just talked about was empathy AI um okay and then VDS yeah would          
1:29:10          
really be helpful to go through these again so yeah the papers you mean just          
1:29:15          
having access yeah and they'll be in the they'll be in the YouTube video          
1:29:21          
description yeah okay yeah so well I'll go through the channels again and you          
1:29:26          
know the papers will be in there and we can also yeah we you know we always I I          
1:29:32          
go through the papers kind of quickly and it's always useful to follow up on some of that stuff but yeah okay so um          
1:29:41          
yeah why don't we go through the channels then um and so the first one          
1:29:46          
here we want to go through is cognition Futures this is the paper structures not          
1:29:53          
strings this I I'll repost this because this is just the link to uh yeah you know it          
1:30:00          
doesn't have the paper um okay so that's there data          
1:30:06          
science ml this channel so this is uh we have          
1:30:12          
speech and language processing here this is for web Stanford yeah I just I just          
1:30:18          
dropped in that's their that's the course material for the NLP it's          
1:30:24          
Stanford okay yeah sorry and um and then that's that's a that's a kind of symbols          
1:30:30          
pushing symbols F it's just interesting self-correction um those are actually          
1:30:36          
all UI UC people oh okay oh this paper yeah yeah yeah yeah so this is yeah          
1:30:42          
self-rewarding correction for mathematical reasoning so this is the idea that you're taking the output of a          
1:30:49          
larg language model and you're making it reflect on itself yeah and and you know like like I          
1:30:58          
mean the reason that this is interesting is that previous attempts to do this          
1:31:04          
have sent models into a a downwards bar right          
1:31:11          
right yeah that's what or you know like like yeah yeah like people have talked          
1:31:17          
about that as being like garbage in garbage out where you and if everything is subsumed by a large language model          
1:31:23          
you train it know a large language model and it becomes kind of like this          
1:31:30          
self-referential mess it becomes a mess and certainly          
1:31:35          
doesn't doesn't um you know expand out of its previous          
1:31:41          
capabilities and this is this is an example where they're they're saying          
1:31:46          
that it's it's as good as at least external reward system which is you know          
1:31:53          
which is again impressive but but with with the caveat that of course          
1:31:59          
these mathematical reasoning models are still just a a very          
1:32:04          
limited limited domain where we know the reasoning landscape          
1:32:12          
better right I'm not sure how best best to put          
1:32:17          
that but yeah well it looks like they're using some sort of reinforcement yeah no they they yeah          
1:32:25          
yeah yeah so they're they're using reinforcement learning to like Smooth over that that sort of garbage in          
1:32:31          
garbage out aspect and so you know it's like you can be recursive but you have          
1:32:36          
to have some sort of reward structure and yeah yeah but it's self but it's          
1:32:42          
self-rewarding right self-rewarding you know so that's that's interesting right          
1:32:47          
which which which is which is um you know          
1:32:54          
which we think we are doing ourselves right well yeah we're we're constantly          
1:33:00          
self- refer referring to symbols and sometimes we do this as a way of kind of          
1:33:06          
it's this garbage and garbage out thing like we kind of have these meanings for symbols that aren't          
1:33:12          
really uh you know they're not good meanings or they're not like you know they can be multiple meanings and you          
1:33:18          
know but it's so I mean human cognition you know we might think of as a gold          
1:33:24          
standard but human cognition can produce a lot of sort of self-referential          
1:33:30          
nonsense as well so uh yeah it's it's interesting though how we get into these          
1:33:35          
kind of systems and see how we can model them um yeah that's from so that's from data science          
1:33:42          
ml um then we have ml Gem and that's uh this is of course we we've talked about          
1:33:48          
these in the past these models where they use this training environment to train models          
1:33:54          
so this is meta ML gy and ml gym bench so this is uh the first gym environment          
1:34:01          
for machine learning tasks enabling research and reinforcement learning algorithms for training such agents mlgm          
1:34:09          
bench consists of 13 diverse and open-ended AI research tasks from diverse domains such as computer vision          
1:34:16          
natural language processing reinforcement learning and Game Theory          
1:34:21          
uh solving these tasks requires world world AI research skills such as generating new ideas and          
1:34:28          
hypotheses creating and processing data implementing ml methods training models          
1:34:35          
running experiments analyzing the results and iterating through this process to improve on a given task so          
1:34:43          
they're doing some something similar to sort of a AI scientist here          
1:34:49          
obviously that's not what they call it but it's this sort of like you testing possible ideas about the world and kind          
1:34:56          
of running through those iteratively and and just again to to          
1:35:03          
modals you know kind of Chief scientists um we're we're only as good as our          
1:35:11          
benchmarks right right or you know our our understanding is is certainly          
1:35:16          
limited by our benchmarks and um so yeah this is this          
1:35:22          
is just one of the the lat you know trying to keep up with just even the benchmarks right is is tough          
1:35:32          
um but but yeah like like there should be a well and and there are groups that          
1:35:39          
are just making meta benchmarks you know of just benchmarks of collections of          
1:35:47          
benchmarks yeah uh so they said we evaluate a number number          
1:35:53          
of Frontier lerge language models on our benchmarks such as CLA 3.5 Sonet llama          
1:36:00          
3.1 405b GPT 401 preview and Gemini 1.5          
1:36:06          
Pro and they say Frontier large language models they mean newer and more powerful ones or what do they mean well I mean          
1:36:15          
this this goes back to our what does foundation model mean um uh uh I think          
1:36:22          
you know um it does seem like this is their their          
1:36:30          
their you know we know it's a moving Target okay yeah so this is what what          
1:36:37          
Frontier is is marketing speak for um you know our the latest release from one          
1:36:44          
of the big companies oh okay yeah so just just to keep up with the          
1:36:49          
times I mean you know this paper is what February know February 20th um you know          
1:36:57          
4.5 is released like it's already out of date yes it's like you know barely          
1:37:04          
barely a week out yeah all right so that yeah that's good          
1:37:11          
um so that's in data science ml we have some other interesting things          
1:37:18          
in here as well um and then let's go to uh Dev M          
1:37:24          
AI so this is the paper between circuits and chsky PR pre-training on formal          
1:37:30          
languages of Parts linguistic biases this is um yeah this is kind of a not the paper          
1:37:38          
we're talking about but this is what is this paper so so this is an interesting          
1:37:44          
one where you know again so many of these          
1:37:51          
right you start with kind of like all the text on the internet kind of scrape          
1:37:58          
right um but          
1:38:04          
here everybody's now trying like let me take one of these Foundation          
1:38:10          
models and now F          
1:38:15          
tunings um or um you know add some          
1:38:21          
training in some special way right so there's the emergent misalignment paper          
1:38:27          
where they actually train it on Mal Mal adaptive code right yeah and it starts          
1:38:34          
behaving in all these sick and demented ways starts outputting all this kind          
1:38:41          
of racist bile yeah and and and I I I          
1:38:47          
wanna I want to have that as as as a pair with this which is this is trained          
1:38:54          
on all like on just formal languages so um you know that's that comes more from          
1:39:03          
from like um Linguistics and and computer          
1:39:09          
science um and again          
1:39:14          
somewhat relates back to um          
1:39:21          
uh the de algebra projects where it's just like if we tried to train if we really          
1:39:29          
tried to to make um a          
1:39:34          
math model um in a in a very general sense then          
1:39:42          
training on formal languages would be a part of it you know it's sort of sort of this          
1:39:49          
somewhat related to the models that trying to include um theem solvers yeah          
1:39:56          
right um and anyway but then again you know          
1:40:02          
so this also just reminded me somewhat of the          
1:40:07          
um Emily Bender Marcus meeting that had the the          
1:40:15          
conference that had a bunch of linguists you know Linguistics people analyzing large langu          
1:40:23          
anguage model output right because um in          
1:40:29          
in you know to kind of follow pen rows here right like these quote again          
1:40:37          
Foundation models is kind of using their marketing speak right just big big large          
1:40:42          
language models uh um when we interact with them what we're          
1:40:50          
seeing is really artificial clevers right right it it's it's and and what          
1:40:59          
say Marcus and Bender would mean by that is like we're just not          
1:41:04          
equipped to prompt and analyze the model in such          
1:41:12          
a way to reveal its its limitations right because we're talking with it like we would somebody we met down in the          
1:41:19          
coffee shop right and          
1:41:24          
you know so just just super valuable to remember that particular meat because          
1:41:30          
you know the linguists were pointing out that these these subtle          
1:41:37          
yet um kind of red flags that they were seeing in          
1:41:43          
these malls that that um yeah those without training wouldn't          
1:41:50          
wouldn't catch he here it's it's a bit more you          
1:41:56          
know well again it's like if you train on these formal formal          
1:42:01          
languages what would you expect so one what would you expect if you if you pretend that you          
1:42:09          
understand what large language models do right or how how how large language          
1:42:15          
models work which again I think you know again I want to have like the emergent          
1:42:21          
misalignment paper as as like do you really understand you          
1:42:28          
know or or would you or would you you know if this was a person would you think that they were you know kind of          
1:42:34          
mentally ill right right and um in the formal language case it's just          
1:42:41          
like it's it's subtler but then you Al you know so there's some there's some          
1:42:47          
existing Benchmark work that suggests that you get improvements          
1:42:53          
but I but I I I think it actually really highlights          
1:42:59          
um that we have to do a lot more          
1:43:06          
um a lot finer and a lot more intelligent probing of the systems to          
1:43:12          
actually get at you know what what what what have we built          
1:43:21          
here and and yeah anyway just just interesting in that          
1:43:27          
regard and you know would want to also pair it with like the theorem the theorem solers and          
1:43:33          
yeah so this is of course pre-training language models on for languages can          
1:43:39          
improve their acquisition of natural language it's unclear which features of the formal language impart an inductive          
1:43:46          
bias it leads to effective transfer so this draws from Linguistics and complexity Theory uh thinking about you          
1:43:54          
know how effective transfer occurs they hypothesize that effective transfer occurs when the formal language both          
1:44:01          
captures dependency structures and natural language and remains of them the computational limits of the model          
1:44:08          
architecture so they focus on Transformers here and they find that formal languages of both these          
1:44:14          
properties enable language models to achieve lower loss a natural language and better linguistic          
1:44:21          
generalization compared to other languag is pre pre-training or training on formal than natural language reduces          
1:44:29          
loss more efficiently than the same amount of natural language uh so I guess          
1:44:35          
uh they're using formal languages to pre-train or pre pre-rain models and it          
1:44:42          
can improve it the performance in terms of natural language so for a 1 billion          
1:44:48          
parameter language model trained on roughly 1.6 billion tokens of natural          
1:44:54          
language uh pre pre-training achieves the same loss and better linguistic          
1:44:59          
generalization with a 33% smaller token budget we also give mechanistic evidence          
1:45:04          
a cross talk transfer from formal to natural language attention heads          
1:45:10          
acquired during formal language pre-training remain crucial for the model's performance on syntactic          
1:45:17          
evaluations so yeah and Morgan was talking about how we could pair that with some other findings in uh in large          
1:45:27          
language model research um yeah and so we have some          
1:45:32          
other things in here though this is the intelligence soft matter paper I talked about earlier uh Morgan actually          
1:45:38          
originally posted that well again it came from came from active INF but he put it in here that's          
1:45:46          
that's how we got okay yeah all right so yeah that's uh yeah so I don't know if          
1:45:54          
we wanted to talk more about the the collection of papers we talked about in          
1:45:59          
this section or we had any more thoughts about uh reasoning and large language          
1:46:04          
models or any of that yeah I I'm I'm you          
1:46:13          
know it's really it's really difficult to assess          
1:46:20          
these obviously you know especially when um I          
1:46:27          
think I I don't use Google AI studio um but I think that's where we're          
1:46:36          
supposed to follow this I I put a link in professional Dev of you know um it's          
1:46:44          
the um Pi PMC developer had had reposted this and um          
1:46:53          
so I put a link this this um developer lead web page but it's SP p p a i g okay          
1:47:06          
like like her name is p yeah yeah okay um I I think her her Twitter handle is          
1:47:13          
dynamic web page yeah yeah I think it's funny okay um anyway she          
1:47:21          
she like I I think some of her kind of Outreach work is showing people how to          
1:47:27          
use this stuff um so yeah I haven't I haven't dived in to see what is          
1:47:34          
accessible with this Gemini too and kind of reasoning but I I wonder you know          
1:47:39          
like their examples you know they they're they're trying to say this is this is scientist          
1:47:46          
because it's discovered things that these researchers say they hadn't published right right          
1:47:54          
um but it that that doesn't that that that perhaps suggests it's not          
1:48:04          
a you know this isn't just a database retrieval right          
1:48:09          
right um fine          
1:48:17          
um yeah it's it's this isn't this isn't really          
1:48:23          
probing the problem though with          
1:48:29          
um it just reminds me somewhat of the aell GPT from last week right which is          
1:48:36          
like we don't really know how to ask the model about its internal          
1:48:45          
reasoning in in in constructive ways that are not um          
1:48:51          
oh um didd just just hit the add button so say add Channel and then you can then          
1:48:59          
you'll get a browse Channel or create channel and you can just say browse          
1:49:05          
Channel and then you can select those those other channels          
1:49:11          
like Dev nuro AI or data SI          
1:49:16          
ML and yeah okay cool um          
1:49:23          
so you know because it's it's I mean again just Devil's Advocate          
1:49:30          
right like okay you know you hadn't published that but maybe you guys have been lost          
1:49:39          
in farce right and and the model's just like well          
1:49:45          
obviously you know because it can look across a lot of material it's just like          
1:49:52          
yeah of course you'd put these three you know these 10 things together like like I mean again I'm I'm          
1:50:01          
I'm not saying this is you know absolutely the case because I          
1:50:06          
haven't I haven't dived into what they're their thing it's but you          
1:50:12          
know I I don't know if it's a convincing like this obviously was you know          
1:50:19          
insightful or uh uh no not not not ins this was obviously          
1:50:24          
imaginative right or you you know what whatever what's what's the right word for what we expect here          
1:50:33          
um and again it's too there's too much of there too much of this kind of vague          
1:50:40          
you know like like fitting fitting A          
1:50:47          
Narrative of what we think science is right as opposed to          
1:50:53          
you know yeah anyway but but Demi          
1:50:59          
on social media is is very very excited about this and you know like          
1:51:07          
like um he he he does you know more so than I think some of the tech leaders          
1:51:15          
you know he he tries to stay grounded and and certainly is is you know I I do          
1:51:22          
know that this has always been his dream right um of course it's been Schmid 's          
1:51:27          
dream too yeah I'm sure Schmid Huber always has like something when it does when it does          
1:51:35          
work would be like I said this in 1992 well it's nice to see tournament          
1:51:42          
selection being featured prominently no no I mean I there was a bunch of thing I          
1:51:47          
mean I mean I you know mention shid humor and then say this but          
1:51:54          
like I think we said this before like like it was obviously you          
1:52:00          
obviously GNA need like a hybrid system you know it wasn't going to be just like more training right right you know it          
1:52:08          
like like and um and I I think that well this is the          
1:52:15          
fascinating thing about this rise of of reasoning models right is that the large          
1:52:20          
language models aren't reasoners right like like you're you're now you're          
1:52:27          
now trying to build it you know it's just like you've got this squawking box          
1:52:34          
and now you're trying to you know get it to do the squawks at the right time you          
1:52:42          
know with the right bits you know um and          
1:52:49          
so a lot of that is is actually very different from you know          
1:52:56          
the supposed like insights of these of transformal models          
1:53:01          
right um so yeah anyway I I I I like I          
1:53:08          
said last week I think you know we need to we need to have a healthy dose of um          
1:53:16          
Mystery Science AI hype theater oh yeah you know          
1:53:23          
just and and like you know the Emily Bender uh yeah yeah yeah the          
1:53:32          
the have you watch Marcus oh sorry have you watched the are you referencing the          
1:53:37          
actual like the dares Mystery Science hype theater thing because I I mean to look at that but I haven't yet so I          
1:53:43          
don't know actually yeah yeah uh so they they twitch next week yeah I think they          
1:53:51          
twitch next week um and uh yeah I I you know and I I like          
1:54:02          
like much like news much like news about politics like I try to avoid it but I          
1:54:10          
get inundated in a a tech hype on          
1:54:17          
a hourly basis right I mean like like like like          
1:54:22          
again like the fact that we call these things foundation and or Frontier models yeah like I'm sorry that's that's just          
1:54:30          
uh you know that's just working for the Matrix yeah and and yeah anyway so it's just          
1:54:40          
like you know yeah let's just remind ourselves of those          
1:54:45          
criticisms and that these benchmarks are are are          
1:54:53          
[Music] crude crude and um yeah so a reason you          
1:54:59          
know and there's this false you know yes I love          
1:55:05          
leaderboards but there's this false you know the leaderboards suggest that          
1:55:11          
there's progress every every release right yeah          
1:55:17          
on this the metric whereas like like is it the progress that we actually wants          
1:55:23          
and or needs you know separate          
1:55:34          
um well father Father Ted should probably be brought up more          
1:55:39          
too I I yeah I've Got a Friend Pat Cooney from          
1:55:46          
high school and um I sent him this is wonderful I I've never Alan par          
1:55:52          
Partridge I don't know if you're familiar with alen Partridge but great great          
1:55:59          
comedy anyway he he he's talking to these two Irish in an          
1:56:06          
episode and I I found out later that these two irish guys are actually the writers of all of these shows like          
1:56:13          
Father Ted's um um anyway yeah I can send you a clip          
1:56:21          
okay okay well that's good let's let's move          
1:56:27          
on here um yeah I think I'll get into this and I know I've been waiting for Jesse to be          
1:56:35          
here because it's about affordances so I want to make sure that he's as some way to feedback I have a collection of          
1:56:42          
things on something called aordance learning so we're moving from large language models now we're moving to more          
1:56:47          
of embodiment and affordances so this is a type of learning called affordance learning uh          
1:56:54          
we'll start with this paper here this is from the archive uh from 20 late          
1:57:02          
2024 and the title here is a foran Centric policy learning uh this is on robot policy          
1:57:09          
learning and affordance Centric task frames so the let's read the abstract then move          
1:57:16          
on to the next paper and see if we can piece together what affordance learning is          
1:57:23          
so uh they're talking about robotic manipulation they're talking about embodied Robotics and they say that          
1:57:30          
affordances are Central to robotic manipulation well most tasks can be          
1:57:36          
simplified to interactions would test specific regions or objects so again          
1:57:42          
affordances are basically these things that help um help a robot or help us          
1:57:48          
interact with the environment it's like a door handle was an affordance uh tells you gives you information about how to          
1:57:54          
push the door open or to pull it close and this is just an example when you'll          
1:58:00          
find affordances especially in in body cognition because affordances you know          
1:58:06          
allow you to interact the world use your body and um have information that's          
1:58:14          
encoded into the world instead of from some sort of          
1:58:19          
memory uh so affordances are Central Rob robotic manipulation most tasks can be          
1:58:25          
simplified to interactions by focusing on these key regions we can abstract away task relevant information          
1:58:32          
simplifying the learning process and enhancing generalization in this paper we propose          
1:58:38          
an affordance Centric policy learning approach that centers and appropriately orients a task frame on these affordance          
1:58:46          
regions allowing us to achieve both intercategory and variance where policies can generalize across different          
1:58:53          
instances within the same object category and spatial invariance which enables consistent          
1:59:01          
performance regardless of object placement in the environment so you have intercategory          
1:59:07          
invariance where policies can generalize so this means that you have policies          
1:59:12          
that are invariant uh within a category uh across instances and then          
1:59:19          
spatial invariant which is like you know if you put the object anywhere in the          
1:59:24          
environment it has the same sort of per you know performance so uh when they          
1:59:32          
talk about affordances affordances are inherently spatial so we're always talking about something that's located          
1:59:39          
in space or something that's located W with respect to something else and so          
1:59:44          
for something to be spatially invariant is good because you you can put it anywhere in the space and it has the          
1:59:50          
same sort of meaning it has has this it results in the same sort of performance and then this intercategory invariance          
1:59:57          
allows you to you know have different shapes or different uh forms of that          
2:00:03          
affordance and have it basically do the same thing or have the same carry the same meaning so we propose a method to          
2:00:10          
leverage existing General Vision models to extract and track these affordance          
2:00:15          
frames and demonstrate that our approach can learn manipulation tasks using Behavior cloning from as little as 10          
2:00:23          
demonstrations with equivalent generalization to image-based policy trained in 305 demonstrations so they're          
2:00:31          
using large Vision models they're using computer vision to uh you know extract          
2:00:36          
frames where you have these affordances and then do this in a very sparse training          
2:00:43          
en okay so that's this paper the next paper is from          
2:00:49          
buildings which is a an DPI Journal so they have these          
2:00:54          
very obscure titles um this is this article is called          
2:00:59          
affordances architecture and the action possibilities of learning environments a          
2:01:05          
critical review of the literature and future Direction so this is from a different area of uh academic research          
2:01:13          
not robotics and AI but like in um          
2:01:18          
construction and architecture so they're they're approaching this a little bit          
2:01:24          
differently they're approaching this more from sort of a behavioral point of view and so the abstract here reads this          
2:01:32          
paper critically reviews the body of literature on affordances relating to the design in inhabitation of school          
2:01:39          
buildings so they're interested in affordances in school buildings and how they're designed and how people interact          
2:01:45          
with those buildings focusing on the influence of learning spaces and pedagogical practice          
2:01:52          
we argue that links between affordances architecture and the action          
2:01:57          
possibilities of school-based environments have largely been overlooked so they're arguing that there          
2:02:03          
needs to be a greater role for affordances in like building schools and having you know people do things in          
2:02:10          
schools and maybe this helps people do things that are positive um and such links hold great          
2:02:17          
promise for better allying space and pedagogy especially amidst changing expectations of what effect could          
2:02:23          
teaching and learning look like uh so emerging Innovative learning environments or IES are designed to          
2:02:31          
enable a wider pedagogical repertoire than traditional classrooms so these are          
2:02:36          
you know you think about a classroom and you know we can improve upon that in order to transcend stereotypical          
2:02:44          
understandings about how the physical environment in schools May afford teaching and learning activities it is          
2:02:50          
becoming increasingly Rec recognize that both design and practice reconceptualization is required for          
2:02:56          
affordances of new learning environments be effectively actualized in support of contemporary education with a focus on          
2:03:03          
the environmental perceptions of Architects Educators and Learners We Believe affordance Theory offers a          
2:03:11          
useful framework for thinking about the design and use of learning spaces we argue that Gibson's affordance Theory so          
2:03:17          
this is affordance Theory it's it's something that Gibson came up with and course it has ties to ecological          
2:03:24          
psychology uh should be more commonly applied to help situate conversations between designers and users about how          
2:03:31          
physical learning environments are conceived received and actioned for Effective teaching and          
2:03:38          
learning right without going into Gibson's affordance Theory uh and we can          
2:03:44          
put a pin in that to say you know this is um you know relevant to affordance learning and how you might conceptualize          
2:03:52          
that let's move on to the next paper here which is this neural networks paper          
2:03:57          
from 2024 on bio inspired affordance learning          
2:04:03          
for six degree of Freedom robotic grasping a Transformer based Global feature and coding approach so we're          
2:04:09          
going back now to artificial intelligence and Robotics uh you'll see that there's a theme where we're going          
2:04:15          
from like architecture to robotics so it's like you know they're they're kind of getting at this from I getting at          
2:04:22          
this from at least two different angles and so what I want to do is I want to set the stage for thinking broadly about         
2:04:28          
affordances and affordance learning and things like that so this abstract reads the six          
2:04:35          
degree of Freedom robotic grasping is a fundamental task in robot manipulation          
2:04:41          
aimed at detecting graspable points and corresponding parameters in a threedom inal space and this is something they          
2:04:47          
call forance learning which is you know again where your this is a very operational uh          
2:04:56          
definition of aordance Zing where this robot is trying to manipulate things in this space and has six degrees of          
2:05:03          
freedom with which to turn things in a three dimensional space so and then a robot executes grasp          
2:05:11          
actions with the detected affordances so it's like the most common easily executable grasp actions that          
2:05:19          
might align with the object or with accessible areas of the space existing research works works on          
2:05:27          
affordance learning predominantly focus on learning local features directly from each Grid in a voxal scene for each          
2:05:34          
point in a point Cloud scene subsequently filtering most promising candidate for execution so the idea is          
2:05:41          
that you have the space and you learn local features of that space and you evaluate those          
2:05:48          
features for you know which are which areas are the the best for doing a          
2:05:53          
certain thing or grasping or moving or whatever so you can decompose that space          
2:05:58          
into the best sort of trajectory the best place to grasp something          
2:06:04          
Etc but it's of course a local evaluation so that's kind of what          
2:06:10          
they're getting at contrarily cognitive models of grasping and this is not uh          
2:06:15          
cognitive models of aordance learning but just a grasping and there's a whole literature on kind of how we grasp          
2:06:23          
things and you know this is sensory motor integration sensory motor loops          
2:06:29          
and things like that uh cognitive models of grasping highlight the significance of global          
2:06:35          
descriptives such as size shape and orientation in grasping so they're          
2:06:40          
focused on different things in those kind of models where they think of not just about the space but they think          
2:06:46          
about the object the size shape and orientation of grasping meaning that the          
2:06:53          
object might have a handle or the object might have a place where it's easier to grasp than in other parts of the object          
2:07:00          
and so those are called Global descriptors these Global descriptors indicate a grasp path mostly tied to          
2:07:07          
actions so you have this grasp this grasp path and then you it's tied to the          
2:07:13          
actions of the performance um inspired by this we proposed the novel bioinspired Neal          
2:07:20          
Network That explicit incorporates Global feature encoding so they include these Global features in addition to          
2:07:26          
these local features that we typically use in a sort of a six degree or Freedom          
2:07:34          
model uh in particular our method utilizes a truncated sign distance function or tsdf is input and employs          
2:07:43          
the recently proposed Transformer model to encode the global features of the scene directly with the effective Global          
2:07:50          
representation we then use deconvolution modules to decode multiple local features and generate graspable          
2:07:57          
candidates so they use local features and they also use this Global representation to build this model in          
2:08:05          
addition to integrate Global and local features we propose using a skip connection module to merge lower label          
2:08:12          
lower layer Global features with higher label local features uh our approach when tested on          
2:08:18          
a recently proposed pile inacted Gras data set for decluttering task surpress          
2:08:25          
surpass state-of-the-art local feature learning methods by approximately 5% in          
2:08:30          
terms of success and declutter rates um and with the pile and pack grasping data          
2:08:36          
set but they have this uh in the paper we also evaluate its running time and          
2:08:42          
generalization ability further demonstrating its          
2:08:48          
superiority so this is the last paper um this this is end to end affordance learning for robotic          
2:08:55          
manipulation um this is from the archive this is from          
2:09:01          
20122 and so they have this uh figure one here aordance examples of different          
2:09:06          
manipulation tasks so this is opening a door opening a pot lid pick and place          
2:09:15          
and dual arm push so there are these different types of activities here          
2:09:21          
manipulation tests um this is so A and B are agent to          
2:09:27          
object affordance Maps so you have the agent to object so          
2:09:32          
the agent is uh interacting with the object and it's finding the affordance          
2:09:38          
so in this case it's it's finding the handle of the WID in this case it's finding the opening for the door it's          
2:09:45          
going and it's also coming out uh in in C we have an agent to object          
2:09:52          
an object toob affordance map so we have this agent to object so it's the agent          
2:09:58          
here to this object the lock so you have to take the lock you have to pick the lock up and you have to place it on the          
2:10:05          
surface down here so you have the agent picking up the object and then the agent          
2:10:12          
is taking that object and moving it to another object and then D is the Dual arm push where you have dual agent to          
2:10:19          
object affordance where you have two agents pushing on this chair and so          
2:10:25          
that's what they're getting at here so they have different ways that agents interact with objects for          
2:10:31          
affordances and you know having this affordance map where they know where the          
2:10:37          
you know the affordances are and where to engage so the abstract here is          
2:10:43          
learning to manipulate 3D objects in an interactive environment has been a challenging problem in reinforcement so          
2:10:50          
this is going to be a reinforcement right approach instead of an neural network approach in particular it is hard to          
2:10:57          
train a policy that can generalize over objects with different semantic categories diverse shap shape geometry          
2:11:05          
and versatile functionality recently the technique of visual affordance has shown great          
2:11:11          
prospects in providing object Centric information priors with effective actionable semantics so we have object          
2:11:19          
you know we have information prior about the object the agent learns the sort of          
2:11:24          
the shape of the object where the affordances might be and so forth we can          
2:11:30          
encode that into a prior and then we have these actionable semantics so we          
2:11:35          
might have semantic labels or things like a handle or a door opening or a          
2:11:42          
lock or some part of a chair uh as such an effective policy can          
2:11:47          
be trained to open a door by knowing how to exert force on the handle so you have to know how to open a door or already to          
2:11:53          
use that affordance uh and then of course you have to know how much force to exert          
2:11:59          
because if you see a handle and you exert the amount of force that you          
2:12:04          
expect to exert and a door doesn't open what does that mean does it mean that it's locked or does it mean that it's a          
2:12:11          
false door or it mean that you misinterpreted something for a handle what does that          
2:12:17          
mean however to learn the affordance it as often requires human defining action perimet which limits the range of          
2:12:23          
applicable tests so we only can train the model on what we know from like          
2:12:30          
human studies and affordances so we might en encounter affordances in nature          
2:12:36          
or we might encounter affordances as a robot that don't behave in ways that we          
2:12:41          
have descriptions for um in this study we take advantage          
2:12:47          
of visual affordance by using the contact information generating during the reinforcement Le training process to          
2:12:54          
predict contact maps of Interest such contact prediction process then leads to          
2:13:00          
an end to end a forance learning framework that can generalize over different types of manipulation tasks          
2:13:06          
surprisingly the effectiveness of such Frameworks holds even under the multi-stage and multi-agent scenarios we          
2:13:13          
tested our method on eight types of manipulation tasks results showed that our methods outperform Baseline          
2:13:19          
algorithms including visual based affordance methods and reinforcement learning methods by a large margin on          
2:13:26          
the success rate so this is uh this other robotics          
2:13:32          
paper so okay yes okay so that's all I have on aordance learning uh and you          
2:13:39          
know I hope that it was kind of wasn't meant to be an exhaustive review but it was kind of meant to be this          
2:13:45          
introduction to the concept and you know to kind of think about in terms of other types of          
2:13:51          
learning and think about in terms of affordances and          
2:13:56          
inod yeah yeah um it uh reminds me of of another meeting that          
2:14:05          
was this week um that I had forgot to          
2:14:10          
mention um buzz robot had uh which is which is an          
2:14:19          
interesting kind of group here in the Bay Area I mean they're kind of down          
2:14:26          
South Bay um they had the          
2:14:35          
um lead developer Remy Carden I think          
2:14:41          
card um from huging face talking about L          
2:14:48          
robots so they're they're the robot is          
2:14:53          
um is an interesting project so you know very          
2:14:59          
very in line with like narch X where it's just like like super affordable you          
2:15:05          
know they it's like 3D printable devices that you can put          
2:15:10          
together for like a couple hundred and          
2:15:16          
um yeah so as well as like lots of lots of support from hugging face and          
2:15:23          
pre-trained models and things like that but what was what was          
2:15:29          
interesting um was something he mentioned where it was just like like          
2:15:36          
there's been a a          
2:15:41          
real breakthrough um and he wanted to point          
2:15:47          
to um this particular um work called          
2:15:56          
Aloha um so I'll I'll put it in um I always do bra bird vehicles for          
2:16:04          
robotics yeah um and          
2:16:11          
um so you'll you'll see in braak bird Vehicles like there's a big Stanford          
2:16:18          
robotics um uh uh          
2:16:24          
representation there and you know they they quite rightfully have some some          
2:16:30          
really good history um I absolutely recommend the secret          
2:16:37          
history of silon Valley um as as for you know wh why why why did          
2:16:46          
everything happen here or you know not here but down south yeah um but          
2:16:54          
uh yeah just just talking you know and what aloha's focused on is          
2:17:02          
is manipulation you know like like like in          
2:17:07          
this case it's this um it's a b manual manipulation you know          
2:17:14          
so it's like two two hands but but you know Remy's point was just like like          
2:17:21          
Aloha really changed things and um I wonder how          
2:17:30          
much you know like like if we looked          
2:17:35          
at yeah just um I think it would be interesting to go and have a look at the          
2:17:42          
kind of language that they're using in terms of you know the particular you I mean this is this is where this is one          
2:17:48          
of those things where it's just like how do          
2:17:54          
roboticist what what vocabulary do roboticist use yeah right and and          
2:18:00          
because they'll be the first to to admit that there's a bunch of things that they          
2:18:05          
use in a in a non-standard way          
2:18:10          
right but but but specifically about what affordances are trying to get          
2:18:19          
at like like how yeah so like how much of these papers are are kind of          
2:18:27          
redescribing Robotics or you know or like but but perhaps          
2:18:33          
missing like these these past researchers are absolutely thinking          
2:18:39          
about affordances yeah yeah but they're just not using the term you know yeah          
2:18:45          
and and yeah anyway I I just I think that would be really interesting          
2:18:52          
um uh I'm meeting with a student this evening like it'll be my evening his          
2:19:00          
morning he's in Singapore um but like this will be like the I think          
2:19:07          
like the third student that is is interested          
2:19:13          
in um you know robot like like          
2:19:21          
neural signals to control robotic arm right um and anyway I I I'm hoping you          
2:19:30          
know like like this L robot talk was the first where I was just like hey wow okay          
2:19:35          
maybe we could actually have you know a bunch of people playing with their own          
2:19:41          
arm because this is something that um uh we can print out ourselves you know and          
2:19:49          
and um so it was really nice and and hopefully          
2:19:54          
we'll be able to say more about it once we get some          
2:20:01          
some oh look there's even a okay yeah so a low a low cost Open Source Hardware          
2:20:07          
system for B manual tele operation with a$          
2:20:13          
20,000 budget it's capable yeah so um we're          
2:20:19          
hoping for something that's more like uh two 300          
2:20:25          
yeah so uh Jesse posted in the chat here          
2:20:31          
uh Janet Johnson and Linkedin someone who may be relevant to affordances and          
2:20:37          
also technological spatial processing affordances uh she's also co-designed by          
2:20:43          
human centered AI course and then DD says I actually have to head out it's          
2:20:48          
been great joining the meet I've learned a lot looking forward to seeing you next Saturday thank you for joining us ason          
2:20:54          
please introduce yourself in the uh General channel the slack and if you want to introduce yourself on narrow          
2:21:01          
Stars you can as well okay uh yes and I'll I'll send those uh papers out or I'll make those          
2:21:09          
papers available in the YouTube description but also going to put them in the uh chat in in          
2:21:17          
slack so that's all for today thank you for joining us and see you          
2:21:24          
next week take care take care bye bye
