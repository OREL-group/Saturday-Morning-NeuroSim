## Meeting Recording

[YouTube link](https://youtu.be/gXwRdwHGs6U)

## Mastodon thread

[link](https://neuromatch.social/@OREL/115557478064660008)

## Feature Videos

[AI Agents and Economic Productivity](https://youtu.be/8KK23Ib5RRk)

[Active Inference Institute 2025 Symposium Review](https://youtu.be/d_rJ1LWodds)

## NOTES



## TRANSCRIPT 
0:00   
Hello. Morning. All right.   
0:08   
So, how are you today? Good. All right. Good.   
0:17   
All right. Yeah, let's get started. So, uh this week we had the diva worm   
0:23   
meeting on Monday and we talked about epithelial mechanics. S always brings a paper to   
0:29   
the the meeting she'd like to go over. So we went over this model of epithelial   
0:36   
mechanical feedback in cell sheets and then we talked about macro molecular   
0:42   
crowding and some of the biological functions of that and you know the phenomenology. So   
0:50   
like you know you see macroolelecular crowding and cell   
0:55   
uh cytoplasm. So, you know, usually when we look at a textbook, we see a nice neat picture of like the organels and   
1:03   
the nucleus inside the cytoplasm and then the membrane on the outside.   
1:11   
And of course, that's not the way reality is. Reality is where we have all   
1:17   
this macroolelecular soup floating around. Sometimes it's very densely packed. Sometimes it forms   
1:24   
these what they call condensates. And so you know we talked about that there there are a lot of papers on that   
1:32   
topic. There's also um another paper that I talked about after that on archa   
1:41   
bacteria and how there there's a I guess an order of   
1:50   
or a I guess a genus of ary bacteria that have   
1:56   
they share uh compartments and membrane compartments inner   
2:02   
compartments with ukarotes. So as you know archa bacteria and   
2:07   
ukarotes are uh related to one another and then bacteria was the sort of the first   
2:15   
uh uh kingdom of life to derive from the common ancestor. So bacteria are out   
2:22   
here and you bacteria or archibacteria   
2:27   
and ukarotes are out in in a plate by themselves. And so they or you would imagine they   
2:35   
would share some common features of cellular organization. And so that's what this paper was about talking about   
2:40   
how you get this sort of macroolelecular crowding sort of the origins of it   
2:46   
there. And of course you know that this is not to say that this is   
2:51   
uh shared with this is not maybe not the same evolutionary origin as you see in   
2:57   
ukarotes but you see this phenomena in ary bacteria.   
3:04   
Okay that was that was deb then uh Wednesday through Friday or   
3:10   
yesterday we had the active symposium. So we   
3:15   
contributed to the active symposium. We contributed two things. We contributed a talk on spatial   
3:22   
intelligence and then a talk on open source sustainability. And those are both on   
3:28   
our YouTube channel uh right in the first few videos.   
3:34   
Um and you know it was a good session. So it's as usual it's usually live   
3:40   
streamed on YouTube. So Daniel Freriedman, he sits there and   
3:46   
he he produces the whole thing. You know, it it's these three marathon   
3:51   
sessions of like 13 or 14 hours and, you know, he has pre-recorded talks and live   
3:59   
talks and panel discussions and he puts it all together and then eventually he   
4:06   
takes all of those sessions and breaks them down into the independent talks.   
4:11   
individual talks will be on the um active inference YouTube channel later   
4:18   
but right now there are these three streams one for each day. So if you go to their YouTube channel active   
4:25   
inference institute they have those long streams available. And so I wanted to spend the first part   
4:32   
of the meeting kind of reviewing the symposium and going over some of the   
4:38   
highlights that I found interesting and you know I encourage people to go there   
4:43   
and see what they find interesting and explore and I think it's a lot of good   
4:49   
stuff there a lot of interesting things.   
4:54   
All right. So, uh the first thing is that you know we have a symposium program.   
5:00   
This kind of talks about the different people contributing to the uh symposium.   
5:06   
So we had all sorts of people who are involved in active inference uh in   
5:12   
industry in academia and you know trying to find places where   
5:18   
this has been applied. So there are a lot of places where this has been applied. Everything from finance to   
5:25   
chiropractic medicine to uh robotics to   
5:31   
computational modeling uh to other areas as well. And so you had all these   
5:36   
different people coming together um and they were doing their talks uh very wide   
5:43   
range of talks in the session. So as you can see this doesn't really have the   
5:49   
talks in order, but um you know eventually they're going to put them all   
5:54   
on the active inference YouTube channel. They'll be in order. A lot of people   
6:00   
here from you know people who are here year after year and then people who are   
6:05   
new and and just kind of uh spending time in the symposium for the first   
6:11   
time. But you know there's a lot of diversity in terms of the talks. Uh it's   
6:17   
not just like cognitive neuroscience. Uh a lot of philosophy   
6:23   
um of course a lot of uh cognition and neuroscience   
6:29   
um a lot of like computational modeling a lot of philosophy. Adam Saffron was uh   
6:37   
giving a talk on his AI alignment work which we'll talk about later.   
6:43   
Um and you know interface design   
6:48   
uh different types of sort of applications of active inference   
6:54   
mathematics of active inference and um and then Carl Fristen gave his   
7:00   
opening keynote which we'll kind of talk a little bit about too.   
7:05   
This is a interesting talk here. net zero job losses from AI and other   
7:11   
surprises emerging from citizens dreaming about the solid solidarity AI stack. This was an interesting talk   
7:18   
thinking about uh sociote techchnical systems and active influence. This is   
7:23   
the logo for the uh part one for the YouTube channel. So this is the live   
7:29   
stream. So this is um something that was created for last um well I guess for the   
7:37   
symposium is this virtual environment and this is uh the URL for this is   
7:44   
pre.experience.nnewman.game. So someone created a virtual environment   
7:51   
for the active inference symposium. Um, there's a textbook there as me as my   
7:58   
avatar. I didn't really bother to get dressed up for this shot. It's just a screenshot. And, you know, here's the   
8:04   
active inference book. You can explore that book. You have uh seating here.   
8:09   
This is kind of like almost like Second Life where, you know, I remember being   
8:14   
in Second Life about 15 years ago doing some academic things. We had some academic spaces in Second Life that were   
8:21   
really interesting. And so this is uh sort of like that. Here's another shot   
8:27   
of the active inference book. This is the symposium program. You can open you   
8:32   
have to hit your E button and you know hold it down and you can read the program. Flip the pages.   
8:39   
There's part of the program there. Um   
8:45   
there's some courses over there. Active inference institute courses. So you can go over there and explore the course   
8:51   
content. Uh this is the active inference institute logo and floating in the air   
8:58   
here. Um there's a meeting room over here. The symposium legacy the cognitive   
9:05   
map of active inference and then this is uh course is scaffolded by the   
9:10   
institute. Physics is information processing active inference or the social sciences.   
9:19   
uh you know this is more of a logo here. You can do all sorts of interesting   
9:24   
things in in these virtual environments like you have to represent   
9:30   
space in interesting ways. So it's very interesting stuff and I I like to see this uh sort of thing. I don't know how   
9:37   
this was I don't know who made this how this was made necessarily. They just gave a short u discussion about the   
9:44   
contents but it's you know it's quite quite useful. I think quite interesting.   
9:51   
Um so this was another talk the phenomenological robot. So this was um   
9:57   
again you know about active inference and applying active inference to different areas of inquiry. This is Jean   
10:04   
Frasier. Um this is a research fellow at the institute. So the research or the   
10:10   
institute supports several research fellows who uh are able to pursue their   
10:16   
work under an umbrella of institute and he did this work with um   
10:23   
looking at robots and phenomenology and active inference. And so one of the   
10:29   
questions he was asking in this talk was what does it take at a minimum for an   
10:35   
autonomous robot to learn to survive in a world it knows initially nothing   
10:41   
about. This is a problem of course that a lot of AI research is interested in.   
10:47   
This is something that's motivated world models where you want the system to know   
10:52   
something about the world without you know training it from the start. So you train the model and then it basically   
11:00   
only knows the things that it's been trained on. You want your robot to be able to sort of boot up in a world,   
11:07   
figure things out and then maybe get training later. Um, you know, so this is kind of a   
11:13   
long-standing question. Um when I was in graduate school, we had a   
11:19   
professor in the cognitive science program who was really on developmental   
11:24   
robotics and he used to have these robot. He his his take on this question   
11:30   
was to use a developmental approach to build this developmental robot. The robot would just kind of boot up and   
11:36   
gain experience from online learning and that was that. You know, I mean there   
11:42   
were some successes there in that research. But uh you know this is a a hard question. It's a hard problem to   
11:48   
solve. And so you know I I was interesting that he's had this take on it.   
11:55   
Okay. So then uh this is Alex Sbine who was doing these simulations of ants. So   
12:00   
this is anterence active anterence. So like you know active inference the um Daniel   
12:08   
Freriedman loves word puns and you know his he likes to talk about the institutes affordances and all this and   
12:16   
he has this thing about anterence because um Daniel's background is in looking at   
12:24   
ant behavior and so this is so this is Alex Sabine's work though and he has   
12:29   
this alexabine.github.io IO and this is a fixed ant colony. So   
12:35   
these ants are all these uh little Markov machines that he has running and they're running around and they're   
12:41   
behaving like real ants. So they're reproducing biological behaviors through   
12:46   
a computational model. This is another shot of this ant colony. So I guess if you go to this uh website,   
12:53   
the GitHub IO site, you can see these animations and these simulations for   
12:58   
yourself. the ant is exploring the space and they're they're kind of behaving like ants I guess.   
13:06   
So that's and then this is more about like sort of how this works. This is modeling a lot of the antatomy   
13:14   
with a head, thorax, abdomen, six legs and antenna. Realistic morph morphagenic   
13:20   
movement with leg coordination and natural gate patterns. There's dampening pheromone networks between ants. There's   
13:28   
tunnel construction, multi-level chambers is modeling the ant colonies as   
13:34   
these structures and dirt. There's collective decision making, territory and combat and memory entanglement. So   
13:42   
all these things of course pheromone networks are important because you know ants when they uh sort of explore their   
13:49   
environment they lay down a pheromone. The pheromone tells other ants where to go for things and they build this   
13:55   
network of pheromone trails that they can use as a collective memory. That's   
14:01   
why he's modeling memory. And then of course you can model the social complexity of social insects as   
14:09   
well. And so this is this is this kind of work uh looks really interesting.   
14:17   
Here's some more about the ant colonies. uh just has this sort of diagram, you   
14:23   
know, diagrams and descriptions, specifications.   
14:28   
This is another thing he did was a marine ecosystem simulation. This again is   
14:35   
a simulation where looking at different types of elbow growth, plankton drift,   
14:42   
oxygen levels with uh different aspects of marine environments. So you have all   
14:48   
these different um marine environments where you see collective behavior and pattern formation. I have to say this   
14:55   
year there were a lot of simulations. Um so we had in terms of software we had   
15:02   
the main software platforms for active inference. Um I think RX infer was   
15:09   
represented there. Um the there is some things on biiobotics   
15:17   
and you know a couple other uh people who were talking about ways that you can   
15:25   
sort of implement active inference as a solution and then you had these uh   
15:30   
software packages or people who are doing almost like a life simulation. So   
15:36   
this is like an AI simulation and you know this is uh interesting to active   
15:41   
inference because we're interested in behavior output behaviors and so this is   
15:46   
all kind of in that category. So this work by Alex Sabine and then there's   
15:53   
work later actually associated with what uh Adam Saffron is doing that's also   
15:59   
very interesting in terms of a life simulations and um you know getting at some of the   
16:05   
questions in active inference. So this was um some of Adam said this is   
16:12   
not the work I was talking about uh with respect to AI safety but this is dream to explore this is a world model   
16:20   
inspired architecture. So this is um Gobesh Subarrage Arenaish and Adam   
16:27   
Saffron and I think Arena was the one who gave the talk and so this is in con   
16:33   
in collaboration with MA in Montreal   
16:38   
and uh this is dream to explore. So what they're doing is they're implementing this dreaming algorithm. They're doing   
16:44   
this uh I guess as a world model. I know we've talked about this in the meetings   
16:50   
uh but this is basically that work. So rim to explore aime   
16:57   
um it this is the architecture so you have data collection you have an agent   
17:02   
that has an exploratory policy you have an environment with limited interactions   
17:07   
you have experience which are these parameters here the unfortunate uh acronym SARS uh replay   
17:16   
buffer which gives real trajectories and that goes down to the world model learning where you have this um   
17:23   
variational autoenccoder which is serves as the prior VR plus perceiver and then   
17:29   
reward model. So you have this representation model, transition model and value prediction.   
17:36   
And then now those all go down to imagination based planning which is where you have the imagined, the actor,   
17:43   
the critic and the explorer. And then that goes down to an improved state policy or an improved safe policy which   
17:51   
then feeds back to data collection. So this is uh this kind of is uh a   
17:57   
framework for AI safety but the focus here is on learning using dreaming as a   
18:03   
mechanism for consolidating memories and other types of behaviors.   
18:10   
So, uh, they talk about vulnerable and embodied AI. Developing AI agents that   
18:15   
recognize and model their own vulnerability, similar to biological systems, encourages behaviors that   
18:22   
prioritize safety and avoid harmful decisions to herself and others. Then   
18:29   
there's unified frameworks for AI safety. So building agents based on the free energy principle to ensure agents   
18:36   
can manage internal states which is the world model in response to external   
18:41   
uncertainties aligning with human compatible objectives and promoting safety and pro-social behavior. So   
18:49   
that's kind of the boiler plate on that. Then there's of course the implications   
18:55   
of dream to explore for AI safety itself but reduced real world interaction. So   
19:01   
physical systems could be damaged during exploration. So we don't necessarily want them to go out and explore the   
19:07   
world. We want them to to gain experience sort of in place while they're dreaming. Human safety could be   
19:14   
compromised by learning agents. We want to make sure the agents are fully trained before we put them in contact   
19:20   
with humans and expensive or irreversible action should be minimized.   
19:26   
So the safer exploration is one of the goals of this. So the model can explore   
19:31   
what if scenarios and imagination before attempting risky behaviors in reality.   
19:38   
And then the containment portion of this is that the imagination based approach creates a natural containment mechanism   
19:45   
where potentially dangerous policies can be evaluated and diagnosed in simulation   
19:51   
before deployment. So this is about training agents and uh this this view of   
19:58   
AI safety that is that this training an agent giving it information about things   
20:05   
it's not seen before is something that happens in this dream state. And so you   
20:11   
know it's like the machine is like dreaming and then it wakes up and it it it goes about its business   
20:18   
fully trained in knowing what's going on in the world. So this is another approach to that question. What how do   
20:24   
you make a machine that behaves adaptively in the face of things it's never seen before.   
20:32   
Now this is the um the talk I mentioned about um the   
20:38   
economics of AI. So this is reflections on economic myths emerging about AI and   
20:46   
the future. This is um this website cgraphy of generative aai.net is this   
20:53   
person's uh website and they're they give this talk on net   
21:00   
zero job losses from AI enriching learning networks for adaptive resilience.   
21:06   
So, it's just just kind of about this idea of AI replacing everyone's jobs and   
21:12   
like how to maybe overcome that or how to think about that differently and so   
21:18   
forth. Um, so then it mentions the two economic   
21:25   
myths about AI's future impact on humanity. Um, the first myth is AI will shrink the   
21:32   
bad. AI will take over inefficient humans which means of course humans are   
21:37   
lose jobs and then that's one myth and of course you like all other technologies   
21:43   
you know there's there's job replacement but there's also job creation um and then the other myth is AI will   
21:50   
help grow the good amplify human meaning and contribution this is where there's a   
21:56   
cooperivity between machines and humans and of course those are both myths in   
22:01   
the sense that you know they're not sort of predestined outcomes.   
22:07   
Uh then he talks about the science of predicting economic futures. Your future is more accurately predicted by   
22:13   
information property clusters, networks and meta networks in which you are in.   
22:19   
So he has this figure from Halgo and Houseman from PNAS 2009   
22:26   
and the atlas of economic complexity which show um GDP per capita and   
22:34   
economic complexity showing countries as they're ranked. And then this graph up   
22:40   
here which I can't really read the uh y axis it's basically observed growth   
22:47   
between 1995 and 2000 or 1985 and 2000 and then growth predicted for mismatch   
22:53   
between economic complexity and GDP per capita in 1985. So this is kind of a   
23:00   
comparison here and just different ways you can plot the data to show that   
23:05   
economic complexity predicts future economic growth. And then there's this interesting graph   
23:11   
here on the right which is where you have over time you have this sort of knowledge and   
23:19   
knowhow in our society's computational capacity. You have person bite, firm bite and then   
23:26   
you have people networks and networks of firms. So you see that people are like   
23:32   
sort of a person bite. The maximum value for knowledge and knowhow is a person   
23:37   
bite. So if you have networks of cells uh which are people you have a capacity   
23:44   
that reaches this person bite which is a unit of information I guess and then in   
23:50   
the networks of people you have an increase maximizing as the firm bite   
23:56   
which is you know a firm like an organization or a company or something   
24:02   
like that and then networks of firms. So not just networks of people but networks   
24:07   
of these firms and that goes beyond the firm bite and maximizes I guess in   
24:13   
something else maybe like a market bite or something like that. So this is kind of a nice graph because it reminds me a   
24:21   
lot of the biomplexity graphs that exist where they're looking at things like   
24:26   
cells, organisms, ecosystems and then looking at bio complexity as it builds   
24:32   
from there. So that's very interesting work. noticed in general there was a very   
24:39   
strong emphasis on complexity theory as well as active inference. So people talking about how they were influenced   
24:45   
by early complexity theory, how they were influenced by current complexity theory and a lot of these themes were   
24:52   
people are dealing with complex systems and they recognize they're dealing with complex systems. So it's very good very   
24:58   
good secondary sort of theme here. This is a key intervention   
25:06   
approach to AI augmented learning capacity building and the the tagline   
25:11   
here is boost the learning capacity of all your networks. So these are nested layers of learning   
25:16   
from I guess from cells cell networks to   
25:22   
organs to humans to networks of humans and you have these nested layers of   
25:28   
learning in each. You go from the individual to the collective. So you   
25:34   
know the individual and the collective deal with learning differently. There's   
25:39   
something called interlearning and the collective which are embedded in the interactions and not the individuals.   
25:46   
This is kind of you know another kind of take on the idea of emergence.   
25:51   
And then of course you know individuals will be influenced by changes in the   
25:57   
individuals even within large groups. Collective learning by contrast are   
26:04   
changes in the agreements and practices between members of the collective. So   
26:09   
you know you have this difference between the individual nodes kind of learning their own things and that not   
26:15   
propagating versus collective learning where all the things that the nodes know are embedded   
26:21   
in a set of rules that manage the connections between the nodes. So the rules of interaction are for the   
26:28   
collective learning the rules of the nodes or the single units are embedded   
26:34   
in individual. So this is a valin car and his uh talk   
26:41   
is active inference for the social sciences. This is something that the active inference institute is interest   
26:47   
quite interested in applying active inference to different social science problems and he's going to go over the   
26:53   
state-of-the-art of this talk. uh he talks about how social structure   
26:59   
is the aggregate of pattern social arrangements in society that are both emergent from and determinate of the   
27:07   
actions of individuals. So this is where um social structure is both emergent   
27:13   
from individuals and determine the actions of individuals. there's this um   
27:19   
feedback that is where social agents both create their social arrangements   
27:27   
and are constrained by those social arrangements. So this opens three important questions.   
27:35   
The first is how social structure is implemented in human minds. Then the second is the relation between   
27:42   
social structure and agency. And then the third is how does social structure change between contexts or   
27:48   
over time. So this is you know social structure is a sort of an old idea goes   
27:54   
back to Levby Strauss and the structuralists and the idea is you know you have these   
27:59   
structures that exist sort of outside the individual something like culture   
28:04   
where you know you're embedded in culture but you also create culture. And so the question is is like how does   
28:11   
culture evolve or come to be? And then you know how do you recognize   
28:17   
culture as sort of a unit of measurement? How does culture change? How do individuals influence cultural   
28:24   
change? Things like that. So that's very relevant to sort of this uh and and it's   
28:31   
interesting the structural views is French school of thinking. So he's also French and so there's this continuity   
28:38   
there. But um it's just thinking about those kinds of questions and then   
28:43   
bringing it active inference past it. So then yeah talk about active inference   
28:49   
and cultural evolution and there's this paper um   
28:54   
that talks about this thinking through other minds a variational approach to cognition and culture. This is applying   
29:02   
active inference to culture. Um   
29:07   
and the tagline here is the transmission of culture is enabled by the integration of   
29:14   
regimes of attention uh is enabled by thinking through other   
29:19   
models. So this is uh thinking about cultural transmission or how cultural   
29:26   
structures are transmitted say to the next generation or to other people or to   
29:32   
um you know just kind of reinforcing cultural norms and values. And so the   
29:39   
transmission of culture um is I guess involves regimes of attention and the   
29:45   
sort of theory of mind. So this is kind of how they're putting this together. Um   
29:50   
it's interesting that cultural transmission of course is also a long-standing research problem. Most   
29:56   
notably was addressed by Dawkins uh with his idea of memes. And then there was a   
30:03   
journal of mimemetics that was around in the 90s and 2000s but it never really   
30:08   
kind of um persisted. And interestingly after that start the the journal of   
30:15   
mimetics went away the actual idea of internet memes came to be. So like as   
30:22   
that journal was failing the actual internet meme   
30:27   
industrial complex came you know rose to prominence. So people don't necessarily   
30:33   
use the idea of memes anymore for cultural units of cultural transmission, but it is a useful sort of maybe less   
30:41   
than useful because it has so much baggage, but it is a useful sort of construct for thinking about the   
30:46   
transmission of culture. And so they offer their own definition here. Uh   
30:52   
yeah, this is a finding common grounds in a life inspired multi- aent simulation environment with research   
30:58   
program or AI alignment. So yeah in this talk Adam Saffron is going to be do   
31:03   
talking about AI alignment but from the standpoint of these artificial life   
31:09   
simulations. So Adam Saffron right now is at the Allen Center for Discovery   
31:14   
which is at Tufts and of course that's working with Michael Leven Michael Levens in that institute and a couple   
31:21   
other people. So this is uh kind of his take on AI alignment   
31:27   
u but it's it's heavily based on these a life simulations we'll see. So it's like there was this main talk and then there   
31:33   
are some simulations that were done uh showing I don't really understand fully   
31:40   
how they did these simulations. It was a platform that they developed at the   
31:45   
Allen Discovery Center and I'm not sure if it's based on continuous lineia or if   
31:50   
it's something else because it looks kind of like continuous line but we'll look at it in a minute.   
31:55   
So this is uh so you know he talked about a number of things about AI alignment in   
32:02   
the talk but he brought up this idea of having a bell labs for AI alignment   
32:08   
research and then you know thinking about this in terms of active inference.   
32:13   
So there are a number of people here involved that are also in the active inference world. Um and so they're   
32:20   
they're doing a number of different topics. Um there you know they have someone who's   
32:26   
building the simulations. People looking at agency and value people looking at bounded rationality   
32:33   
and formal modeling of multi-agent alignment. Uh people looking at path flexibility   
32:39   
and resilience. The mathematical modeling of value canalization. Canalization is this thing   
32:47   
in development where things are buffered into certain channels through a developmental space. Hence canalization   
32:54   
which is channelization um and uh you know looking at sort of   
33:00   
that in terms of free energy landscapes. So taking it from the developmental   
33:05   
uh from the epigenetic landscape or developmental landscape metaphor to a   
33:11   
free energy landscape. Um also looking at multi-agent alignment   
33:17   
protocols, biologically inspired architectures, artificial consciousness, effective   
33:23   
alignment and maturity and then uh effective neuroscience, EGI   
33:29   
and biologically inspired approaches to alignment. So this is something I think where they're looking for collaborators.   
33:36   
Uh I don't know. We know we talked about Bell Labs a couple weeks ago in our uh   
33:41   
meeting and you know I don't know if he's getting at you know can we harness the sort of creativity and innovation of   
33:49   
Pal Labs but that's that's what they're trying that's sort of the mental model they're using for this collaboration.   
33:57   
So this is their platform common grounds and so this is where they're building   
34:02   
these aife simulations. So, they have these little agents and they're exploring the world and they're playing   
34:08   
with these uh balls of different things. They have these different   
34:13   
sensory radi. So, they're sensing things in the environment. Like I said, I'm not really   
34:19   
familiar exactly with how these work, but I thought they were interesting to sort of mention. Uh, so this is where   
34:26   
you have the simulator. You can play with these simulators as video games and   
34:32   
then you have these benchmarks for performance. So we built an Aife simulator simultaneously support virtual   
34:39   
worlds or games which bridge humans and AI through play. So people are playing the video games. There's a simulator   
34:48   
generating this stuff. People play these games and then there's a benchmark. The benchmarks themselves are the social and   
34:55   
cultural effects of AI interaction. So they're looking at how people are interacting with AI agents. And then   
35:02   
there's this research and nurturing of benevolent embodied AI. So this is where   
35:08   
they're trying to develop more uh sort of humanist AI or benevolent sort of   
35:15   
embodied AI. That's that's kind of they're trying to do. That's very interesting. Um they uh I think the   
35:23   
simulator and benchmarks are open access. Um, they're going to release the game demo on the web and Steam and then   
35:31   
iteratively improve simulatorbenchmarks. So, this is the uh environment.   
35:39   
Uh, this is a screenshot of this is I think concept art actually of the thing. So, they're not quite done with it yet,   
35:45   
but they're kind of showing how this works. You have interactors here interacting with different structures in   
35:52   
the world. And then this is kind of where they're   
35:57   
um they have this sensory gradient here and they're doing things in the world.   
36:03   
I'm not really sure how this works, but they were just giving a short demo of this stuff at the symposium.   
36:11   
This is the scaling of body and world. And so again, you have these, like I   
36:16   
said, it could be continuous linear based on something like that. Continuous lineia is a continuous cellular automa   
36:24   
where you have locations in the world that are sort of centrids and then you have they they behave kind of like cells   
36:32   
in a cellular automa. So I'm not really sure what they're doing for this but it is does look   
36:38   
interesting. Um then this is just more scaling of the body and world.   
36:44   
And then they're doing things like looking at interosception for body parts. You have this network   
36:50   
within the agent. You have uh energy levels, nutrient levels, temperature,   
36:57   
damage. There's some record of the organ state and basically you know the agent the   
37:03   
agents have this interosceptive system which monitors their internal state which monitors kind of how they behave.   
37:11   
uh interosceptive control uh provides analoges of hormones slash visceral   
37:17   
motor system. So if the agent is interacting with things in a good way,   
37:22   
you know, it's it's it's feeling good. If it's interacting in a bad way, it feels bad. And then there's this whole   
37:28   
interceptive control system which is based of course on allasis which uh will   
37:35   
regulate the behavior of the agent. It's quite an uh I think an ambitious kind of   
37:40   
approach to AI safety and agent safety. Uh then there's vision. So they have   
37:46   
this is their visual system where you know they kind of model this agent visual system as a one-dimensional array   
37:52   
of ray hits. So it's like it's the rays are coming out of the eyes or rays are going into the eyes   
37:59   
um where there's a connection between objects in the environment and the visual system. So it's capturing all of   
38:07   
these images or all these points and there's this one dimensional set of points like a line that kind of connects   
38:15   
all this and gives a map of the world like this. So this is the first person view of these visual rays hitting these   
38:22   
different objects. So you see these green objects and then there's there these other the body here which is this   
38:30   
light tan object and then this uh ball down here which the dark green or dark   
38:36   
uh brown object and then the dark green objects are these balls here. So you can   
38:43   
see how it's kind of mapping the visual system from sort of a a side view of the   
38:49   
eye and the interactions with objects to this first person view. And so there are these different viewpoints that one can   
38:56   
take on the environment that affects the interosceptive regulation.   
39:02   
And then this is more concept art. This is the visual uh or this is the sort of the sensory uh uh radius here. There are   
39:11   
these interaction points of objects. There's this interosceptive network. And   
39:17   
so you can see examples of that here. And then they want to look at varieties   
39:22   
of life. So they look at this as a model of development and evolution. You have reproduction where new cells   
39:29   
but off of old cells. And then you have communications between cells to sense   
39:34   
and act. So it's really this kind of a life approach to agent um development   
39:40   
and then that leading to AI safety because agents uh kind of know their own   
39:45   
bodies. They know their own context. Here are two cooperating individuals.   
39:52   
This is synchronous joint activity where they're vicariously enjoying the single object. they're sharing access to the   
40:00   
object. And then this is a potential future work cooperating molecules where they have   
40:07   
molecular scale particle cell simulations, membranes as strings of particles, cells coupled by gap   
40:14   
junctions. You can see the red molecules of various types information metabolic   
40:20   
structural and then the sort of this analogy of origin modeling the origin of   
40:25   
life and undersea events. So, you know, you could model the this this this sort of simulation environment isn't just   
40:32   
good for AI safety and for agent design, but also for looking at different things   
40:38   
having to do with cells, development of cells, evolution of cells, and the organ of life in these vesicles.   
40:49   
And so yeah, then you you know they talk about how all of these simulations are   
40:54   
in 2D space, but if you and they don't do in 3D space, but if you extend it out to 3D space, they're interesting   
41:01   
properties that emerge. So rich embodiment possible in 2D continuous space. 2D is simpler, easier to   
41:08   
visualize, computationally cheaper. But there are rich properties of 3D space that 2D space does not have. Um and so   
41:17   
we may actually have good enough 2D approximations to do knotted strings and embeddings of arbitrary graphs. Um you   
41:26   
actually I guess that's the 3D space but then the 2D space um has crossover   
41:32   
points to that. So you can capture some of that in 2D but to capture it fully   
41:37   
you need three-dimensional representations. You also have these non-communitive rich   
41:43   
rotation groups where you can map the full range of real world motor skills   
41:48   
for these agents. Um you also have a rich range of motor skills available in 2D continuous space.   
41:56   
And there's this hypothesis in cognitive science called the grounding hypothesis   
42:03   
or the grounding idea that you need to ground conceptual   
42:08   
metaphors. So you need to have sort of a ground state for for metaphors or for   
42:14   
meaning and they don't seem to find that in their simulations. So it's interesting that they're, you   
42:20   
know, using embodiment sort of as a standin for that. So there are all sorts of things, you   
42:26   
know, you can look at these different representations of space and how they're   
42:32   
uh how that contributes to these simulations as well. This is uh another kind of shot from one   
42:39   
of the So there were a bunch of talks after Adam's talk. It was like these two-minute talks. They had two-minute   
42:45   
talks and they had the longer talks. So the two-minute talks were kind of like, you know, very focused topics. This is   
42:54   
um something called crosscale agency and inner alignment. This is an I just like   
43:01   
this picture because it was like of a of an amphibian kind of had this c cartoon   
43:07   
cross-section but this is talking about agent scale action via the collective effects of microaggents   
43:14   
and action as an extension of autopolesis. So autocois is related to   
43:19   
action or action rather being an extension of this autopoetic regulation.   
43:25   
So you know there were a lot of course everyone was talking about these topics autooesis homeostasis alsostasis   
43:34   
regulatory control um complexity it's very much in in the theme of the   
43:42   
symposium. Now we get to Carl Friston's keynote   
43:47   
which was actually at the beginning of the of the symposium. Um this is his   
43:54   
so they always give the keynote main keynote to Carl Fristen obviously because he's sort of the person who   
44:00   
developed active inference uh as a tool or at least initially his talk was   
44:06   
active inference learning and selection and this talk offers a formal account of   
44:12   
insight and learning in terms of acting basian inference.   
44:17   
So active inference is basian in nature at least according to Carl. It deals   
44:22   
with the dual problem of inferring states of the world and learning its statistical structure. In contrast to   
44:29   
current trends in machine learning with deep learning we focus on how agents learn from a small number of ambiguous   
44:35   
outcomes to form insight. The presentation uses simulations of abstract rule learning and approximate   
44:42   
basian inference to show that minimized or expected free energy leads to active   
44:48   
sampling of novel contingencies. This epistemic curiositydriven directed   
44:54   
behavior closes explanatory gaps in knowledge and the causal structure of the world thereby reducing ignorance in   
45:02   
addition to resolving uncertainty about states of the known world. We then move from inference to model selection or   
45:08   
structure learning to show how abductive processes. So this is abduction in the   
45:14   
logical sense in the reasoning sense. Um and this is not like deduction or you   
45:21   
know this is abduction which is where you're kind of coming up with new solutions emerge when agents test plausible   
45:28   
hypotheses about symmetries in their generative models of the world. The   
45:33   
ensuing basian model reduction evokes mechanisms associated with sleep and has   
45:39   
all the hallmarks of all so-called aha moments. We talked about this in past   
45:44   
meetings and we actually mentioned aha moments in our paper on old models   
45:50   
regulator theorem. I just wanted to revisit that because I thought it was interesting. We conclude with the   
45:56   
generalization of active model selection to simulate artificial reasoning. So this talk was about looking at   
46:03   
artificial intelligence going back to sort of the statistical roots of in   
46:08   
physics of artificial intelligence statistical physics and then moving forward to modern models.   
46:16   
So this he introduces the three faces of what he calls AI physics which was   
46:21   
Herman von Helmholtz, Richard Fineman and Edwin James. So Richard Feman of   
46:28   
course everyone knows who he is. Jon Helmholtz developed a lot of the early   
46:34   
um like things like Boltzman machines and things like that or what would become called Boltzman machines. And   
46:40   
then James had this interesting relationship with uh what they call surprisal or what   
46:48   
you might call entropy or some people call neg entropy which is   
46:54   
looking at sort of entropy you know informationational entropy   
47:00   
and how that relates to thermodynamics. It's a very interesting like uh bit of work. I actually did uh work through   
47:08   
some of James James's work in a in a talk that we had. It was like two years   
47:13   
ago. Um it was on the stuff we did for thermodynamics and   
47:19   
um and learning. And so I'll have to pull that back up next week. But for now,   
47:25   
just remember that James is an important person. Fineman's an important person. And von Helmholtz is an important   
47:32   
person. Uh so von Helmholtz has this quote associated with him. Objects are always   
47:39   
imagined as being present in the field of vision as would have to be be there   
47:45   
in order to produce the same impression on the nervous mechanism. So this is   
47:50   
talking about um sort of visual perception and you know thinking about   
47:56   
um you know visual neuroscience and sort of these statistical what would   
48:03   
become sort of known as the psychopysics of visual neuroscience but also then   
48:10   
leading to this sort of statistical uh interpretation of visual uh perception   
48:18   
And this of course would contribute greatly to our current AI models.   
48:24   
And so we have uh in this figure we have Richard Gregory but at the bottom we   
48:29   
have Thomas Baze. So we have Baze theorem. We have the Helm Holtz machine in the basian brain. We have Jeff Hinton   
48:37   
and Peter Diane who are Jeff Hinton of course develop deep learning. Peter Dian   
48:43   
is a computational neuroscientist. And then Richard Fineman over here who developed some of the stat mech for   
48:49   
this. So here's Richard Fineman. Here's Judea Pearl. Uh and so you know Judea Pearl of   
48:58   
course is a causality person. And so this is kind of thinking about the basian brain. This is a model of the   
49:05   
basian brain where you have sensory states encoded in S dot the brain   
49:10   
encoded in mu dot with internal states. Then active states with a dot and then   
49:17   
there's a feedback between active states and internal states and then active   
49:22   
states produce external states or uh ada dot and then external states contribute   
49:29   
to sensory states which then contribute to internal states. So you have these two feedback loops um that culminate in   
49:37   
internal states producing active states being influenced by sensory states and   
49:42   
then external states or the action is uh or the active state contributes to the   
49:48   
external state which is the action outcome and then that shapes the sensory   
49:53   
state. So that's kind of the idea here and there are a whole bunch of equations in there that sort of allow us to model   
50:01   
that mathematically. Then there's the variational free energy   
50:06   
and that leads us to the paths of least action. So we have statements for action   
50:11   
and perception. For action we're trying to minimize action. For perception we're trying to   
50:19   
minimize the sensory states. For action we're trying to minimize the active states. So that's how that works. And   
50:26   
then we have the markoff blanket which leads the free energy principle the path integral formulation that leads us to   
50:33   
active predictive coding and of course we have to understand causality in that is we have to understand what causes   
50:39   
what. Okay. So this is uh   
50:45   
this is just kind of like working through the math here. there's expected free energy variational free energy   
50:53   
and then that's so that's that model we introduced that then the Helmholtz decomposition   
50:59   
which contribute to gradient flows and predictive coding so this is just kind of thinking about some of the math that   
51:05   
Helmholtz introduced that allow us to minimize prediction error and produce   
51:11   
this percept of this howling wolf in this case from an image that's sort of   
51:16   
grainy and noisy and so We can take that prediction error and minimize it and come up with a conscious percept.   
51:25   
Sometimes we can do this improperly which is sort of the minimization aspect of this. You minimize your error.   
51:32   
Sometimes you converge upon something that's wrong. So we could say that the wolf is you know this isn't a wolf, this   
51:38   
is a dog or this is a some it could be a rabbit even if it's like ambiguous.   
51:46   
Uh then this is Jean's work. So this is the free energy and maximum path entropy principle. So this is Jean's work on ent   
51:53   
minimum entropy production the minimum entropy production principle which was   
51:59   
published in the annual area of physical chemistry. Um where he talks about   
52:05   
maximum path entropy being equivalent to minimum entropy production. So this is   
52:11   
where you have this path entropy of the different paths taken through the world and then that leads to minimum entropy   
52:18   
production which is this sort of maximization of of efficiency the   
52:24   
minimization of error and so this is uh the equation that he states here there's   
52:29   
this constraint there's path entropy and then that's greater than the entropy of   
52:35   
the system. So this is all to say that this is useful in looking at um free   
52:42   
energy minimization and it also relates to optimizing uh deep learning networks   
52:49   
because remember deep learning networks are about minimizing your error and   
52:55   
doing this on a gradient. So we have the similar mathematics who have a similar sort of statistical structure.   
53:03   
I don't know if people tuned into the live stream. Uh it was definitely an interesting time and any comments or   
53:10   
questions about this stuff or anything you maybe wanted me to cover and I   
53:15   
didn't. No, that was great. Thanks for the summary. Um I I didn't get to catch all   
53:24   
the all the talks, but um I did catch you live. Oh, yeah.   
53:32   
So that that was Yeah, unfortunately it was just a super   
53:38   
busy week. Oh yeah. Um and uh yeah, so good stuff. And um uh   
53:48   
how about or you know like um I'm always I'm always wondering if there's um you   
53:55   
know I mean I take it Chris Smith didn't speak.   
54:00   
Uh you don't. No no sorry not what am I saying? It's not it's not   
54:07   
um I mean you know so Carl was talking   
54:13   
about very active inference things not necessarily in relation to computational   
54:18   
psychiatry directly. Yeah.   
54:25   
Yeah. I'll I'll look up the guy's name in just a minute. Okay.   
54:30   
Sorry. I'm I'm actually just heading to the store here. Okay. Yeah, that's fine. Um yeah, so I   
54:36   
mean there were I guess whoever kind of wanted to give a talk gave a talk like   
54:41   
there were people in the active inference community weren't there obviously but um you know I don't think   
54:48   
uh person I think you're talking about gave a talk there. Um and you know a lot   
54:55   
of it was sort of application driven like you know   
55:00   
software platform driven you know there were some of course like Carl's talk was   
55:05   
very theoretical and talking about sort of the roots of some of these concepts   
55:10   
and so yeah I don't know I don't think I don't think the person you were thinking of was there   
55:16   
yeah um uh you know to register they   
55:21   
like what you're what you're coming for, what you heard before. Yeah.   
55:26   
And I'm always hoping for, you know, tutorials.   
55:32   
Yeah. So, they did have some of that. It was like there was a a person named   
55:38   
Octopus who did a tutorial on sort of mathematics and mathematical learning.   
55:44   
wasn't really an active inference deep active inference tutorial, but um there   
55:50   
were some other uh they didn't really have any like tutorials where you could follow along otherwise. It was kind of   
55:57   
like just showing the software and showing what things can be done. Um you   
56:03   
know, it's it's something it's hard to kind of pull off in a live environment. So,   
56:10   
but yeah, I would like to see that too. Totally. I mean, you know, like I get that too.   
56:18   
Yeah. So, yeah. I don't know if uh Jesse, where are you?   
56:29   
Okay. I appreciate the recap as well. It's been a a surprisingly busy week. So,   
56:37   
I was curious about um   
56:43   
Adam's presentation, I suppose. Yeah, I was able to I think we talked about it a little bit last week, but I didn't get   
56:52   
which which day was he on? Um I think he was on day two.   
56:58   
Okay. Um, yeah, it's   
57:05   
it's so interesting. I feel like I'm always on the cusp of sort of conversations with her around his   
57:11   
material. So, I might look more into that as well.   
57:16   
Yeah. I think it was day three now that I think about it.   
57:22   
But yeah, it's like they he did the the well, it was all AI safety work. The   
57:27   
first talk was on the stuff uh with Arena Rich that was on world models and   
57:34   
I assume that's going to be the old model special issue. And then the other   
57:39   
one was this stuff on a life simulations the simulations for the the agents that   
57:46   
interosceptive. This is this I think this follows up   
57:52   
from last week where we kind of talked about some things about productivity and   
57:57   
AI agents and that maybe it was a couple weeks ago but this is an interesting article from Ethan Mollik.   
58:05   
This is real a AI agents and real work and I think it's off of Substack or something.   
58:11   
This is the race between human- centered work and infinite powerpoints. So this is about kind of the thing about you   
58:18   
know replacing workers with agents and things like that. And I think he's done   
58:24   
like I think he did we did another article that he wrote about similar to this on thinking about you know how easy   
58:32   
is it to replace uh people with agents. So that's kind of the idea here. He   
58:38   
starts off saying AIs have quietly crossed the threshold. They can now perform real economically relevant work.   
58:45   
Last week, OpenAI released a new test of AI ability, but this one differs from the usual benchmarks built around math   
58:51   
or trivia. For this test, Open AAI gathered experts with an average of 14   
58:56   
years of experience in industries ranging from finance to law to retail   
59:03   
and had them design realistic tasks that would take a human expert an average of four to seven hours to complete. You can   
59:11   
see all the tasks here. has a link to the task list. Open AAI then had both AI   
59:17   
and other experts do the tasks themselves. A third group of experts graded the results not knowing which   
59:24   
answers came from the AI and which came from the human. A process which took about an hour per question.   
59:31   
said this is an example task um where you have um   
59:38   
you know these different types of uh tasks for doing business strategy and   
59:44   
other things like that. Um I think these are all legal uh topics. So they have a   
59:52   
parallegal or a lawyer preparing sort of um you know different   
59:58   
types of forums and things like that. And so that's and then they had the AI   
1:00:03   
do it. And so um then they had these different AI models ranging from GPT40,   
1:00:11   
the Gro 4 to Gemini 2.5 Pro, the 04 mini high model, the 03 high to GPT5 high,   
1:00:19   
the quad opus 4.1 with their win rate percentage and then uh this is 50% is   
1:00:27   
parody with industry expert. So quad opus is coming close to the industry   
1:00:33   
expert. Um and then that's like in the best case scenario. Um so human experts one but   
1:00:40   
barely and the margins vary dramatically by industry. Yet AI is improving fast   
1:00:46   
with the more recent AI models scoring much higher than older ones. Interestingly, the major reason for AI   
1:00:51   
losing to humans was not hallucinations and errors, but a failure to format results well or follow instructions   
1:00:59   
exactly. Areas of rapid improvement. If the current patterns hold, the next   
1:01:04   
generation of AI model should beat human experts on average in this test. Does that mean that AI is ready to replace   
1:01:11   
human jobs? So, we have all these different things that they tested. Um, so human experts versus models. Uh first   
1:01:19   
line supervisors or production operating workers, industrial engineers,   
1:01:24   
mechanical engineers, shipping receiving and inventory clerks, software developers, project management   
1:01:30   
specialists, lawyers, computer and information systems managers, financial   
1:01:36   
managers, medical secretaries, nurse practitioners, registered nurses,   
1:01:42   
personal financial advisors, securities, commodities and financial services, sales agents. So these are all   
1:01:49   
kind of just various. Some of them are, you know, the models are above human expert level. Some of them don't really   
1:01:56   
meet anywhere close to human expert level. And then the tests that they give them are, you know, I don't know how   
1:02:02   
accurate those reflect the actual day-to-day work that the uh these   
1:02:08   
professionals are doing, but they gave in in any case this is the benchmark.   
1:02:14   
Uh so his conclusion here is that no at least not soon in terms of AI replacing   
1:02:21   
human experts. No, at least not soon because what was being measured was not jobs but tasks. So this is what I mean   
1:02:29   
that you know you have these benchmarks they don't necessarily reflect the   
1:02:34   
day-to-day work that the experts are doing or the knowledge that they're employing. So our job consists of many   
1:02:42   
tasks. My job as a professor is not just one thing. It involves teaching,   
1:02:48   
researching, writing, filling out annual reports, supporting my students,   
1:02:53   
reading, administrative work, and more. AI doing one or more of these tasks does not replace my entire job, which shifts   
1:03:01   
what I do. And it cannot substitute for all the complexities of my job. I gave   
1:03:06   
the new Claude Sonnet 4.5 to which I had early access, the text of a sophisticated economics paper involving   
1:03:14   
a number of experiments along with the archive of all the replication data. I   
1:03:19   
did not do anything other than give Claude the files and prompts uh that say replicate the findings in   
1:03:26   
this paper from the data set they uploaded. You need to do this yourself. If you cannot attempt a full   
1:03:31   
replication, do what you can. And because it involved complex statistics,   
1:03:36   
I asked to go further. Can you also replicate the foreigner actions as much as possible? And so this is where you   
1:03:43   
know you're getting into these uh sort of how Claude is going to sort of   
1:03:51   
reproduce this task. So without further instruction, Claude read the paper,   
1:03:56   
opened up the archive, sorted through the files, converted the statistical code from one language STA to another   
1:04:03   
Python, and methodically went through all the findings before reporting a successful reproduction.   
1:04:10   
I spot checked the results and had another AI model, GPT5 Pro, reproduce   
1:04:15   
the reproduction. It all checked out. I tried this on several other papers with similarly good results, though some were   
1:04:22   
inaccessible due to the file size limitations or issues with the replication data provided. Doing this   
1:04:29   
manually would have taken many hours. So, it cuts down on the amount of time doing these kind of tasks and it's   
1:04:37   
accuracy is pretty decent. But the revolutionary part is not how I saved a lot of time. It it is that a crisis that   
1:04:45   
has shaken entire academic fields could be partially resolved with reproduction. But doing so requires painstaking   
1:04:52   
expensive human effort that was impossible to do at scale. Now it appears that AI could check many   
1:04:58   
published papers, reproducing results with implications for all scientific   
1:05:03   
research. There are still barriers to doing this, including benchmarking for accuracy and fairness, but it is now a   
1:05:11   
real possibility. Reproducing research may be an AI task, not a job. But it might also change an   
1:05:17   
entire field of human endeavor dramatically. So this is possible because AI agents   
1:05:23   
have gotten much better very quickly. So again, agents uh generative AI agents have helped a   
1:05:31   
lot of people do tasks since the original chat GPT, but the limit was always a human user. AI makes mistakes   
1:05:38   
and errors. So without a human guiding and on each step nothing valuable could be accomplished. The dream of autonomous   
1:05:45   
AI agents which when given a task and plan and use tools to accomplishment   
1:05:51   
seem far away. After all AI makes mistakes. So one failure in a long chain of steps that an agent has to follow to   
1:05:59   
accomplish a task would result in overall failure. So however this is not   
1:06:04   
how things worked out and another new paper explains why. We'll get to that paper in a minute here. Uh it turns out   
1:06:11   
that most of our assumptions about AI agents were wrong. Even small increases in accuracy, the new models are much   
1:06:18   
less prone to errors leads to huge increases in the number of tasks an AI   
1:06:23   
can do. And the biggest and latest thinking models are actually self-correcting so they don't get   
1:06:29   
stopped by errors. All this means that AI agents can accomplish far more steps   
1:06:35   
than they could before and can use tools which basically include anything your computer can do without substantial   
1:06:41   
human intervention. So this is a a graphic that says four reasons why agents are suddenly viable.   
1:06:50   
The first is that thinking enables execution of long tasks in a single   
1:06:55   
turn. So this is task length in terms of steps and then these are the different   
1:07:00   
models. So GPT5 has a task length of like a thousand uh   
1:07:06   
steps. Uh quad for sonic goes up to like 470 or 480.   
1:07:13   
Rock 4 goes up to like 250 and then you see a decrease over the   
1:07:19   
other models. But you have this sort of uh I guess there's no thinking chain of   
1:07:25   
thought and thinking. Um so this is kind of showing this uh how this is   
1:07:31   
implemented in different models. The second point is that larger models are more prone to self conditioning. So   
1:07:38   
you have this turn 100 accuracy on the y- axis number of parameters. As you go   
1:07:44   
up to 32 billion parameters, you have zero errors in context in green and 50%   
1:07:51   
errors in context in pink. And you can see that there are these fewer errors in   
1:07:57   
context in the 32 billion parameter model and more 0% errors in context for   
1:08:03   
that 32 billion parameter model. So as you're increasing the number of parameters, your zero errors in context   
1:08:09   
is going up and then your your 50% errors in context is also going up from   
1:08:15   
the 4 billion to the 14 par 14 billion parameter model but for the 32 billion   
1:08:20   
parameter model. Um and then the third point is diminishing gains on a single step and   
1:08:28   
lead exponential gains over a long horizon. And then this shows kind of uh   
1:08:33   
step accuracy for model release dates. So step accuracy is improving and then task   
1:08:40   
length is also improving dramatically uh over time. And then the fourth thing   
1:08:46   
is scaling model size enables the execution of all the tasks.   
1:08:52   
So we have task length on the y- axis, model size in terms of billions of   
1:08:57   
parameters on x- axis and as we move towards 30 million parameter models, our   
1:09:03   
task lengths get longer and so they can encode more steps and they can do   
1:09:08   
selfcorrection and things like that. So this is uh interesting that the one   
1:09:15   
of the few measures of AI ability that covers the full range of AI models in the past few years from GPT3 to GPT5   
1:09:24   
is met test uh which is from measuring AI ability to complete long tasks. Meet   
1:09:31   
test of the length of tasks that AI can accomplish alone with at least 50% accuracy.   
1:09:37   
The exponential gains from GPT3 to GPT5 are very consistent over five years   
1:09:42   
showing the ongoing improvement in agentive work. So this is a time horizon. This is for   
1:09:49   
meta uh time horizon of software engineering tasks differ for different   
1:09:55   
large language models can complete 50% of the time. So this is where you know   
1:10:00   
you can look at sort of these this time. So the task duration for humans in terms   
1:10:08   
of time, you know, 30 minutes to two hours and then these specific tasks and context. So for example, training a   
1:10:15   
classifier will take a human just under an hour or finding a fact on the web   
1:10:20   
will take about 10 minutes or fixing a bug in a small Python library will take   
1:10:25   
around an hour and 15 minutes. Scraping a record from a website with anti-bot protection will take about an hour and   
1:10:32   
45 minutes. So those are the human tasks and their duration.   
1:10:38   
And then you know this the idea is is that this green dotted line is sort of   
1:10:44   
the uh trend for all these models and the models here uh the sort of the dots   
1:10:52   
for these models shows when the model can perform that task correctly at least   
1:10:59   
50% of the time. So you see that as over time that goes   
1:11:06   
from almost no human tasks to these tasks that take a little over two hours.   
1:11:13   
And again, these are all kind of, you know, I mean, we can do a lot of things like in two hours and it's like that's   
1:11:19   
kind of a arbitrary time window because you could write a song in two hours or   
1:11:24   
you could write a song in 10 hours or you could u do a lot of things in two hours that aren't say as computationally   
1:11:33   
easy as some of the tasks they list here. They're really listing things like   
1:11:38   
you know code maintenance or something like that. So this is not like um you   
1:11:44   
know and then like finding a fact on the web is interesting because like finding a fact is pretty subjective depending on   
1:11:51   
the fact and depending on the person I guess. Um so yeah this is all kind of   
1:11:56   
just giving some context to these these these benchmarks. But again the point is   
1:12:03   
is that it's these models are performing better in terms of like being able to do   
1:12:09   
complex tasks. What's this say? Uh, they had some activity here in the   
1:12:15   
Oh, I'm just writing the chat. Oh, yeah. When you're done, I'm just making notes.   
1:12:21   
Yeah. Oh, yeah. All right. Yeah. So, um,   
1:12:26   
so back to this. So, how to use AI to do economically valuable things? So,   
1:12:31   
agents, however, don't have true agency in the human sense. For now, we need to decide what to do with them, and that   
1:12:37   
will determine a lot about the future of work. The risk everyone focuses on is using AI to replace human labor. And it   
1:12:45   
is not hard to see this becoming a major concern in the coming years, especially for unimaginative organizations that   
1:12:51   
focused on cost cutting rather than using these new capabilities to expand or transform them. So yesterday I gave   
1:12:57   
my a or my um term you know um every term I give a   
1:13:04   
automation lecture my project management course and he was making this point yesterday quite uh forcefully that um   
1:13:12   
you know there is this aspect of how do you how do you implement automation? Do you implement it just for cost cutting   
1:13:19   
or do you implement it for some sort of synergy or you know really thinking about what you can get productively out   
1:13:25   
of automation and a lot of times when you just implement it for cost cutting you end up spending more later to fix   
1:13:32   
things or to um kind of figure out exactly the use case you want. So, you   
1:13:40   
know, this is an an important point about like replacing people with machines is that with with AI systems or   
1:13:47   
with other types of automation, sometimes we just kind of throw automation at problems and try to hope   
1:13:53   
they go away. And that's not the way that, you know, responsible organization   
1:13:58   
should behave. Um, oh, go ahead.   
1:14:03   
I was gonna say, yeah, technical. Oh, yeah. Yeah.   
1:14:09   
everything you were saying. Yeah, it's fine.   
1:14:17   
Sorry. But there is a second very likely risk   
1:14:23   
about using AI at work, using agents to do more of the tasks we do now   
1:14:28   
unthinkingly. So as a preview of this particular nightmare, I gave Claude a corporate   
1:14:33   
memo and asked it to turn it into a PowerPoint and then another PowerPoint from a different perspective and another   
1:14:39   
one. So the nightmare of the poweroints basically which is bad enough but then when an AI is doing it, you know. Uh   
1:14:47   
so this is the PowerPoint mega collection. Um you now have 17 unique PowerPoint   
1:14:53   
presentations covering every angle of your AI transformation strategy. Um, this is   
1:14:59   
your complete PowerPoint arsson. It sounds like hell. So, you get 17 different powerpoints   
1:15:06   
using the strategy. All different sorts of like corporate speak and, you know,   
1:15:12   
different types of takes on it. This just too many powerpoints. So, this is like, you know, I want a take on X or   
1:15:20   
something, Blue Sky, and I get it because, not because I asked for it, but   
1:15:25   
because people are offering it to me. Um, so yeah, this is uh not really we   
1:15:31   
don't want all of this sort of stuff. We just want like maybe one thing or you   
1:15:38   
know we don't want to waste a lot of time or we don't want to generate a lot of things that are kind of mindless. So   
1:15:44   
if we don't think hard about why we are doing work and what work we should look like, what work should look like, we are   
1:15:51   
going to drown in a wave vape AI content. What is the alternative? The open AI paper suggests that suggested   
1:15:59   
that experts can work with AI to solve problems by delegating a task to an AI   
1:16:04   
as a first pass and reviewing the work. If it isn't good enough, they should try a couple of attempts to give corrections   
1:16:11   
or better instructions. If that doesn't work, they should just do the work themselves.   
1:16:17   
Experts follow this workflow. The paper estimates that they would get work done 40% faster and 60% cheaper. Even more   
1:16:25   
importantly, retain control over the AI agents are here. They can do real work.   
1:16:31   
It is valuable in increasing. But the same technology that can replicate academic papers in minutes can also   
1:16:38   
generate 17 versions of a PowerPoint deck that nobody needs. The difference between these features isn't in the AI,   
1:16:46   
it's how we choose to use it. By using our judgment and deciding what's worth doing, not just what can be done, we can   
1:16:52   
ensure these tools make us more capable, not just more productive. Okay, so that's good. Now, I have a a   
1:17:00   
bone to pick about this particular point about the powerpoints is that nominally   
1:17:06   
we don't need 17 versions of a PowerPoint book. We also don't need a thousand different tapes on some event.   
1:17:14   
But um the thing is is that that's cultural production. That's actually   
1:17:19   
what we get when we have something that happens and then we want to know what   
1:17:24   
the meaning is or we want to have like some we want to kind of uh say we want   
1:17:29   
to measure the zeitgeist of of a recent event. So we go to social media to do it. So we go we look at the different   
1:17:35   
posts and we get a collection of uh posts and they're all different. Sometimes they converge upon the same   
1:17:42   
take. You know, I think you know what I'm getting at. And then, you know, sometimes they're different takes. We   
1:17:47   
could classify those takes into like, you know, different ways of viewing an event, different things people focus on,   
1:17:54   
etc., etc. And that's like kind of part of cultural production. I mean, we kind   
1:18:00   
of unpack things and there's multiple layers of meaning with things. And you   
1:18:05   
know when you see 17 versions of a PowerPoint deck you think well that's overwork and that's over production but   
1:18:12   
really that's important for sort of cultural production because you know you   
1:18:18   
need to have all those different takes to fully understand the significance of something and so maybe not in terms of   
1:18:25   
producing overproducing PowerPoint decks which no one needs but in producing   
1:18:30   
different takes on things or different perspectives on things that could be useful. So, I just wanted to make that   
1:18:37   
point. Um, it's not directly relevant to the comparison he was making, but it is   
1:18:43   
worth thinking. And so, this is uh a paper   
1:18:48   
from the archive. And this is uh so this is the illusion   
1:18:56   
of diminishing returns measuring long horizon execution in large language models. Um and then this is uh about   
1:19:04   
this problem of sort of the illusion of diminishing returns. So you know   
1:19:10   
thinking about the returns that large language models yield. So does continued   
1:19:16   
scaling of large language models yield diminishing returns? In this work we show that short task benchmarks may give   
1:19:23   
an illusion of slower progress even as marginal gains and singlestep accuracy can compound into exponential   
1:19:29   
improvements. and the lengths of tasks a model can successfully complete.   
1:19:35   
Then we argue that failures in large language models and simple tasks are made longer arise from mistakes in   
1:19:42   
execution rather than an inability to reason. So this is where again you have   
1:19:47   
this you know the more steps you have the more executable steps you have the   
1:19:53   
more you can correct your mistakes and so forth. So it's not that exe the   
1:19:58   
models are making execution mistakes or they're making execution mistakes rather   
1:20:03   
than this inability to reason. So we propose isolating execution capability   
1:20:10   
by explicitly providing the knowledge and plan needed to solve long horizon   
1:20:15   
tasks. So there's of course the knowledge in the plan. First, we find   
1:20:20   
that larger models can correctly execute significantly more turns even when small models have nearperfect single turn   
1:20:27   
accuracy. We then observe that the per step accuracy of models degrades as the   
1:20:33   
number of steps increases. This is not just due to long context limitations.   
1:20:38   
Curiously, we observe a selfconditional effect. Models become more likely to make mistakes when the context contains   
1:20:45   
their errors for prior terms or from prior terms. Self conditioning does not reduce just   
1:20:51   
by scaling the model size. But we find that thinking mitigates self   
1:20:57   
conditioning and also enables execution of much longer tasks in a single term.   
1:21:03   
We conclude by benchmarking frontier thinking models. So they're now focusing   
1:21:08   
thinking models on the length of tasks tasks they can execute in a single term.   
1:21:15   
Overall, by focusing on the ability to execute, we hope to reconcile debates on how large language models can solve   
1:21:22   
complex reasoning problems yet fail at simple tasks when made longer and   
1:21:27   
highlight the massive benefits of scaling model size, sequential test time compute for long horizon tasks. So, this   
1:21:35   
just kind of underscores what Ethan was talking about in the article. Pop out   
1:21:41   
this, you know, this uh number of steps that the model can take   
1:21:48   
and the kinds of tasks that it's asked to perform, things like that.   
1:21:54   
Is this my end or is it still kind of the same? Uh, it's it's I think it's okay.   
1:22:02   
Okay. I'm trying to just have it feel less crazy. Um, no, that's   
1:22:09   
good. That makes a point a few points. Um,   
1:22:15   
I I find a lot of this to   
1:22:22   
kind of frustrating like like just just to have a a straight reaction. It It's so   
1:22:28   
I feel like I'm just inundated all the time with people talking about how it has affected work, it has affected work,   
1:22:35   
is affecting this. And I find it to be like that. Like there's there's   
1:22:41   
I'm discontented at the lack of sort of what's what's around   
1:22:47   
the corner from it because always it always seems to me well you know the models are getting better at this and   
1:22:53   
there's no real and and I think there are I just you   
1:22:59   
know some of it's not centered and it's not as popular I suppose but really just like what what   
1:23:07   
you know if if the models do get better like how does that actually change the structure of things and I don't need to   
1:23:12   
go into full like UBI talk or or all that kind of stuff but   
1:23:18   
it it strikes me as sort of   
1:23:23   
there has to be a certain kind of um   
1:23:29   
disingenuousness to it that it feels and this this not the commentary on the Ethan's post and   
1:23:36   
Ethan's you know intention and job is to report on the models development and I think that's exactly what he should be   
1:23:43   
doing. But there's sort of this um and this this is this is a preview   
1:23:48   
segue into other things where it's just you know I find there to be and maybe   
1:23:55   
I'm I'm identifying the niche I want to do more and doing more and want to speak more about and just my complaining about   
1:24:02   
stuff. But I find it very interesting like yes like models are getting better at this specific thing. Um, and I don't   
1:24:11   
think it's just a matter of unoriginal workplaces. I think yes, like the the the implementation right now. Why many   
1:24:18   
people's jobs have already been affected by this is because oh, you know, let's cut costs and if I can pay this for that   
1:24:26   
and have this thing, you know, there's this meta   
1:24:33   
from from the workplace down to, you know, content consumption. there's sort   
1:24:38   
of the the lacking of it's just sort of the results. Well, if I get the thing that looks like the   
1:24:44   
success, then it's that is then it's success then it's the results that it's work and it's labor. Um, and so I don't   
1:24:52   
know that's that's that's my initial raw reaction. I think it's the least helpful thing to really   
1:24:59   
comment on. But I do think I'm I'm very fascinated to see, you know, where where   
1:25:07   
is the where are the transitions? Where where where is the real talk about okay,   
1:25:16   
you know, um models are better and what   
1:25:22   
is what is on the other side of that? Um At any rate, um I would definitely be   
1:25:29   
interested in the automation portion of the lecture you mentioned. I know you   
1:25:34   
periodically do that and and that's something that's and even even   
1:25:42   
the sense of um one of the speaking or or panels I'm   
1:25:50   
I don't know facilitating running next year which is only a few months away I suppose in one way believe it or not um   
1:25:59   
I would really I would appreciate being able to sort of convey your I would invite you to sort of be a   
1:26:06   
part of that directly but I think it will be very in person and we'll see if there's a way to do it but like basically what you know as in the   
1:26:13   
context of that panel it would be you're a young person and how do you deal with   
1:26:18   
implementing augmentation that you likely experience in your job because that's I mean what the point of that   
1:26:25   
panel is essentially going to be how do you deal with Gen AI and automation and   
1:26:32   
you know, as an early career worker, researcher, individual. Um, so   
1:26:38   
that's certainly that's certainly a what are we doing   
1:26:43   
about it? Um, and what can we do and what should people be informed of type of thing. Um,   
1:26:53   
and I'm just looking back in the chat here. There's the notes that I made.   
1:27:00   
You mentioned something about fact finding according to which person I think yeah like the reality is now we're   
1:27:06   
in it's kind of a I think we all here are   
1:27:12   
familiar with sort of the personal data versus the notion of AI safety and you   
1:27:19   
and that came from and and and so on and I feel like I'm not sure people really   
1:27:27   
maybe they have Um, but I think we're kind of very close to not just personal   
1:27:32   
universe. You can have your personal social media platform which also now   
1:27:41   
like I'm alluding to Elon Musk and Twitter and Gro Groedia   
1:27:47   
and like you can have a platform and your truth your AI enhanced uh your AI   
1:27:54   
you know generated or refined or revised or your particular tent or whatever for   
1:28:01   
your knowledge base too like it's it's social knowledge base um and community   
1:28:08   
and and perhaps an interesting segue to other things that I'm going to talk about um   
1:28:17   
I what I will what I will cover or move towards and feel free to jump in   
1:28:24   
or study whatever and then do uh   
1:28:29   
I'm going to talk a little bit about my exploration into better language   
1:28:35   
um as a concept as a as sort of a domain agnostic concept my particular take on   
1:28:42   
language models and artifacts which is this thing I've been saying for years and trying to codify and put into a   
1:28:49   
specific paper um eventually and and then I'll move I'll move towards   
1:28:55   
words. That's sort of a a background thing, but then I have a I'll share this. I'll I'll go through this um   
1:29:05   
this I I had sort of a post that I I pushed like type things and it   
1:29:12   
just goes through a whole bunch of stuff that's been happening this week and kind of in and around the community. Um and   
1:29:19   
it's it's literally just an immediate snapshot because there was so much I just wanted to to mention from the   
1:29:26   
complexity group to um ecological stuff to clusters to um local things on   
1:29:35   
governance and and nation stuff to moral ambition to some stuff happening in San   
1:29:43   
Francisco and I mentioned friendship because there's so much stuff happening there I don't comment I just threw it in   
1:29:48   
So, I'm sure Morgan has things to say about her Tower at some point. Um, and then also, yeah, like um a little bit a   
1:29:57   
little bit for next year because it's already kind of that transition into the   
1:30:02   
next year for all this stuff. So, before I get into that post, um, is there anything?   
1:30:11   
Um, I guess I'll be on camera.   
1:30:18   
Yeah, I'm still I'm in my magic background of awesome. How about that?   
1:30:24   
Um, any any comments or questions before I kind of go into pattern language stuff?   
1:30:31   
No, no, I think that's good. Yeah, I think that's you know, I don't know if you were here when we were talking about   
1:30:38   
the AI work uh talk at the active inference symposium. He's basically   
1:30:45   
going through Yeah. So maybe we'll you can review that in the recording or the   
1:30:51   
talk I think is in one of the the days of the uh symposium where he basically goes over sort of the economics of AI   
1:31:00   
and and work and all that. So that's interesting. Um yeah, but I think that   
1:31:06   
that's again, you know, something that it's kind of like that futurism take, right? It's like we have the futurism   
1:31:13   
take where people try to predict things in the future and a lot of times they get them mildly wrong because   
1:31:20   
you know that's just the nature of futurism. But how do you make those predictions more accurate?   
1:31:28   
Yeah. And and I think I keep I have this internal urge to put   
1:31:34   
a bow on what I said and make it a little more coherent and and less just I   
1:31:39   
don't know less complainy about stuff. But um I I find   
1:31:47   
I find there to be a certain um   
1:31:56   
awkward discreetness about some of the conversations. Um and again it's not   
1:32:02   
anything wrong with article discussion. It's more   
1:32:08   
the stopping point seems strange. So, let's move on. I said to   
1:32:14   
get away from it because it's gonna it's gonna it's gonna need some time in trivia. Um, so let's talk about pattern   
1:32:20   
language for a little bit. Um, I will attempt to   
1:32:26   
Yeah, I'll go through   
1:32:32   
I think I mentioned this at the very end of last um   
1:32:39   
last week. Uh, sure. This update is an   
1:32:44   
operating system. I think all the permissions are like to do this again.   
1:32:52   
Okay. Um   
1:32:57   
maybe maybe Yeah, let me try this way. Hold on one second. Sorry.   
1:33:04   
Yeah, let's do let's do separate. Um   
1:33:12   
here sure I think this will work. Yeah.   
1:33:18   
Okay. Wikipedia was just a dream.   
1:33:26   
Um, in terms of pattern language,   
1:33:32   
um, a little personal background of pattern language is that something that came up through plot twistes, which I'll   
1:33:38   
mention again in a minute. We've we had our first play   
1:33:43   
test exession and it went really well. Uh, but more about that soon. Um,   
1:33:50   
pattern language is something that I learned about through plot twisters, which is kind of co-ounded by a couple   
1:33:55   
people heavily based in design. Um, and and you know, design status Jenny Jenny   
1:34:03   
Lang is a designer on many fronts and a wonderful   
1:34:09   
uh visionary and designer. And I think I think I first got the idea of it through   
1:34:16   
her and some of her um   
1:34:23   
thesis writing type type things and and eventually plot versus proper and   
1:34:30   
I I kind of got it and appreciated it. Now I I realize   
1:34:38   
um I'm I'm taking a deeper dive into it into both what it is, how it came about   
1:34:43   
and it's it's cross domain applications and it's sort of you know there's sort of this there's this interesting tension   
1:34:48   
I'm going to build to like pattern language versus kind of meta-cience or   
1:34:55   
um meta science almost as like top down   
1:35:01   
categorical um philosophy of science type stuff, but then pattern language as this generative   
1:35:09   
almost a gentle um and and and you know, one of one of the things somewhere   
1:35:16   
somewhere in in here um   
1:35:22   
uh yeah, somewhere in here there's a reference to um Alexander's   
1:35:31   
work. The work is essentially It's really it's really a series of books and and I do agree that you can't really   
1:35:39   
just it's not really this is the book that references a pattern language but it's it's also about the the the   
1:35:48   
first book in the series which is something about like the timeless way of building and that's that's that reads   
1:35:54   
quite a bit like a lot of um Zen Zen Zen con or Zen scriptures like   
1:36:03   
it lessons the art of leadership has a sort of um uh   
1:36:10   
poetic ways of discussing a variety of of topics and   
1:36:16   
in it in it he gets at um quite a quite an interesting set of   
1:36:24   
uh demarcating spaces within you can say a problem space and then different levels patterns   
1:36:33   
and and I wouldn't say topics like emergence like I don't I never I I don't think it's about emergence but the   
1:36:39   
concept of emergence is is quite quite embedded in it. Um also like things   
1:36:45   
related to the embodied mind. I don't know if if uh   
1:36:50   
Christopher Alexander was savvy to Varela and that whole thing there, but   
1:36:56   
it's it's quite related to a lot of the concepts of um   
1:37:03   
uh parts of Buddhist influence on the embodied mind and also sort of the sense   
1:37:10   
that the the Buddhist chen um stuff about it's not about a dogma, it's about   
1:37:16   
that which completes It's there's always a right answer. It's about doing the thing which makes the outcome as it   
1:37:23   
should be. Uh which is sort of you know um   
1:37:28   
related to this. So where am I going with all this stuff is like you know pattern pattern language has been very   
1:37:35   
influential on um uh programming. has been influential on   
1:37:41   
the curriculum design which is kind of what I'm looking at uh particularly recently is like papers on I can pull   
1:37:48   
one up easily here. Um, I have Can I get this window?   
1:37:56   
Um, here   
1:38:02   
I can do that. Oh, you can't. Well, I'll I'll let me just quickly   
1:38:09   
show this. Um,   
1:38:14   
this go to this.   
1:38:20   
Yeah. So, I've been looking a bit at things like this, which is like a pattern language for teacher education.   
1:38:26   
Um, can't I can't really manipulate this   
1:38:31   
part of it. Uh, but Just to to introduce this briefly, redesigning teacher   
1:38:37   
education programs at UIC have been unique. Bar from architecture. The framework has enabled college of   
1:38:44   
education faculty, art, science, public school administrators to reinvision teaching and teacher   
1:38:51   
education by developing a language of practice called a pattern language for teaching.   
1:38:57   
uh this identifies areas of knowledge necessary for teaching and specifies the process of structures for achieving the   
1:39:04   
goal. The concept of developing using a shared language um has allowed us to deliberate   
1:39:11   
uh about in about and incorporate goals across disciplines. So what they kind of   
1:39:16   
get at here is um   
1:39:21   
uh so this is sort of an interesting theoretical basis with you know the ability to draw upon and orchestrate   
1:39:27   
bodies of knowledge using context sensitive skills good teaching involves the mastery of language practice and all   
1:39:35   
this this sort of thing like that um and and to kind of I don't want to take too   
1:39:40   
deep a dive in this paper but you know it it is an interesting sort of   
1:39:46   
methodological approach to you know Alexander as I currently   
1:39:53   
understand him sees I think making a pattern language is also another way of essentially ma like   
1:40:00   
mapping a problem space but then that problem space these recurring   
1:40:06   
patterns or um patterns in which things happen which is   
1:40:12   
interesting emerge like like you know in the original pattern language versus a   
1:40:19   
pattern language was about architecture and and you know things of that a   
1:40:25   
building building real building that people would have it have you know like like a macro pack is like an isolated I   
1:40:32   
forget the term but like a a collection of you know a a broad neighborhood and then   
1:40:40   
you have like you know a house or like a bath and then a bath and a bathroom and it's like bigger patterns out of that   
1:40:45   
contain patterns of smaller, you know, thing. I I kind of think about if anybody's ever played like, you know,   
1:40:53   
one of the the sim games, it's like patterns. Pattern language is sort of   
1:40:58   
that which enables or like like in a Fallout 4, Town Crafter, if you go like, okay, I want to I want to build a   
1:41:04   
staircase. I want to build a pre-built a pre-fabricated house that you hear here   
1:41:09   
sort of like you go into that section of the menu and you pick that thing and you you you then you instantiate it and then   
1:41:16   
you implement it and then you make you know these adjacent things that that build it out. And so I think applying this to concepts   
1:41:24   
like is this sort of it's a very interesting way in sort of a frontier map type way to look at um you know how   
1:41:31   
do you take a a vague problem space how do you take a I put here about lean uh   
1:41:38   
yeah um there's an interesting sort of aential and economic component to to   
1:41:45   
these things because um they're essentially around what you   
1:41:52   
um what you're doing and the activity within that space. Um and and so what   
1:42:01   
does that mean for like making a pattern language around um   
1:42:06   
teaching? What does it mean about leading people through   
1:42:14   
de a demarcated space where you have you know   
1:42:19   
these these patterns that you've identified and then in in like the third   
1:42:26   
third book in the series and some of the later works like in this this particular University of Cincinnati take is like   
1:42:32   
the making a language and making a plant that's very organic and integrated into   
1:42:38   
other people contributing to that language, contributing to the patterns and contributing to the planet action   
1:42:43   
what to do or how to build or how to design the thing that you're doing. So   
1:42:49   
um this is an extremely a bridged um you know opening on pattern language but I'm   
1:42:55   
interested in it quite a bit because I think it can be a tool that will map   
1:43:00   
particularly in the constructionoriented sense which is kind of my my frustration from earlier things like okay how do we   
1:43:07   
talk better about doing the constructive work in different fields or across fields or in whatever 2025 or 2026 is I   
1:43:15   
think pattern language um as a way not just of of making a language of patterns   
1:43:22   
but as a implementation authoring or implementation tool for how   
1:43:28   
to instantiate certain things and make some coherent language around them. I   
1:43:33   
think that's a very um something that will keep coming up like and also you   
1:43:38   
know Bradley knows a little bit about this and I've spoken kind of vaguely about this in general but like I talk   
1:43:45   
about curriculum design I talk about um I talk about things like you know the   
1:43:50   
future journal this other stuff and and this elements of you know how do we we   
1:43:57   
build these these these new things in and you know is it a pattern language is is it a curriculum is it is it Is it   
1:44:03   
teaching? Is it education? Is it perspective? Um all of those things uh which are you   
1:44:09   
know um challenging and on the table. Um I know   
1:44:16   
we're kind of I might I might I might I might kind of have this be like a part   
1:44:22   
one or a part zero discussion that we talk about next time. um just to move on   
1:44:27   
to other things, but I'll I'll pause for any comments or questions before we basically go into   
1:44:33   
um to this the uh news broader updates from this   
1:44:40   
week. So um yeah, that's good. Um yeah, of   
1:44:45   
course, as you know that they it's not that we just use pattern   
1:44:50   
languages in like education or architecture. you'll see is in programming and that's actually a place   
1:44:57   
where you could it's interesting because you apply this like to sort of a functional you can apply it immediately   
1:45:03   
to a functional setting. You can use patterns in programming and use them to   
1:45:08   
sort of improve coding and you can measure that pretty easily. So it's a good empirical test of pattern languages   
1:45:15   
obviously in architecture where it originated and then of course in education where I guess what they're   
1:45:21   
suggesting is that you know these five-year plans these 10ear plans are useless need to have like a more organic   
1:45:29   
type of you know breage of things that are possible so you can shift and and   
1:45:34   
and as long as things conform to the overall pattern then that's fine actually   
1:45:41   
uh pattern language probably is also useful in project management. Think about that as a as a an application.   
1:45:48   
Yeah. I think we could have some really fun conversations and I would be totally open to trying to structure them this way around um both   
1:45:57   
essentially like in the arena of pro like you could say a programming language is sort of a pattern language.   
1:46:02   
inherently but like like because you're you you're both doing like it's structuring the library structuring like   
1:46:08   
maybe data like data structure the way you're structuring what you're trying to do in   
1:46:15   
the language and what what what is the what is the activity the economy of it well programming languages are about   
1:46:21   
computing so you're doing the the compute the transforms the the thing by which the input is being changed into   
1:46:29   
something else right and I think I use the interesting lens and kind of one that's a little bit separate from other   
1:46:35   
ways that we do this stuff. And so yeah, absolutely. For like you take the project management, they take it to curriculum design, you have these other   
1:46:43   
sort of computation and this this is where I'll kind of shout like I will try   
1:46:49   
to tag this and say for atti and the computational philosophy salon type stuff like this is this is this is   
1:46:54   
something we could talk about very interested in leading discussions about this in that space but   
1:47:01   
uh project management um doing things in the world curriculum. we   
1:47:07   
have this sort of computation adjacent um activity and you say computation in   
1:47:13   
the sense of how how does how do moving between states moving between okay   
1:47:18   
here's their initial state initial input what is what is how do we even just   
1:47:25   
demarcation no less the actual like computation of going between the states towards specific development or towards   
1:47:31   
specific and and similarly Bradley mentioned not just the fiveyear plan but   
1:47:37   
But there's and why I mentioned the little living um a gentle bit is like   
1:47:43   
there's a   
1:47:49   
a an implicit valuing or or professed valuing of um   
1:47:56   
organic sequential development and doing one of the components at a   
1:48:03   
time and then building organically adjacent as opposed to   
1:48:09   
um this sort of you know I would say basically inorganic um top down   
1:48:17   
imposition this is the structure whereas like no it's like how does it how does it emerge in more of a   
1:48:24   
biologically connected um I don't want to say embryogenesis but   
1:48:30   
like this sort of generative feels very biological biological gen that's sort of like in the building process for like   
1:48:36   
design or um uh curriculum in that case or or building the the thing. You don't   
1:48:43   
just you don't just like you know cut everything raise it to the ground and   
1:48:48   
then just immediately put a brick here to break here to break here brick here to break here. Just sort of like you know it it's you're building one of the   
1:48:55   
structures in in a structure or at least a plan of the structure at a time you know and and   
1:49:04   
then there is an adjust adjustment or re recalibration or okay, we we completed   
1:49:09   
the structure where go from here and yeah I think there's some incredibly you know useful ties to um project   
1:49:20   
management or program management or building you know building a town like   
1:49:28   
what's interesting before we move on to the the the thing here what I kind of close is like   
1:49:34   
it's really interesting to see and I also found a lecture I've just started   
1:49:40   
but I found an actual lecture of Christopher Alexander talking um about his his stuff and he was   
1:49:46   
talking to I believe oh yeah it might talk about this next time it's a lecture where um he introduces how it came up by   
1:49:54   
saying a bunch of computer science nerds in San Francisco wanted to talk about   
1:50:00   
stuff and and it it it it kind of you know circles the topics that we've talked about here and he's like, well,   
1:50:06   
he's like, hey, I don't know anything about computing, but here's what I know about. Um, and and that's sort of how he he   
1:50:12   
goes into it. And it's interesting to see him talk and to see   
1:50:17   
um what his considerations are. And I I appreciate his   
1:50:27   
he speaks with a useful amount of   
1:50:32   
declarative declaring inadequacy of how certain things work. And I find that to be a   
1:50:38   
skill I'm desperately trying to master and not very good idea because I want to be able to say here are things in a a   
1:50:46   
theory, in a structure, in a what in a in a whatnot. in a space in the world that are clearly inadequate   
1:50:54   
and we need to do something better. But how do you do that in a way that's not   
1:50:59   
kind of whining or sensational or you   
1:51:05   
know you got to offer the positives of aspiration while also being   
1:51:11   
you know speaking to a world that that doesn't want to hear that it's wrong about what it has available you know.   
1:51:19   
So, I'll leave it at that, but I think there's a tremendous amount of follow-up opportunities there. Um,   
1:51:26   
and I guess I'll move on to whatever this list is of um just so much. Any   
1:51:32   
other final comments? I know this is one. No, I think we're okay.   
1:51:38   
Um, I'll try to go through this very quickly. People can read the whole thing. They want this is literally just   
1:51:44   
there's so much happened this week, but I wanted to just note and I didn't even I didn't even put things on active   
1:51:50   
event which we just covered. So I maybe go back and add that or what or um   
1:51:56   
whatever else. Uh but just just super quick um recap of things. Um, first this   
1:52:02   
this this recap um or not this recap but the this I I would   
1:52:09   
recommend checking this out in the sense for people that are interested in you know funding but also getting a sense of   
1:52:16   
what um Shaughnessy and company are doing. Um they they've helped out with   
1:52:23   
Echolapto and they funded Addie and some other folks and and there's a certain I appreciate Jim's openness to both um   
1:52:34   
not not being hype oriented but being pretty sober. We had a nice conversation about people and founders and ideas   
1:52:40   
being pre or postfall and kind of the like fall as in kind of like pride quarterfall as in like are you are you   
1:52:48   
we are you are you do you have requisite humility to realize how fragile the development of your ideas and things are   
1:52:56   
or are you kind of in this gung-ho phase about things that you need but also like   
1:53:01   
are you are you sort of belligerent in your enthusiasm or or or   
1:53:07   
um foolish and have you have you you know that was a nice a nice part of the   
1:53:12   
talk but also just like what is structurally happening um here um and I   
1:53:17   
appreciate this optimism and soy around that uh so   
1:53:23   
computational philosophy club stuff is sort of happening there too um we also had a a very interesting   
1:53:31   
brain inspired um complexity group discussion this week was le Darren love   
1:53:38   
but it was cool because you know Kevin Mitchell Rel um and others Louis was   
1:53:44   
there this week and for those that are no regard through the complexity papers   
1:53:50   
as identified by David Cracker and Santa Fe Institute and his books around that   
1:53:57   
stuff the Minsk was interesting because   
1:54:03   
you know it's it's such a landmark paper and reference like you are so associated   
1:54:08   
with like AI winter and AI summer and it's a lot of blame for why   
1:54:14   
you steered us in the wrong direction and and you know this is is not just that paper but like you know there's   
1:54:22   
even a nice discussion on you know steps towards artificial intelligence versus other titles around   
1:54:30   
um you know potential huristics that maybe lead to something like this.   
1:54:38   
The nature of titles, the nature of funding, the nature of sort of the pressure, whatever banish people have to   
1:54:45   
communicate their idea and and and the folly that that may take or or just the   
1:54:52   
unintended consequences or the intended consequences and how messy it is. So, that was really cool. Moving very   
1:54:58   
quickly and I think I think this is available somewhere. This first one is definitely available on YouTube. This   
1:55:04   
happened yesterday. Um, and it was wonderful. Um, oops, sorry, I'm not   
1:55:11   
course. This I didn't get to see this one, but this is sort of our the machine perception and cognitive robotics lab at   
1:55:17   
FAU where folks from Epalopa are Will Han and I think Elon are both there. I   
1:55:24   
didn't get to attend this directly. Oh, doesn't appear to be on here yet.   
1:55:31   
Um, that's too bad. Uh but this is I believe this was   
1:55:37   
supposed to be live stream um about some of what they're doing in in the lab   
1:55:42   
space. Uh this uh small house court u from clock twisters. This happened   
1:55:50   
yesterday and if anybody's interested there'll actually be two more public   
1:55:56   
there's going to be more private sessions as well but there's two more like kind we like you to play test and talk about our ideas. Um,   
1:56:05   
I don't know the best way to show this, but um, basically plot twisters small halfless   
1:56:10   
court is part of sort of a a component of the game. cultures is is is a group   
1:56:15   
of people trying to make an online or an edtech educationalish   
1:56:22   
research inspired game world that's around like literacy or interpersonal   
1:56:29   
development and dealing dealing with narratives dealing with um these   
1:56:34   
important skills and even even dealing with things like people pressure around what how do you how do you construct   
1:56:41   
your identity and manage things in the present shop digital digital age that we're in. Um   
1:56:48   
small houseless court is this sort of implementation where you know um you   
1:56:56   
would go with someone who maybe you have a a conflict of some sort with and you would resolve it and go through a   
1:57:02   
different you know not quite mediation but it's a sort of a structured discussion that is as part of maybe the   
1:57:08   
game the game world and and how do we how do we both make this a useful like modeling a useful skill for real world   
1:57:15   
but also interesting So we did this play testing sessions with um a number of very interesting   
1:57:22   
people. Um people who are invested very much in   
1:57:27   
game design, people who work with um federal agencies, people working with youth um development, people who are   
1:57:34   
familiar with um um all manner of like digital platforms   
1:57:40   
and and making spaces in them and dealing with in you know interpersonal   
1:57:46   
um I would say conflict but interpersonal issues and matters how to do it. Uh one of the things that came up   
1:57:52   
in the session was court is maybe a jarring term to many especially in the west. Uh so maybe you   
1:58:00   
know clutches itself is sort of this the approach to the game was very light-hearted and fun. You have this   
1:58:05   
sort of you know hokey fun you know um dragon person leading you through the   
1:58:11   
cord. But like how do how do we how do we make it approachable? How do we make   
1:58:17   
it um how do we let people come into it? And I   
1:58:24   
I would we can go into more detail some other time interested in this, but it was we went we ran this very simple   
1:58:30   
scenario of two people coming to the court to discuss a particular matter. It was it was very fun to see everybody's   
1:58:36   
reaction even the sense of multiple people embodying one person here or like   
1:58:42   
thinking about one person uh as a play test or sort of a general demo and then   
1:58:48   
getting actual individual people to go through it and how that will be different sort of like understanding how the play testing could work and drawing   
1:58:54   
interesting things from that. Um yeah and we presented on this last year   
1:59:00   
at the inclusive gaming conference and Oxford and good stuff. Uh I didn't get to go to this in person. This was sort   
1:59:07   
of a what ties the nation together. Um I I I kind of give a shout out to the   
1:59:13   
center for center for ethics which is here in Cambridge at Harvard because I think they're they're trying   
1:59:20   
to put themselves out there a bit more. I I I I like a lot of what they've done. I've been to a few of their events in   
1:59:26   
person and as much as they're kind of, you know, some things at Harvard are   
1:59:32   
stuffy and removed, but they've been pretty open and I encourage their openness. There was a um Manolas Kellis   
1:59:42   
does a lot of um kind of salons at his house basically like he just invites   
1:59:47   
people over to his house. I didn't get to go to this one. Uh but it's it's in   
1:59:52   
Brooklyn. It's it's outside of um this   
1:59:58   
shows yet. Yeah, this is sort of you know this is with David Bennett and Resilient Mind and Brain the Community   
2:00:04   
and Aging. Um there's a lot a lot that was going on there. I know people that went to it, had a good time, and I know   
2:00:12   
uh Malus is um you know, he he's trying to sort of make   
2:00:19   
his own salon space and that's cool. Uh moral ambition, I know there was some uh   
2:00:26   
he came to Boston. He's got this big bus. He's doing Boston and Princeton and Stanford and um I I've almost worked   
2:00:33   
with them on a certain project. I'm Trump. I'm a fan of them. Um   
2:00:40   
I I don't agree with everything, but I think the idea of of intentionally   
2:00:45   
trying to give people alternatives is very very much um   
2:00:52   
what I want to do. Like one of the things I'm doing with the future center specifically is a project on how do we   
2:00:59   
identify alternatives? Um, and so I think he's doing it kind of on the ground. You know, they're   
2:01:05   
launching this uh fellowship for juniors at Harvard um as as a pilot and hopefully other places too, but if   
2:01:12   
you're a junior at Harvard, they want to basically incentivize you not to take your, you know, um   
2:01:20   
not not just, you know, don't just go into Mckenzie consultant. Um let's try   
2:01:27   
to give you other options to focus on bigger problems in the world. There's fellowship program for for that group to   
2:01:32   
do that and obviously he's got his book and there's other stuff going on here. I have a discussion around that. Um and   
2:01:38   
the last few things in San Francisco um there's a very cool uh through Joel again distress scientist department.   
2:01:45   
Shout out to distress scientist department. Um it's not really   
2:01:53   
that's not showing. There we go. Um but she has a lot of   
2:01:59   
great stuff as um things that are happening in in in San   
2:02:05   
Francisco on development and she's trying to to shift all the things about there's a there's a a gala happening of   
2:02:14   
the researcher creative we're sort of we talk about philosopher builders occasionally the researcher creatives um   
2:02:20   
and this is directly from her I talked a little bit about this but basically there's there's an exhibition for   
2:02:26   
researcher creatives trying to bring the working papers some tangible artifacts. Um kind of augmentation lab derived. I   
2:02:33   
think I mentioned them down here. Um some echolapto stuff trying to do sort of the creative parts emerging stuff   
2:02:39   
into the gala. So if you're in there, check it out. Um and then I mentioned   
2:02:45   
Tower at the end. There's just so much there's always stuff going on here. So I just I just plug their entire calendar.   
2:02:51   
Maybe segway if um here wants to say anything about them.   
2:02:56   
But that's that's that's there's just this is everything that's happening this week. And there's also like imagination   
2:03:01   
and action climate focused stuff starting here today. I might try to go   
2:03:07   
to some of those things. That's this is the John Warner event. And then yes, Princeton Envision in February and New   
2:03:16   
York Celebration will be computing in April. I will be at both of those events. Um and we'll be doing some   
2:03:22   
things with them ahead. So just tons of stuff. happening right now and I feel like   
2:03:28   
there's a big a big a big blast of stuff and everybody's going to be quiet again because it's finals and academic stuff's   
2:03:35   
going to bottle up. Then we're going to go into winter break and then it's it's it it's going to come it's going to   
2:03:42   
explode again. So, um any comments or questions? I know a lot. I know we're   
2:03:47   
kind of out of time. Uh but anything you want to say?   
2:03:52   
No, I think that's good. Uh, a lot of stuff going on. Um, yeah,   
2:03:58   
big big big shout out to Plot Twisters for because I haven't seen everything that's been going on in the space, but   
2:04:05   
this is a nice thing that they did and they they really made   
2:04:10   
um Amanda has has spearheaded a lot of like the medic of putting it out there,   
2:04:16   
orchestrating this stuff, and I think she's done a lot of that and all like, you know, um Jenny of course and Um,   
2:04:26   
I don't want to forget the last join. Those they the main persons for it. So,   
2:04:31   
um, shout out like that being a really fun time and I'm looking forward to next Friday doing I want to join the next the   
2:04:38   
version next Friday as well. Um, because I think it's it's it's both a fun problem to deal with in terms of   
2:04:44   
implementing and then it's it's cool to see what comes from it as trying to make   
2:04:50   
you know in one sense small court and the pattern language development senses sort of the first structure in the   
2:04:58   
community being built and then we already have links to the storytelling cards for these other things the field   
2:05:04   
of feelings the other comp these other physical spaces in twister world and   
2:05:09   
then the actual like what are you doing in those spaces so it's fun to think about in that sense too   
2:05:16   
yeah that went really well and I encourage everybody to   
2:05:21   
talk more about that Next, that's the next event. Here we go. And I will leave it at that.   
2:05:28   
All right. Thank you, Morgan. You wanted to say something?   
2:05:33   
Well, you know, I appreciate all the the Frontier Tower shout outs.   
2:05:39   
Uh, I'll change this to Frontier Tower in the background to be Thank you. I mean, you know, like or   
2:05:47   
identifying the uh the Boston location. Um   
2:05:53   
uh it it was nice to I'll be happy to call myself like front   
2:05:58   
chair Tower Boston ambassador. I'm sure you have many of those. I'll send you I'll send you a link for   
2:06:05   
the telegram. Um, I think there's there, you know, like uh   
2:06:12   
I don't know, you know, there's cities that I would choose to to focus on and um but like   
2:06:21   
like New York is the big location and I I think that has to do with basically   
2:06:27   
the the financing, you know, but in terms of like trying to find a good real estate   
2:06:36   
um opportunity. I would like, you know, and and I'm biased here because I've got   
2:06:41   
a place in Baltimore, but I would go for Baltimore, you know. I mean, anyway, but uh did you   
2:06:49   
already do the the Orwell event at Stanford um this Sunday?   
2:06:55   
I didn't. Okay. And I mean, you know, if you're talking San Francisco events, it's um you know,   
2:07:03   
it's uh controls the the future I think is the   
2:07:08   
the title. Who's that? Who's running that? I thought it was augmentation lab.   
2:07:14   
Yeah. Yeah. Yeah. It's Indra from I I think I think it doesn't say   
2:07:20   
augmentation lab directly and it's like you know anyway but it's down in Stanford and um   
2:07:26   
here it is. If I wasn't right, this is like supremely relevant to   
2:07:34   
that should completely be on the list here. If you're if you're there tomorrow   
2:07:40   
at Stanford, uh, highly recommend Yeah. checking it out because it's what we   
2:07:45   
talk. Yeah. Okay. It's echalapto is the Yeah.   
2:07:52   
Um, anyway, that was that was the uh, yeah, it's not San Francisco,   
2:07:59   
it's South Bay, but but it seemed like worth mentioning.   
2:08:06   
Um I made I don't think there's a virtual   
2:08:12   
yeah it is it is getting close to the holidays and um so   
2:08:19   
you know at this point uh I mean everybody's got their sights on um uh SFN right now right because that's   
2:08:27   
that's the yeah so it's like like a lot of last   
2:08:32   
week was actually uh conference conferences that were that   
2:08:39   
you know full conferences that are run as pre-conferences to SFN   
2:08:44   
right so so neer is the big you know brain   
2:08:50   
implant um um conference which is run by   
2:08:55   
IAPS and um there's also the like international society of something like   
2:09:02   
developmental psychobiology Um where uh um but you know but   
2:09:10   
electrical geodessics goes because they they're high density EEG systems are are   
2:09:16   
widely used with kids because they you know because they just use little sponges instead of um you know using   
2:09:24   
paste things like that. Anyway, um but that's a really interesting meeting if   
2:09:30   
you're interested in yeah developmental neurobiio.   
2:09:35   
Um uh we've we've started u we started flex   
2:09:44   
flex session for the um connection the camera uh working with students in Ghana   
2:09:51   
or at least my my team's in Ghana. So we get have a real real data set to   
2:09:58   
work with from now until January 23rd. So um I don't think that means that um   
2:10:06   
like there won't be any conflict on Saturdays. I don't think okay uh uh you know but um but looking   
2:10:14   
forward to looking forward to that and um   
2:10:21   
uh yeah so even even though like everyone's sites are on SFN and there's   
2:10:28   
definitely some interesting things that are you know I'm I'm looking forward to some uh like announcements as well as as   
2:10:36   
getting to see at least hopefully people taking pictures of posters and sending and   
2:10:42   
posting them. Um uh   
2:10:47   
yeah just the my sightes are on the the the next San Diego meeting which is   
2:10:53   
which should be nerves and the the work workshop around ner   
2:11:00   
that's what um yeah   
2:11:05   
yeah I don't know I I had there was a version   
2:11:13   
where I was going to be in San Diego basically this like in November um for some things but I don't think I'm going   
2:11:19   
to be able to do it. I had some um things here that I had to stay here and   
2:11:26   
attend to some emergencies as well. So um yeah, are you going to be are you going to be going to any of those events   
2:11:32   
in person yourself? I I I absolutely want to catch the the   
2:11:38   
workshop on foundation models for I think it's body and   
2:11:44   
mind. Let me let me find the uh the exact um   
2:11:55   
it's yeah foundation models for the brain and body.   
2:12:02   
So it's it's brain bodyfmworkshop.github.io.   
2:12:08   
Um that that should be should be really interesting. That's going to include you   
2:12:15   
know some of the the organizers. So one of one of the organizers works   
2:12:21   
currently at tower. So there's there's that connection. Um   
2:12:27   
but uh this has also got Paul Scotty who's leads the   
2:12:34   
mind's eye team you know. So what was it stability AI and now it's softened um is   
2:12:42   
is also part of this. So they'll be talking about, you know, uh, fMRI   
2:12:47   
foundation models, you know, electro foundation models, EMG foundation   
2:12:52   
models, um, you know, since some of the the wrist actuator stuff, some of the   
2:12:59   
um, yeah, anyway, that that that I want to catch plus the EEG challenge, which   
2:13:05   
is which is uh, also going to be, you know, the winner will be announced and   
2:13:10   
um, so it's kind of kind of my is to actually catch up with the the um the   
2:13:17   
people who were doing that who also leads kind of the neurotex project of   
2:13:24   
Moab um or as the as the French guys say it Moab BB   
2:13:32   
and uh uh yeah anyway so those lots you know and that's that'll probably you   
2:13:39   
know then it'll be some like some some Christmas parties and you know and that   
2:13:44   
everybody will be you know that everything will be on pause until January, right?   
2:13:50   
Yeah. Um um yeah, but   
2:13:56   
that's a question that I have is like I'm do do you in terms of all the events   
2:14:02   
that we talk about everything is there basically going to be a dead period from like when is ner or   
2:14:10   
so it's December 2nd to the 7th so it's it's technically to the you know 2nd to   
2:14:16   
the 5th and then the workshops are the again. So basically after that it's going to be   
2:14:23   
kind of a dead period for a while because holidays there there's a few   
2:14:28   
things you know I mean it's just like like if if I could um if I had a   
2:14:34   
teleporter uh and didn't have to actually fly in   
2:14:41   
weird um lift driven aircraft uh um I would uh the ebrains conference   
2:14:50   
is in Brussels. So that's the you know that's a successor to the European uh   
2:14:56   
human brain project. Um I think that goes until the 11th.   
2:15:03   
So, you know, it's like, yeah, but I I just can't I just can't justify being a   
2:15:11   
jet setter to to hit the, you know, there there's also like something on the   
2:15:17   
island of like it's I'm gonna say it's Capri, but it's it's not, but it's al   
2:15:23   
it's something like that where it's just like it's a bunch of, you know, virtual brain project people that are meeting on   
2:15:30   
some Mediterranean island, you know, some picturesque Mediterranean island and justifying it as a as a scientific,   
2:15:38   
you know, scientific workshop. Um but but yeah, essentially um what   
2:15:48   
what I what I someone you know the the former director of um uh   
2:15:56   
of Neuroch X who's a posttock at John's Hopkins sent me uh this cool   
2:16:06   
um second one sec um   
2:16:12   
sent me um   
2:16:19   
this particular   
2:16:26   
um so this will be in mid January at at Hopkins.   
2:16:33   
um that is   
2:16:41   
is about organoids and it's got it's got like all all the Hopkins organoid people   
2:16:49   
speaking. Um so okay 16. So yeah, uh I I I had to   
2:16:59   
check with him because I was just like, "Wait, wait, do I need to be a John's Hopkins employee,   
2:17:06   
you know, and uh he he he checked and and it is actually an open meeting." Um   
2:17:14   
but um uh it sound sounds like a great way to   
2:17:19   
start the year in terms of of actually getting to meet, you know, Harding, you   
2:17:25   
know, Lena Smova, um, and, you know, the the the other   
2:17:31   
list of of of organoids luminaries there. Um, so looking looking forward to   
2:17:38   
that in the new year and um, yeah, I selfishly asked because I'm I'm trying   
2:17:45   
to plan both for like data and direction and general stuff like I I need to have   
2:17:50   
a very clear dead period myself. I'm trying to think when's the best time to start it. I think it's basically going to be after the first week of December.   
2:17:57   
I don't know if Bradley, you have any Do you have any like planned down times when we know we're not meeting here like   
2:18:04   
probably the last maybe two weeks of January or December and maybe the first week of I you know we usually take that   
2:18:11   
time off. So that'll be uh you know kind of our downtime.   
2:18:17   
Okay, that's all that's all for me. I have to go soon. All right. Yeah. Sounds good.   
2:18:24   
Thank you all for Sounds great. Yeah, thank you. Good conversations and we'll follow up on   
2:18:29   
this stuff next week. Uh see you then. Thanks. Bye.   
2:18:35   
See you. See you.   
